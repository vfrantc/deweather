{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vfrantc/deweather/blob/main/run_ZID_on_sots.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzpwHbov4psu",
        "outputId": "5facc8b7-cb13-4d99-e596-44f53fe5e700"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jul  5 14:46:57 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    25W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WLvgF133L05",
        "outputId": "06639d22-e603-499f-c842-e7991185a3f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xThgAOTF3PW7",
        "outputId": "051b58ac-9173-48de-c5c9-5a29bc966550"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  SOTS.zip\n",
            "   creating: SOTS/\n",
            "   creating: SOTS/indoor/\n",
            "   creating: SOTS/indoor/gt/\n",
            " extracting: SOTS/indoor/gt/1400.png  \n",
            " extracting: SOTS/indoor/gt/1401.png  \n",
            " extracting: SOTS/indoor/gt/1402.png  \n",
            " extracting: SOTS/indoor/gt/1403.png  \n",
            " extracting: SOTS/indoor/gt/1404.png  \n",
            " extracting: SOTS/indoor/gt/1405.png  \n",
            " extracting: SOTS/indoor/gt/1406.png  \n",
            " extracting: SOTS/indoor/gt/1407.png  \n",
            " extracting: SOTS/indoor/gt/1408.png  \n",
            " extracting: SOTS/indoor/gt/1409.png  \n",
            " extracting: SOTS/indoor/gt/1410.png  \n",
            " extracting: SOTS/indoor/gt/1411.png  \n",
            " extracting: SOTS/indoor/gt/1412.png  \n",
            " extracting: SOTS/indoor/gt/1413.png  \n",
            " extracting: SOTS/indoor/gt/1414.png  \n",
            " extracting: SOTS/indoor/gt/1415.png  \n",
            " extracting: SOTS/indoor/gt/1416.png  \n",
            " extracting: SOTS/indoor/gt/1417.png  \n",
            " extracting: SOTS/indoor/gt/1418.png  \n",
            " extracting: SOTS/indoor/gt/1419.png  \n",
            " extracting: SOTS/indoor/gt/1420.png  \n",
            " extracting: SOTS/indoor/gt/1421.png  \n",
            " extracting: SOTS/indoor/gt/1422.png  \n",
            " extracting: SOTS/indoor/gt/1423.png  \n",
            " extracting: SOTS/indoor/gt/1424.png  \n",
            " extracting: SOTS/indoor/gt/1425.png  \n",
            " extracting: SOTS/indoor/gt/1426.png  \n",
            " extracting: SOTS/indoor/gt/1427.png  \n",
            " extracting: SOTS/indoor/gt/1428.png  \n",
            " extracting: SOTS/indoor/gt/1429.png  \n",
            " extracting: SOTS/indoor/gt/1430.png  \n",
            " extracting: SOTS/indoor/gt/1431.png  \n",
            " extracting: SOTS/indoor/gt/1432.png  \n",
            " extracting: SOTS/indoor/gt/1433.png  \n",
            " extracting: SOTS/indoor/gt/1434.png  \n",
            " extracting: SOTS/indoor/gt/1435.png  \n",
            " extracting: SOTS/indoor/gt/1436.png  \n",
            " extracting: SOTS/indoor/gt/1437.png  \n",
            " extracting: SOTS/indoor/gt/1438.png  \n",
            " extracting: SOTS/indoor/gt/1439.png  \n",
            " extracting: SOTS/indoor/gt/1440.png  \n",
            " extracting: SOTS/indoor/gt/1441.png  \n",
            " extracting: SOTS/indoor/gt/1442.png  \n",
            " extracting: SOTS/indoor/gt/1443.png  \n",
            " extracting: SOTS/indoor/gt/1444.png  \n",
            " extracting: SOTS/indoor/gt/1445.png  \n",
            " extracting: SOTS/indoor/gt/1446.png  \n",
            " extracting: SOTS/indoor/gt/1447.png  \n",
            " extracting: SOTS/indoor/gt/1448.png  \n",
            " extracting: SOTS/indoor/gt/1449.png  \n",
            "   creating: SOTS/indoor/hazy/\n",
            " extracting: SOTS/indoor/hazy/1400_1.png  \n",
            " extracting: SOTS/indoor/hazy/1400_10.png  \n",
            " extracting: SOTS/indoor/hazy/1400_2.png  \n",
            " extracting: SOTS/indoor/hazy/1400_3.png  \n",
            " extracting: SOTS/indoor/hazy/1400_4.png  \n",
            " extracting: SOTS/indoor/hazy/1400_5.png  \n",
            " extracting: SOTS/indoor/hazy/1400_6.png  \n",
            " extracting: SOTS/indoor/hazy/1400_7.png  \n",
            " extracting: SOTS/indoor/hazy/1400_8.png  \n",
            " extracting: SOTS/indoor/hazy/1400_9.png  \n",
            " extracting: SOTS/indoor/hazy/1401_1.png  \n",
            " extracting: SOTS/indoor/hazy/1401_10.png  \n",
            " extracting: SOTS/indoor/hazy/1401_2.png  \n",
            " extracting: SOTS/indoor/hazy/1401_3.png  \n",
            " extracting: SOTS/indoor/hazy/1401_4.png  \n",
            " extracting: SOTS/indoor/hazy/1401_5.png  \n",
            " extracting: SOTS/indoor/hazy/1401_6.png  \n",
            " extracting: SOTS/indoor/hazy/1401_7.png  \n",
            " extracting: SOTS/indoor/hazy/1401_8.png  \n",
            " extracting: SOTS/indoor/hazy/1401_9.png  \n",
            " extracting: SOTS/indoor/hazy/1402_1.png  \n",
            " extracting: SOTS/indoor/hazy/1402_10.png  \n",
            " extracting: SOTS/indoor/hazy/1402_2.png  \n",
            " extracting: SOTS/indoor/hazy/1402_3.png  \n",
            " extracting: SOTS/indoor/hazy/1402_4.png  \n",
            " extracting: SOTS/indoor/hazy/1402_5.png  \n",
            " extracting: SOTS/indoor/hazy/1402_6.png  \n",
            " extracting: SOTS/indoor/hazy/1402_7.png  \n",
            " extracting: SOTS/indoor/hazy/1402_8.png  \n",
            " extracting: SOTS/indoor/hazy/1402_9.png  \n",
            " extracting: SOTS/indoor/hazy/1403_1.png  \n",
            " extracting: SOTS/indoor/hazy/1403_10.png  \n",
            " extracting: SOTS/indoor/hazy/1403_2.png  \n",
            " extracting: SOTS/indoor/hazy/1403_3.png  \n",
            " extracting: SOTS/indoor/hazy/1403_4.png  \n",
            " extracting: SOTS/indoor/hazy/1403_5.png  \n",
            " extracting: SOTS/indoor/hazy/1403_6.png  \n",
            " extracting: SOTS/indoor/hazy/1403_7.png  \n",
            " extracting: SOTS/indoor/hazy/1403_8.png  \n",
            " extracting: SOTS/indoor/hazy/1403_9.png  \n",
            " extracting: SOTS/indoor/hazy/1404_1.png  \n",
            " extracting: SOTS/indoor/hazy/1404_10.png  \n",
            " extracting: SOTS/indoor/hazy/1404_2.png  \n",
            " extracting: SOTS/indoor/hazy/1404_3.png  \n",
            " extracting: SOTS/indoor/hazy/1404_4.png  \n",
            " extracting: SOTS/indoor/hazy/1404_5.png  \n",
            " extracting: SOTS/indoor/hazy/1404_6.png  \n",
            " extracting: SOTS/indoor/hazy/1404_7.png  \n",
            " extracting: SOTS/indoor/hazy/1404_8.png  \n",
            " extracting: SOTS/indoor/hazy/1404_9.png  \n",
            " extracting: SOTS/indoor/hazy/1405_1.png  \n",
            " extracting: SOTS/indoor/hazy/1405_10.png  \n",
            " extracting: SOTS/indoor/hazy/1405_2.png  \n",
            " extracting: SOTS/indoor/hazy/1405_3.png  \n",
            " extracting: SOTS/indoor/hazy/1405_4.png  \n",
            " extracting: SOTS/indoor/hazy/1405_5.png  \n",
            " extracting: SOTS/indoor/hazy/1405_6.png  \n",
            " extracting: SOTS/indoor/hazy/1405_7.png  \n",
            " extracting: SOTS/indoor/hazy/1405_8.png  \n",
            " extracting: SOTS/indoor/hazy/1405_9.png  \n",
            " extracting: SOTS/indoor/hazy/1406_1.png  \n",
            " extracting: SOTS/indoor/hazy/1406_10.png  \n",
            " extracting: SOTS/indoor/hazy/1406_2.png  \n",
            " extracting: SOTS/indoor/hazy/1406_3.png  \n",
            " extracting: SOTS/indoor/hazy/1406_4.png  \n",
            " extracting: SOTS/indoor/hazy/1406_5.png  \n",
            " extracting: SOTS/indoor/hazy/1406_6.png  \n",
            " extracting: SOTS/indoor/hazy/1406_7.png  \n",
            " extracting: SOTS/indoor/hazy/1406_8.png  \n",
            " extracting: SOTS/indoor/hazy/1406_9.png  \n",
            " extracting: SOTS/indoor/hazy/1407_1.png  \n",
            " extracting: SOTS/indoor/hazy/1407_10.png  \n",
            " extracting: SOTS/indoor/hazy/1407_2.png  \n",
            " extracting: SOTS/indoor/hazy/1407_3.png  \n",
            " extracting: SOTS/indoor/hazy/1407_4.png  \n",
            " extracting: SOTS/indoor/hazy/1407_5.png  \n",
            " extracting: SOTS/indoor/hazy/1407_6.png  \n",
            " extracting: SOTS/indoor/hazy/1407_7.png  \n",
            " extracting: SOTS/indoor/hazy/1407_8.png  \n",
            " extracting: SOTS/indoor/hazy/1407_9.png  \n",
            " extracting: SOTS/indoor/hazy/1408_1.png  \n",
            " extracting: SOTS/indoor/hazy/1408_10.png  \n",
            " extracting: SOTS/indoor/hazy/1408_2.png  \n",
            " extracting: SOTS/indoor/hazy/1408_3.png  \n",
            " extracting: SOTS/indoor/hazy/1408_4.png  \n",
            " extracting: SOTS/indoor/hazy/1408_5.png  \n",
            " extracting: SOTS/indoor/hazy/1408_6.png  \n",
            " extracting: SOTS/indoor/hazy/1408_7.png  \n",
            " extracting: SOTS/indoor/hazy/1408_8.png  \n",
            " extracting: SOTS/indoor/hazy/1408_9.png  \n",
            " extracting: SOTS/indoor/hazy/1409_1.png  \n",
            " extracting: SOTS/indoor/hazy/1409_10.png  \n",
            " extracting: SOTS/indoor/hazy/1409_2.png  \n",
            " extracting: SOTS/indoor/hazy/1409_3.png  \n",
            " extracting: SOTS/indoor/hazy/1409_4.png  \n",
            " extracting: SOTS/indoor/hazy/1409_5.png  \n",
            " extracting: SOTS/indoor/hazy/1409_6.png  \n",
            " extracting: SOTS/indoor/hazy/1409_7.png  \n",
            " extracting: SOTS/indoor/hazy/1409_8.png  \n",
            " extracting: SOTS/indoor/hazy/1409_9.png  \n",
            " extracting: SOTS/indoor/hazy/1410_1.png  \n",
            " extracting: SOTS/indoor/hazy/1410_10.png  \n",
            " extracting: SOTS/indoor/hazy/1410_2.png  \n",
            " extracting: SOTS/indoor/hazy/1410_3.png  \n",
            " extracting: SOTS/indoor/hazy/1410_4.png  \n",
            " extracting: SOTS/indoor/hazy/1410_5.png  \n",
            " extracting: SOTS/indoor/hazy/1410_6.png  \n",
            " extracting: SOTS/indoor/hazy/1410_7.png  \n",
            " extracting: SOTS/indoor/hazy/1410_8.png  \n",
            " extracting: SOTS/indoor/hazy/1410_9.png  \n",
            " extracting: SOTS/indoor/hazy/1411_1.png  \n",
            " extracting: SOTS/indoor/hazy/1411_10.png  \n",
            " extracting: SOTS/indoor/hazy/1411_2.png  \n",
            " extracting: SOTS/indoor/hazy/1411_3.png  \n",
            " extracting: SOTS/indoor/hazy/1411_4.png  \n",
            " extracting: SOTS/indoor/hazy/1411_5.png  \n",
            " extracting: SOTS/indoor/hazy/1411_6.png  \n",
            " extracting: SOTS/indoor/hazy/1411_7.png  \n",
            " extracting: SOTS/indoor/hazy/1411_8.png  \n",
            " extracting: SOTS/indoor/hazy/1411_9.png  \n",
            " extracting: SOTS/indoor/hazy/1412_1.png  \n",
            " extracting: SOTS/indoor/hazy/1412_10.png  \n",
            " extracting: SOTS/indoor/hazy/1412_2.png  \n",
            " extracting: SOTS/indoor/hazy/1412_3.png  \n",
            " extracting: SOTS/indoor/hazy/1412_4.png  \n",
            " extracting: SOTS/indoor/hazy/1412_5.png  \n",
            " extracting: SOTS/indoor/hazy/1412_6.png  \n",
            " extracting: SOTS/indoor/hazy/1412_7.png  \n",
            " extracting: SOTS/indoor/hazy/1412_8.png  \n",
            " extracting: SOTS/indoor/hazy/1412_9.png  \n",
            " extracting: SOTS/indoor/hazy/1413_1.png  \n",
            " extracting: SOTS/indoor/hazy/1413_10.png  \n",
            " extracting: SOTS/indoor/hazy/1413_2.png  \n",
            " extracting: SOTS/indoor/hazy/1413_3.png  \n",
            " extracting: SOTS/indoor/hazy/1413_4.png  \n",
            " extracting: SOTS/indoor/hazy/1413_5.png  \n",
            " extracting: SOTS/indoor/hazy/1413_6.png  \n",
            " extracting: SOTS/indoor/hazy/1413_7.png  \n",
            " extracting: SOTS/indoor/hazy/1413_8.png  \n",
            " extracting: SOTS/indoor/hazy/1413_9.png  \n",
            " extracting: SOTS/indoor/hazy/1414_1.png  \n",
            " extracting: SOTS/indoor/hazy/1414_10.png  \n",
            " extracting: SOTS/indoor/hazy/1414_2.png  \n",
            " extracting: SOTS/indoor/hazy/1414_3.png  \n",
            " extracting: SOTS/indoor/hazy/1414_4.png  \n",
            " extracting: SOTS/indoor/hazy/1414_5.png  \n",
            " extracting: SOTS/indoor/hazy/1414_6.png  \n",
            " extracting: SOTS/indoor/hazy/1414_7.png  \n",
            " extracting: SOTS/indoor/hazy/1414_8.png  \n",
            " extracting: SOTS/indoor/hazy/1414_9.png  \n",
            " extracting: SOTS/indoor/hazy/1415_1.png  \n",
            " extracting: SOTS/indoor/hazy/1415_10.png  \n",
            " extracting: SOTS/indoor/hazy/1415_2.png  \n",
            " extracting: SOTS/indoor/hazy/1415_3.png  \n",
            " extracting: SOTS/indoor/hazy/1415_4.png  \n",
            " extracting: SOTS/indoor/hazy/1415_5.png  \n",
            " extracting: SOTS/indoor/hazy/1415_6.png  \n",
            " extracting: SOTS/indoor/hazy/1415_7.png  \n",
            " extracting: SOTS/indoor/hazy/1415_8.png  \n",
            " extracting: SOTS/indoor/hazy/1415_9.png  \n",
            " extracting: SOTS/indoor/hazy/1416_1.png  \n",
            " extracting: SOTS/indoor/hazy/1416_10.png  \n",
            " extracting: SOTS/indoor/hazy/1416_2.png  \n",
            " extracting: SOTS/indoor/hazy/1416_3.png  \n",
            " extracting: SOTS/indoor/hazy/1416_4.png  \n",
            " extracting: SOTS/indoor/hazy/1416_5.png  \n",
            " extracting: SOTS/indoor/hazy/1416_6.png  \n",
            " extracting: SOTS/indoor/hazy/1416_7.png  \n",
            " extracting: SOTS/indoor/hazy/1416_8.png  \n",
            " extracting: SOTS/indoor/hazy/1416_9.png  \n",
            " extracting: SOTS/indoor/hazy/1417_1.png  \n",
            " extracting: SOTS/indoor/hazy/1417_10.png  \n",
            " extracting: SOTS/indoor/hazy/1417_2.png  \n",
            " extracting: SOTS/indoor/hazy/1417_3.png  \n",
            " extracting: SOTS/indoor/hazy/1417_4.png  \n",
            " extracting: SOTS/indoor/hazy/1417_5.png  \n",
            " extracting: SOTS/indoor/hazy/1417_6.png  \n",
            " extracting: SOTS/indoor/hazy/1417_7.png  \n",
            " extracting: SOTS/indoor/hazy/1417_8.png  \n",
            " extracting: SOTS/indoor/hazy/1417_9.png  \n",
            " extracting: SOTS/indoor/hazy/1418_1.png  \n",
            " extracting: SOTS/indoor/hazy/1418_10.png  \n",
            " extracting: SOTS/indoor/hazy/1418_2.png  \n",
            " extracting: SOTS/indoor/hazy/1418_3.png  \n",
            " extracting: SOTS/indoor/hazy/1418_4.png  \n",
            " extracting: SOTS/indoor/hazy/1418_5.png  \n",
            " extracting: SOTS/indoor/hazy/1418_6.png  \n",
            " extracting: SOTS/indoor/hazy/1418_7.png  \n",
            " extracting: SOTS/indoor/hazy/1418_8.png  \n",
            " extracting: SOTS/indoor/hazy/1418_9.png  \n",
            " extracting: SOTS/indoor/hazy/1419_1.png  \n",
            " extracting: SOTS/indoor/hazy/1419_10.png  \n",
            " extracting: SOTS/indoor/hazy/1419_2.png  \n",
            " extracting: SOTS/indoor/hazy/1419_3.png  \n",
            " extracting: SOTS/indoor/hazy/1419_4.png  \n",
            " extracting: SOTS/indoor/hazy/1419_5.png  \n",
            " extracting: SOTS/indoor/hazy/1419_6.png  \n",
            " extracting: SOTS/indoor/hazy/1419_7.png  \n",
            " extracting: SOTS/indoor/hazy/1419_8.png  \n",
            " extracting: SOTS/indoor/hazy/1419_9.png  \n",
            " extracting: SOTS/indoor/hazy/1420_1.png  \n",
            " extracting: SOTS/indoor/hazy/1420_10.png  \n",
            " extracting: SOTS/indoor/hazy/1420_2.png  \n",
            " extracting: SOTS/indoor/hazy/1420_3.png  \n",
            " extracting: SOTS/indoor/hazy/1420_4.png  \n",
            " extracting: SOTS/indoor/hazy/1420_5.png  \n",
            " extracting: SOTS/indoor/hazy/1420_6.png  \n",
            " extracting: SOTS/indoor/hazy/1420_7.png  \n",
            " extracting: SOTS/indoor/hazy/1420_8.png  \n",
            " extracting: SOTS/indoor/hazy/1420_9.png  \n",
            " extracting: SOTS/indoor/hazy/1421_1.png  \n",
            " extracting: SOTS/indoor/hazy/1421_10.png  \n",
            " extracting: SOTS/indoor/hazy/1421_2.png  \n",
            " extracting: SOTS/indoor/hazy/1421_3.png  \n",
            " extracting: SOTS/indoor/hazy/1421_4.png  \n",
            " extracting: SOTS/indoor/hazy/1421_5.png  \n",
            " extracting: SOTS/indoor/hazy/1421_6.png  \n",
            " extracting: SOTS/indoor/hazy/1421_7.png  \n",
            " extracting: SOTS/indoor/hazy/1421_8.png  \n",
            " extracting: SOTS/indoor/hazy/1421_9.png  \n",
            " extracting: SOTS/indoor/hazy/1422_1.png  \n",
            " extracting: SOTS/indoor/hazy/1422_10.png  \n",
            " extracting: SOTS/indoor/hazy/1422_2.png  \n",
            " extracting: SOTS/indoor/hazy/1422_3.png  \n",
            " extracting: SOTS/indoor/hazy/1422_4.png  \n",
            " extracting: SOTS/indoor/hazy/1422_5.png  \n",
            " extracting: SOTS/indoor/hazy/1422_6.png  \n",
            " extracting: SOTS/indoor/hazy/1422_7.png  \n",
            " extracting: SOTS/indoor/hazy/1422_8.png  \n",
            " extracting: SOTS/indoor/hazy/1422_9.png  \n",
            " extracting: SOTS/indoor/hazy/1423_1.png  \n",
            " extracting: SOTS/indoor/hazy/1423_10.png  \n",
            " extracting: SOTS/indoor/hazy/1423_2.png  \n",
            " extracting: SOTS/indoor/hazy/1423_3.png  \n",
            " extracting: SOTS/indoor/hazy/1423_4.png  \n",
            " extracting: SOTS/indoor/hazy/1423_5.png  \n",
            " extracting: SOTS/indoor/hazy/1423_6.png  \n",
            " extracting: SOTS/indoor/hazy/1423_7.png  \n",
            " extracting: SOTS/indoor/hazy/1423_8.png  \n",
            " extracting: SOTS/indoor/hazy/1423_9.png  \n",
            " extracting: SOTS/indoor/hazy/1424_1.png  \n",
            " extracting: SOTS/indoor/hazy/1424_10.png  \n",
            " extracting: SOTS/indoor/hazy/1424_2.png  \n",
            " extracting: SOTS/indoor/hazy/1424_3.png  \n",
            " extracting: SOTS/indoor/hazy/1424_4.png  \n",
            " extracting: SOTS/indoor/hazy/1424_5.png  \n",
            " extracting: SOTS/indoor/hazy/1424_6.png  \n",
            " extracting: SOTS/indoor/hazy/1424_7.png  \n",
            " extracting: SOTS/indoor/hazy/1424_8.png  \n",
            " extracting: SOTS/indoor/hazy/1424_9.png  \n",
            " extracting: SOTS/indoor/hazy/1425_1.png  \n",
            " extracting: SOTS/indoor/hazy/1425_10.png  \n",
            " extracting: SOTS/indoor/hazy/1425_2.png  \n",
            " extracting: SOTS/indoor/hazy/1425_3.png  \n",
            " extracting: SOTS/indoor/hazy/1425_4.png  \n",
            " extracting: SOTS/indoor/hazy/1425_5.png  \n",
            " extracting: SOTS/indoor/hazy/1425_6.png  \n",
            " extracting: SOTS/indoor/hazy/1425_7.png  \n",
            " extracting: SOTS/indoor/hazy/1425_8.png  \n",
            " extracting: SOTS/indoor/hazy/1425_9.png  \n",
            " extracting: SOTS/indoor/hazy/1426_1.png  \n",
            " extracting: SOTS/indoor/hazy/1426_10.png  \n",
            " extracting: SOTS/indoor/hazy/1426_2.png  \n",
            " extracting: SOTS/indoor/hazy/1426_3.png  \n",
            " extracting: SOTS/indoor/hazy/1426_4.png  \n",
            " extracting: SOTS/indoor/hazy/1426_5.png  \n",
            " extracting: SOTS/indoor/hazy/1426_6.png  \n",
            " extracting: SOTS/indoor/hazy/1426_7.png  \n",
            " extracting: SOTS/indoor/hazy/1426_8.png  \n",
            " extracting: SOTS/indoor/hazy/1426_9.png  \n",
            " extracting: SOTS/indoor/hazy/1427_1.png  \n",
            " extracting: SOTS/indoor/hazy/1427_10.png  \n",
            " extracting: SOTS/indoor/hazy/1427_2.png  \n",
            " extracting: SOTS/indoor/hazy/1427_3.png  \n",
            " extracting: SOTS/indoor/hazy/1427_4.png  \n",
            " extracting: SOTS/indoor/hazy/1427_5.png  \n",
            " extracting: SOTS/indoor/hazy/1427_6.png  \n",
            " extracting: SOTS/indoor/hazy/1427_7.png  \n",
            " extracting: SOTS/indoor/hazy/1427_8.png  \n",
            " extracting: SOTS/indoor/hazy/1427_9.png  \n",
            " extracting: SOTS/indoor/hazy/1428_1.png  \n",
            " extracting: SOTS/indoor/hazy/1428_10.png  \n",
            " extracting: SOTS/indoor/hazy/1428_2.png  \n",
            " extracting: SOTS/indoor/hazy/1428_3.png  \n",
            " extracting: SOTS/indoor/hazy/1428_4.png  \n",
            " extracting: SOTS/indoor/hazy/1428_5.png  \n",
            " extracting: SOTS/indoor/hazy/1428_6.png  \n",
            " extracting: SOTS/indoor/hazy/1428_7.png  \n",
            " extracting: SOTS/indoor/hazy/1428_8.png  \n",
            " extracting: SOTS/indoor/hazy/1428_9.png  \n",
            " extracting: SOTS/indoor/hazy/1429_1.png  \n",
            " extracting: SOTS/indoor/hazy/1429_10.png  \n",
            " extracting: SOTS/indoor/hazy/1429_2.png  \n",
            " extracting: SOTS/indoor/hazy/1429_3.png  \n",
            " extracting: SOTS/indoor/hazy/1429_4.png  \n",
            " extracting: SOTS/indoor/hazy/1429_5.png  \n",
            " extracting: SOTS/indoor/hazy/1429_6.png  \n",
            " extracting: SOTS/indoor/hazy/1429_7.png  \n",
            " extracting: SOTS/indoor/hazy/1429_8.png  \n",
            " extracting: SOTS/indoor/hazy/1429_9.png  \n",
            " extracting: SOTS/indoor/hazy/1430_1.png  \n",
            " extracting: SOTS/indoor/hazy/1430_10.png  \n",
            " extracting: SOTS/indoor/hazy/1430_2.png  \n",
            " extracting: SOTS/indoor/hazy/1430_3.png  \n",
            " extracting: SOTS/indoor/hazy/1430_4.png  \n",
            " extracting: SOTS/indoor/hazy/1430_5.png  \n",
            " extracting: SOTS/indoor/hazy/1430_6.png  \n",
            " extracting: SOTS/indoor/hazy/1430_7.png  \n",
            " extracting: SOTS/indoor/hazy/1430_8.png  \n",
            " extracting: SOTS/indoor/hazy/1430_9.png  \n",
            " extracting: SOTS/indoor/hazy/1431_1.png  \n",
            " extracting: SOTS/indoor/hazy/1431_10.png  \n",
            " extracting: SOTS/indoor/hazy/1431_2.png  \n",
            " extracting: SOTS/indoor/hazy/1431_3.png  \n",
            " extracting: SOTS/indoor/hazy/1431_4.png  \n",
            " extracting: SOTS/indoor/hazy/1431_5.png  \n",
            " extracting: SOTS/indoor/hazy/1431_6.png  \n",
            " extracting: SOTS/indoor/hazy/1431_7.png  \n",
            " extracting: SOTS/indoor/hazy/1431_8.png  \n",
            " extracting: SOTS/indoor/hazy/1431_9.png  \n",
            " extracting: SOTS/indoor/hazy/1432_1.png  \n",
            " extracting: SOTS/indoor/hazy/1432_10.png  \n",
            " extracting: SOTS/indoor/hazy/1432_2.png  \n",
            " extracting: SOTS/indoor/hazy/1432_3.png  \n",
            " extracting: SOTS/indoor/hazy/1432_4.png  \n",
            " extracting: SOTS/indoor/hazy/1432_5.png  \n",
            " extracting: SOTS/indoor/hazy/1432_6.png  \n",
            " extracting: SOTS/indoor/hazy/1432_7.png  \n",
            " extracting: SOTS/indoor/hazy/1432_8.png  \n",
            " extracting: SOTS/indoor/hazy/1432_9.png  \n",
            " extracting: SOTS/indoor/hazy/1433_1.png  \n",
            " extracting: SOTS/indoor/hazy/1433_10.png  \n",
            " extracting: SOTS/indoor/hazy/1433_2.png  \n",
            " extracting: SOTS/indoor/hazy/1433_3.png  \n",
            " extracting: SOTS/indoor/hazy/1433_4.png  \n",
            " extracting: SOTS/indoor/hazy/1433_5.png  \n",
            " extracting: SOTS/indoor/hazy/1433_6.png  \n",
            " extracting: SOTS/indoor/hazy/1433_7.png  \n",
            " extracting: SOTS/indoor/hazy/1433_8.png  \n",
            " extracting: SOTS/indoor/hazy/1433_9.png  \n",
            " extracting: SOTS/indoor/hazy/1434_1.png  \n",
            " extracting: SOTS/indoor/hazy/1434_10.png  \n",
            " extracting: SOTS/indoor/hazy/1434_2.png  \n",
            " extracting: SOTS/indoor/hazy/1434_3.png  \n",
            " extracting: SOTS/indoor/hazy/1434_4.png  \n",
            " extracting: SOTS/indoor/hazy/1434_5.png  \n",
            " extracting: SOTS/indoor/hazy/1434_6.png  \n",
            " extracting: SOTS/indoor/hazy/1434_7.png  \n",
            " extracting: SOTS/indoor/hazy/1434_8.png  \n",
            " extracting: SOTS/indoor/hazy/1434_9.png  \n",
            " extracting: SOTS/indoor/hazy/1435_1.png  \n",
            " extracting: SOTS/indoor/hazy/1435_10.png  \n",
            " extracting: SOTS/indoor/hazy/1435_2.png  \n",
            " extracting: SOTS/indoor/hazy/1435_3.png  \n",
            " extracting: SOTS/indoor/hazy/1435_4.png  \n",
            " extracting: SOTS/indoor/hazy/1435_5.png  \n",
            " extracting: SOTS/indoor/hazy/1435_6.png  \n",
            " extracting: SOTS/indoor/hazy/1435_7.png  \n",
            " extracting: SOTS/indoor/hazy/1435_8.png  \n",
            " extracting: SOTS/indoor/hazy/1435_9.png  \n",
            " extracting: SOTS/indoor/hazy/1436_1.png  \n",
            " extracting: SOTS/indoor/hazy/1436_10.png  \n",
            " extracting: SOTS/indoor/hazy/1436_2.png  \n",
            " extracting: SOTS/indoor/hazy/1436_3.png  \n",
            " extracting: SOTS/indoor/hazy/1436_4.png  \n",
            " extracting: SOTS/indoor/hazy/1436_5.png  \n",
            " extracting: SOTS/indoor/hazy/1436_6.png  \n",
            " extracting: SOTS/indoor/hazy/1436_7.png  \n",
            " extracting: SOTS/indoor/hazy/1436_8.png  \n",
            " extracting: SOTS/indoor/hazy/1436_9.png  \n",
            " extracting: SOTS/indoor/hazy/1437_1.png  \n",
            " extracting: SOTS/indoor/hazy/1437_10.png  \n",
            " extracting: SOTS/indoor/hazy/1437_2.png  \n",
            " extracting: SOTS/indoor/hazy/1437_3.png  \n",
            " extracting: SOTS/indoor/hazy/1437_4.png  \n",
            " extracting: SOTS/indoor/hazy/1437_5.png  \n",
            " extracting: SOTS/indoor/hazy/1437_6.png  \n",
            " extracting: SOTS/indoor/hazy/1437_7.png  \n",
            " extracting: SOTS/indoor/hazy/1437_8.png  \n",
            " extracting: SOTS/indoor/hazy/1437_9.png  \n",
            " extracting: SOTS/indoor/hazy/1438_1.png  \n",
            " extracting: SOTS/indoor/hazy/1438_10.png  \n",
            " extracting: SOTS/indoor/hazy/1438_2.png  \n",
            " extracting: SOTS/indoor/hazy/1438_3.png  \n",
            " extracting: SOTS/indoor/hazy/1438_4.png  \n",
            " extracting: SOTS/indoor/hazy/1438_5.png  \n",
            " extracting: SOTS/indoor/hazy/1438_6.png  \n",
            " extracting: SOTS/indoor/hazy/1438_7.png  \n",
            " extracting: SOTS/indoor/hazy/1438_8.png  \n",
            " extracting: SOTS/indoor/hazy/1438_9.png  \n",
            " extracting: SOTS/indoor/hazy/1439_1.png  \n",
            " extracting: SOTS/indoor/hazy/1439_10.png  \n",
            " extracting: SOTS/indoor/hazy/1439_2.png  \n",
            " extracting: SOTS/indoor/hazy/1439_3.png  \n",
            " extracting: SOTS/indoor/hazy/1439_4.png  \n",
            " extracting: SOTS/indoor/hazy/1439_5.png  \n",
            " extracting: SOTS/indoor/hazy/1439_6.png  \n",
            " extracting: SOTS/indoor/hazy/1439_7.png  \n",
            " extracting: SOTS/indoor/hazy/1439_8.png  \n",
            " extracting: SOTS/indoor/hazy/1439_9.png  \n",
            " extracting: SOTS/indoor/hazy/1440_1.png  \n",
            " extracting: SOTS/indoor/hazy/1440_10.png  \n",
            " extracting: SOTS/indoor/hazy/1440_2.png  \n",
            " extracting: SOTS/indoor/hazy/1440_3.png  \n",
            " extracting: SOTS/indoor/hazy/1440_4.png  \n",
            " extracting: SOTS/indoor/hazy/1440_5.png  \n",
            " extracting: SOTS/indoor/hazy/1440_6.png  \n",
            " extracting: SOTS/indoor/hazy/1440_7.png  \n",
            " extracting: SOTS/indoor/hazy/1440_8.png  \n",
            " extracting: SOTS/indoor/hazy/1440_9.png  \n",
            " extracting: SOTS/indoor/hazy/1441_1.png  \n",
            " extracting: SOTS/indoor/hazy/1441_10.png  \n",
            " extracting: SOTS/indoor/hazy/1441_2.png  \n",
            " extracting: SOTS/indoor/hazy/1441_3.png  \n",
            " extracting: SOTS/indoor/hazy/1441_4.png  \n",
            " extracting: SOTS/indoor/hazy/1441_5.png  \n",
            " extracting: SOTS/indoor/hazy/1441_6.png  \n",
            " extracting: SOTS/indoor/hazy/1441_7.png  \n",
            " extracting: SOTS/indoor/hazy/1441_8.png  \n",
            " extracting: SOTS/indoor/hazy/1441_9.png  \n",
            " extracting: SOTS/indoor/hazy/1442_1.png  \n",
            " extracting: SOTS/indoor/hazy/1442_10.png  \n",
            " extracting: SOTS/indoor/hazy/1442_2.png  \n",
            " extracting: SOTS/indoor/hazy/1442_3.png  \n",
            " extracting: SOTS/indoor/hazy/1442_4.png  \n",
            " extracting: SOTS/indoor/hazy/1442_5.png  \n",
            " extracting: SOTS/indoor/hazy/1442_6.png  \n",
            " extracting: SOTS/indoor/hazy/1442_7.png  \n",
            " extracting: SOTS/indoor/hazy/1442_8.png  \n",
            " extracting: SOTS/indoor/hazy/1442_9.png  \n",
            " extracting: SOTS/indoor/hazy/1443_1.png  \n",
            " extracting: SOTS/indoor/hazy/1443_10.png  \n",
            " extracting: SOTS/indoor/hazy/1443_2.png  \n",
            " extracting: SOTS/indoor/hazy/1443_3.png  \n",
            " extracting: SOTS/indoor/hazy/1443_4.png  \n",
            " extracting: SOTS/indoor/hazy/1443_5.png  \n",
            " extracting: SOTS/indoor/hazy/1443_6.png  \n",
            " extracting: SOTS/indoor/hazy/1443_7.png  \n",
            " extracting: SOTS/indoor/hazy/1443_8.png  \n",
            " extracting: SOTS/indoor/hazy/1443_9.png  \n",
            " extracting: SOTS/indoor/hazy/1444_1.png  \n",
            " extracting: SOTS/indoor/hazy/1444_10.png  \n",
            " extracting: SOTS/indoor/hazy/1444_2.png  \n",
            " extracting: SOTS/indoor/hazy/1444_3.png  \n",
            " extracting: SOTS/indoor/hazy/1444_4.png  \n",
            " extracting: SOTS/indoor/hazy/1444_5.png  \n",
            " extracting: SOTS/indoor/hazy/1444_6.png  \n",
            " extracting: SOTS/indoor/hazy/1444_7.png  \n",
            " extracting: SOTS/indoor/hazy/1444_8.png  \n",
            " extracting: SOTS/indoor/hazy/1444_9.png  \n",
            " extracting: SOTS/indoor/hazy/1445_1.png  \n",
            " extracting: SOTS/indoor/hazy/1445_10.png  \n",
            " extracting: SOTS/indoor/hazy/1445_2.png  \n",
            " extracting: SOTS/indoor/hazy/1445_3.png  \n",
            " extracting: SOTS/indoor/hazy/1445_4.png  \n",
            " extracting: SOTS/indoor/hazy/1445_5.png  \n",
            " extracting: SOTS/indoor/hazy/1445_6.png  \n",
            " extracting: SOTS/indoor/hazy/1445_7.png  \n",
            " extracting: SOTS/indoor/hazy/1445_8.png  \n",
            " extracting: SOTS/indoor/hazy/1445_9.png  \n",
            " extracting: SOTS/indoor/hazy/1446_1.png  \n",
            " extracting: SOTS/indoor/hazy/1446_10.png  \n",
            " extracting: SOTS/indoor/hazy/1446_2.png  \n",
            " extracting: SOTS/indoor/hazy/1446_3.png  \n",
            " extracting: SOTS/indoor/hazy/1446_4.png  \n",
            " extracting: SOTS/indoor/hazy/1446_5.png  \n",
            " extracting: SOTS/indoor/hazy/1446_6.png  \n",
            " extracting: SOTS/indoor/hazy/1446_7.png  \n",
            " extracting: SOTS/indoor/hazy/1446_8.png  \n",
            " extracting: SOTS/indoor/hazy/1446_9.png  \n",
            " extracting: SOTS/indoor/hazy/1447_1.png  \n",
            " extracting: SOTS/indoor/hazy/1447_10.png  \n",
            " extracting: SOTS/indoor/hazy/1447_2.png  \n",
            " extracting: SOTS/indoor/hazy/1447_3.png  \n",
            " extracting: SOTS/indoor/hazy/1447_4.png  \n",
            " extracting: SOTS/indoor/hazy/1447_5.png  \n",
            " extracting: SOTS/indoor/hazy/1447_6.png  \n",
            " extracting: SOTS/indoor/hazy/1447_7.png  \n",
            " extracting: SOTS/indoor/hazy/1447_8.png  \n",
            " extracting: SOTS/indoor/hazy/1447_9.png  \n",
            " extracting: SOTS/indoor/hazy/1448_1.png  \n",
            " extracting: SOTS/indoor/hazy/1448_10.png  \n",
            " extracting: SOTS/indoor/hazy/1448_2.png  \n",
            " extracting: SOTS/indoor/hazy/1448_3.png  \n",
            " extracting: SOTS/indoor/hazy/1448_4.png  \n",
            " extracting: SOTS/indoor/hazy/1448_5.png  \n",
            " extracting: SOTS/indoor/hazy/1448_6.png  \n",
            " extracting: SOTS/indoor/hazy/1448_7.png  \n",
            " extracting: SOTS/indoor/hazy/1448_8.png  \n",
            " extracting: SOTS/indoor/hazy/1448_9.png  \n",
            " extracting: SOTS/indoor/hazy/1449_1.png  \n",
            " extracting: SOTS/indoor/hazy/1449_10.png  \n",
            " extracting: SOTS/indoor/hazy/1449_2.png  \n",
            " extracting: SOTS/indoor/hazy/1449_3.png  \n",
            " extracting: SOTS/indoor/hazy/1449_4.png  \n",
            " extracting: SOTS/indoor/hazy/1449_5.png  \n",
            " extracting: SOTS/indoor/hazy/1449_6.png  \n",
            " extracting: SOTS/indoor/hazy/1449_7.png  \n",
            " extracting: SOTS/indoor/hazy/1449_8.png  \n",
            " extracting: SOTS/indoor/hazy/1449_9.png  \n",
            "   creating: SOTS/outdoor/\n",
            "   creating: SOTS/outdoor/gt/\n",
            " extracting: SOTS/outdoor/gt/0001.png  \n",
            "  inflating: SOTS/outdoor/gt/0002.png  \n",
            "  inflating: SOTS/outdoor/gt/0003.png  \n",
            "  inflating: SOTS/outdoor/gt/0004.png  \n",
            " extracting: SOTS/outdoor/gt/0006.png  \n",
            "  inflating: SOTS/outdoor/gt/0007.png  \n",
            "  inflating: SOTS/outdoor/gt/0009.png  \n",
            "  inflating: SOTS/outdoor/gt/0010.png  \n",
            "  inflating: SOTS/outdoor/gt/0011.png  \n",
            "  inflating: SOTS/outdoor/gt/0014.png  \n",
            "  inflating: SOTS/outdoor/gt/0016.png  \n",
            "  inflating: SOTS/outdoor/gt/0017.png  \n",
            "  inflating: SOTS/outdoor/gt/0018.png  \n",
            "  inflating: SOTS/outdoor/gt/0019.png  \n",
            " extracting: SOTS/outdoor/gt/0021.png  \n",
            "  inflating: SOTS/outdoor/gt/0022.png  \n",
            " extracting: SOTS/outdoor/gt/0023.png  \n",
            " extracting: SOTS/outdoor/gt/0024.png  \n",
            " extracting: SOTS/outdoor/gt/0025.png  \n",
            "  inflating: SOTS/outdoor/gt/0026.png  \n",
            " extracting: SOTS/outdoor/gt/0029.png  \n",
            "  inflating: SOTS/outdoor/gt/0030.png  \n",
            "  inflating: SOTS/outdoor/gt/0033.png  \n",
            "  inflating: SOTS/outdoor/gt/0034.png  \n",
            "  inflating: SOTS/outdoor/gt/0036.png  \n",
            "  inflating: SOTS/outdoor/gt/0039.png  \n",
            "  inflating: SOTS/outdoor/gt/0040.png  \n",
            "  inflating: SOTS/outdoor/gt/0042.png  \n",
            "  inflating: SOTS/outdoor/gt/0045.png  \n",
            " extracting: SOTS/outdoor/gt/0046.png  \n",
            "  inflating: SOTS/outdoor/gt/0047.png  \n",
            "  inflating: SOTS/outdoor/gt/0048.png  \n",
            "  inflating: SOTS/outdoor/gt/0049.png  \n",
            "  inflating: SOTS/outdoor/gt/0051.png  \n",
            "  inflating: SOTS/outdoor/gt/0052.png  \n",
            "  inflating: SOTS/outdoor/gt/0053.png  \n",
            "  inflating: SOTS/outdoor/gt/0054.png  \n",
            "  inflating: SOTS/outdoor/gt/0055.png  \n",
            "  inflating: SOTS/outdoor/gt/0056.png  \n",
            "  inflating: SOTS/outdoor/gt/0057.png  \n",
            "  inflating: SOTS/outdoor/gt/0058.png  \n",
            "  inflating: SOTS/outdoor/gt/0059.png  \n",
            "  inflating: SOTS/outdoor/gt/0060.png  \n",
            "  inflating: SOTS/outdoor/gt/0061.png  \n",
            " extracting: SOTS/outdoor/gt/0062.png  \n",
            "  inflating: SOTS/outdoor/gt/0063.png  \n",
            "  inflating: SOTS/outdoor/gt/0064.png  \n",
            "  inflating: SOTS/outdoor/gt/0065.png  \n",
            "  inflating: SOTS/outdoor/gt/0066.png  \n",
            "  inflating: SOTS/outdoor/gt/0068.png  \n",
            "  inflating: SOTS/outdoor/gt/0069.png  \n",
            "  inflating: SOTS/outdoor/gt/0070.png  \n",
            " extracting: SOTS/outdoor/gt/0071.png  \n",
            "  inflating: SOTS/outdoor/gt/0072.png  \n",
            "  inflating: SOTS/outdoor/gt/0073.png  \n",
            "  inflating: SOTS/outdoor/gt/0074.png  \n",
            "  inflating: SOTS/outdoor/gt/0075.png  \n",
            "  inflating: SOTS/outdoor/gt/0076.png  \n",
            "  inflating: SOTS/outdoor/gt/0077.png  \n",
            "  inflating: SOTS/outdoor/gt/0079.png  \n",
            "  inflating: SOTS/outdoor/gt/0081.png  \n",
            "  inflating: SOTS/outdoor/gt/0082.png  \n",
            "  inflating: SOTS/outdoor/gt/0083.png  \n",
            "  inflating: SOTS/outdoor/gt/0084.png  \n",
            "  inflating: SOTS/outdoor/gt/0085.png  \n",
            "  inflating: SOTS/outdoor/gt/0086.png  \n",
            "  inflating: SOTS/outdoor/gt/0087.png  \n",
            "  inflating: SOTS/outdoor/gt/0088.png  \n",
            "  inflating: SOTS/outdoor/gt/0089.png  \n",
            "  inflating: SOTS/outdoor/gt/0090.png  \n",
            "  inflating: SOTS/outdoor/gt/0091.png  \n",
            "  inflating: SOTS/outdoor/gt/0092.png  \n",
            "  inflating: SOTS/outdoor/gt/0093.png  \n",
            "  inflating: SOTS/outdoor/gt/0094.png  \n",
            "  inflating: SOTS/outdoor/gt/0095.png  \n",
            "  inflating: SOTS/outdoor/gt/0096.png  \n",
            "  inflating: SOTS/outdoor/gt/0097.png  \n",
            "  inflating: SOTS/outdoor/gt/0098.png  \n",
            "  inflating: SOTS/outdoor/gt/0099.png  \n",
            "  inflating: SOTS/outdoor/gt/0100.png  \n",
            "  inflating: SOTS/outdoor/gt/0101.png  \n",
            " extracting: SOTS/outdoor/gt/0102.png  \n",
            "  inflating: SOTS/outdoor/gt/0104.png  \n",
            "  inflating: SOTS/outdoor/gt/0105.png  \n",
            "  inflating: SOTS/outdoor/gt/0106.png  \n",
            "  inflating: SOTS/outdoor/gt/0107.png  \n",
            "  inflating: SOTS/outdoor/gt/0108.png  \n",
            "  inflating: SOTS/outdoor/gt/0109.png  \n",
            "  inflating: SOTS/outdoor/gt/0110.png  \n",
            " extracting: SOTS/outdoor/gt/0111.png  \n",
            "  inflating: SOTS/outdoor/gt/0112.png  \n",
            "  inflating: SOTS/outdoor/gt/0113.png  \n",
            "  inflating: SOTS/outdoor/gt/0115.png  \n",
            "  inflating: SOTS/outdoor/gt/0116.png  \n",
            "  inflating: SOTS/outdoor/gt/0117.png  \n",
            "  inflating: SOTS/outdoor/gt/0118.png  \n",
            "  inflating: SOTS/outdoor/gt/0119.png  \n",
            "  inflating: SOTS/outdoor/gt/0120.png  \n",
            "  inflating: SOTS/outdoor/gt/0121.png  \n",
            "  inflating: SOTS/outdoor/gt/0123.png  \n",
            " extracting: SOTS/outdoor/gt/0125.png  \n",
            "  inflating: SOTS/outdoor/gt/0126.png  \n",
            "  inflating: SOTS/outdoor/gt/0127.png  \n",
            "  inflating: SOTS/outdoor/gt/0129.png  \n",
            "  inflating: SOTS/outdoor/gt/0131.png  \n",
            "  inflating: SOTS/outdoor/gt/0132.png  \n",
            "  inflating: SOTS/outdoor/gt/0133.png  \n",
            "  inflating: SOTS/outdoor/gt/0134.png  \n",
            " extracting: SOTS/outdoor/gt/0135.png  \n",
            " extracting: SOTS/outdoor/gt/0137.png  \n",
            " extracting: SOTS/outdoor/gt/0138.png  \n",
            " extracting: SOTS/outdoor/gt/0139.png  \n",
            " extracting: SOTS/outdoor/gt/0140.png  \n",
            " extracting: SOTS/outdoor/gt/0141.png  \n",
            "  inflating: SOTS/outdoor/gt/0142.png  \n",
            " extracting: SOTS/outdoor/gt/0143.png  \n",
            " extracting: SOTS/outdoor/gt/0145.png  \n",
            "  inflating: SOTS/outdoor/gt/0146.png  \n",
            "  inflating: SOTS/outdoor/gt/0147.png  \n",
            " extracting: SOTS/outdoor/gt/0148.png  \n",
            " extracting: SOTS/outdoor/gt/0149.png  \n",
            "  inflating: SOTS/outdoor/gt/0150.png  \n",
            "  inflating: SOTS/outdoor/gt/0151.png  \n",
            " extracting: SOTS/outdoor/gt/0152.png  \n",
            " extracting: SOTS/outdoor/gt/0153.png  \n",
            "  inflating: SOTS/outdoor/gt/0154.png  \n",
            "  inflating: SOTS/outdoor/gt/0155.png  \n",
            "  inflating: SOTS/outdoor/gt/0157.png  \n",
            "  inflating: SOTS/outdoor/gt/0158.png  \n",
            " extracting: SOTS/outdoor/gt/0160.png  \n",
            "  inflating: SOTS/outdoor/gt/0161.png  \n",
            "  inflating: SOTS/outdoor/gt/0162.png  \n",
            "  inflating: SOTS/outdoor/gt/0163.png  \n",
            "  inflating: SOTS/outdoor/gt/0164.png  \n",
            "  inflating: SOTS/outdoor/gt/0165.png  \n",
            "  inflating: SOTS/outdoor/gt/0166.png  \n",
            "  inflating: SOTS/outdoor/gt/0167.png  \n",
            "  inflating: SOTS/outdoor/gt/0168.png  \n",
            "  inflating: SOTS/outdoor/gt/0169.png  \n",
            "  inflating: SOTS/outdoor/gt/0170.png  \n",
            "  inflating: SOTS/outdoor/gt/0171.png  \n",
            "  inflating: SOTS/outdoor/gt/0172.png  \n",
            "  inflating: SOTS/outdoor/gt/0174.png  \n",
            "  inflating: SOTS/outdoor/gt/0175.png  \n",
            "  inflating: SOTS/outdoor/gt/0176.png  \n",
            "  inflating: SOTS/outdoor/gt/0178.png  \n",
            "  inflating: SOTS/outdoor/gt/0179.png  \n",
            "  inflating: SOTS/outdoor/gt/0180.png  \n",
            "  inflating: SOTS/outdoor/gt/0181.png  \n",
            "  inflating: SOTS/outdoor/gt/0182.png  \n",
            "  inflating: SOTS/outdoor/gt/0183.png  \n",
            "  inflating: SOTS/outdoor/gt/0184.png  \n",
            " extracting: SOTS/outdoor/gt/0185.png  \n",
            "  inflating: SOTS/outdoor/gt/0187.png  \n",
            "  inflating: SOTS/outdoor/gt/0188.png  \n",
            " extracting: SOTS/outdoor/gt/0189.png  \n",
            "  inflating: SOTS/outdoor/gt/0191.png  \n",
            " extracting: SOTS/outdoor/gt/0194.png  \n",
            "  inflating: SOTS/outdoor/gt/0195.png  \n",
            "  inflating: SOTS/outdoor/gt/0196.png  \n",
            " extracting: SOTS/outdoor/gt/0197.png  \n",
            "  inflating: SOTS/outdoor/gt/0198.png  \n",
            " extracting: SOTS/outdoor/gt/0199.png  \n",
            "  inflating: SOTS/outdoor/gt/0200.png  \n",
            "  inflating: SOTS/outdoor/gt/0201.png  \n",
            " extracting: SOTS/outdoor/gt/0202.png  \n",
            "  inflating: SOTS/outdoor/gt/0204.png  \n",
            "  inflating: SOTS/outdoor/gt/0205.png  \n",
            "  inflating: SOTS/outdoor/gt/0206.png  \n",
            "  inflating: SOTS/outdoor/gt/0207.png  \n",
            "  inflating: SOTS/outdoor/gt/0208.png  \n",
            "  inflating: SOTS/outdoor/gt/0209.png  \n",
            "  inflating: SOTS/outdoor/gt/0210.png  \n",
            "  inflating: SOTS/outdoor/gt/0212.png  \n",
            "  inflating: SOTS/outdoor/gt/0213.png  \n",
            "  inflating: SOTS/outdoor/gt/0215.png  \n",
            "  inflating: SOTS/outdoor/gt/0216.png  \n",
            "  inflating: SOTS/outdoor/gt/0217.png  \n",
            "  inflating: SOTS/outdoor/gt/0218.png  \n",
            "  inflating: SOTS/outdoor/gt/0219.png  \n",
            "  inflating: SOTS/outdoor/gt/0220.png  \n",
            "  inflating: SOTS/outdoor/gt/0222.png  \n",
            "  inflating: SOTS/outdoor/gt/0223.png  \n",
            "  inflating: SOTS/outdoor/gt/0225.png  \n",
            "  inflating: SOTS/outdoor/gt/0226.png  \n",
            "  inflating: SOTS/outdoor/gt/0228.png  \n",
            "  inflating: SOTS/outdoor/gt/0230.png  \n",
            "  inflating: SOTS/outdoor/gt/0233.png  \n",
            "  inflating: SOTS/outdoor/gt/0235.png  \n",
            "  inflating: SOTS/outdoor/gt/0237.png  \n",
            "  inflating: SOTS/outdoor/gt/0238.png  \n",
            "  inflating: SOTS/outdoor/gt/0239.png  \n",
            "  inflating: SOTS/outdoor/gt/0240.png  \n",
            "  inflating: SOTS/outdoor/gt/0242.png  \n",
            "  inflating: SOTS/outdoor/gt/0243.png  \n",
            "  inflating: SOTS/outdoor/gt/0244.png  \n",
            "  inflating: SOTS/outdoor/gt/0245.png  \n",
            " extracting: SOTS/outdoor/gt/0246.png  \n",
            " extracting: SOTS/outdoor/gt/0248.png  \n",
            " extracting: SOTS/outdoor/gt/0249.png  \n",
            " extracting: SOTS/outdoor/gt/0251.png  \n",
            "  inflating: SOTS/outdoor/gt/0253.png  \n",
            " extracting: SOTS/outdoor/gt/0255.png  \n",
            " extracting: SOTS/outdoor/gt/0256.png  \n",
            " extracting: SOTS/outdoor/gt/0258.png  \n",
            " extracting: SOTS/outdoor/gt/0259.png  \n",
            " extracting: SOTS/outdoor/gt/0260.png  \n",
            " extracting: SOTS/outdoor/gt/0261.png  \n",
            " extracting: SOTS/outdoor/gt/0262.png  \n",
            " extracting: SOTS/outdoor/gt/0263.png  \n",
            " extracting: SOTS/outdoor/gt/0264.png  \n",
            "  inflating: SOTS/outdoor/gt/0266.png  \n",
            "  inflating: SOTS/outdoor/gt/0267.png  \n",
            " extracting: SOTS/outdoor/gt/0268.png  \n",
            "  inflating: SOTS/outdoor/gt/0269.png  \n",
            "  inflating: SOTS/outdoor/gt/0270.png  \n",
            "  inflating: SOTS/outdoor/gt/0271.png  \n",
            "  inflating: SOTS/outdoor/gt/0272.png  \n",
            "  inflating: SOTS/outdoor/gt/0273.png  \n",
            "  inflating: SOTS/outdoor/gt/0274.png  \n",
            "  inflating: SOTS/outdoor/gt/0275.png  \n",
            "  inflating: SOTS/outdoor/gt/0276.png  \n",
            "  inflating: SOTS/outdoor/gt/0277.png  \n",
            "  inflating: SOTS/outdoor/gt/0279.png  \n",
            "  inflating: SOTS/outdoor/gt/0280.png  \n",
            "  inflating: SOTS/outdoor/gt/0281.png  \n",
            "  inflating: SOTS/outdoor/gt/0282.png  \n",
            "  inflating: SOTS/outdoor/gt/0283.png  \n",
            "  inflating: SOTS/outdoor/gt/0284.png  \n",
            "  inflating: SOTS/outdoor/gt/0285.png  \n",
            "  inflating: SOTS/outdoor/gt/0286.png  \n",
            "  inflating: SOTS/outdoor/gt/0287.png  \n",
            "  inflating: SOTS/outdoor/gt/0288.png  \n",
            "  inflating: SOTS/outdoor/gt/0290.png  \n",
            "  inflating: SOTS/outdoor/gt/0291.png  \n",
            "  inflating: SOTS/outdoor/gt/0292.png  \n",
            "  inflating: SOTS/outdoor/gt/0294.png  \n",
            "  inflating: SOTS/outdoor/gt/0295.png  \n",
            "  inflating: SOTS/outdoor/gt/0296.png  \n",
            " extracting: SOTS/outdoor/gt/0297.png  \n",
            "  inflating: SOTS/outdoor/gt/0298.png  \n",
            "  inflating: SOTS/outdoor/gt/0299.png  \n",
            "  inflating: SOTS/outdoor/gt/0300.png  \n",
            "  inflating: SOTS/outdoor/gt/0302.png  \n",
            "  inflating: SOTS/outdoor/gt/0303.png  \n",
            "  inflating: SOTS/outdoor/gt/0304.png  \n",
            "  inflating: SOTS/outdoor/gt/0305.png  \n",
            "  inflating: SOTS/outdoor/gt/0306.png  \n",
            "  inflating: SOTS/outdoor/gt/0307.png  \n",
            "  inflating: SOTS/outdoor/gt/0308.png  \n",
            "  inflating: SOTS/outdoor/gt/0309.png  \n",
            "  inflating: SOTS/outdoor/gt/0311.png  \n",
            "  inflating: SOTS/outdoor/gt/0312.png  \n",
            "  inflating: SOTS/outdoor/gt/0313.png  \n",
            "  inflating: SOTS/outdoor/gt/0314.png  \n",
            " extracting: SOTS/outdoor/gt/0315.png  \n",
            "  inflating: SOTS/outdoor/gt/0316.png  \n",
            "  inflating: SOTS/outdoor/gt/0317.png  \n",
            "  inflating: SOTS/outdoor/gt/0318.png  \n",
            "  inflating: SOTS/outdoor/gt/0319.png  \n",
            "  inflating: SOTS/outdoor/gt/0320.png  \n",
            "  inflating: SOTS/outdoor/gt/0321.png  \n",
            "  inflating: SOTS/outdoor/gt/0323.png  \n",
            "  inflating: SOTS/outdoor/gt/0324.png  \n",
            "  inflating: SOTS/outdoor/gt/0325.png  \n",
            "  inflating: SOTS/outdoor/gt/0326.png  \n",
            "  inflating: SOTS/outdoor/gt/0327.png  \n",
            "  inflating: SOTS/outdoor/gt/0329.png  \n",
            "  inflating: SOTS/outdoor/gt/0330.png  \n",
            "  inflating: SOTS/outdoor/gt/0331.png  \n",
            "  inflating: SOTS/outdoor/gt/0332.png  \n",
            "  inflating: SOTS/outdoor/gt/0333.png  \n",
            "  inflating: SOTS/outdoor/gt/0334.png  \n",
            "  inflating: SOTS/outdoor/gt/0335.png  \n",
            "  inflating: SOTS/outdoor/gt/0337.png  \n",
            "  inflating: SOTS/outdoor/gt/0338.png  \n",
            "  inflating: SOTS/outdoor/gt/0340.png  \n",
            "  inflating: SOTS/outdoor/gt/0341.png  \n",
            "  inflating: SOTS/outdoor/gt/0342.png  \n",
            "  inflating: SOTS/outdoor/gt/0343.png  \n",
            "  inflating: SOTS/outdoor/gt/0344.png  \n",
            "  inflating: SOTS/outdoor/gt/0345.png  \n",
            " extracting: SOTS/outdoor/gt/0346.png  \n",
            " extracting: SOTS/outdoor/gt/0348.png  \n",
            " extracting: SOTS/outdoor/gt/0349.png  \n",
            "  inflating: SOTS/outdoor/gt/0350.png  \n",
            "  inflating: SOTS/outdoor/gt/0351.png  \n",
            "  inflating: SOTS/outdoor/gt/0353.png  \n",
            " extracting: SOTS/outdoor/gt/0354.png  \n",
            " extracting: SOTS/outdoor/gt/0355.png  \n",
            " extracting: SOTS/outdoor/gt/0356.png  \n",
            " extracting: SOTS/outdoor/gt/0357.png  \n",
            "  inflating: SOTS/outdoor/gt/0359.png  \n",
            "  inflating: SOTS/outdoor/gt/0361.png  \n",
            "  inflating: SOTS/outdoor/gt/0362.png  \n",
            "  inflating: SOTS/outdoor/gt/0364.png  \n",
            "  inflating: SOTS/outdoor/gt/0366.png  \n",
            " extracting: SOTS/outdoor/gt/0369.png  \n",
            " extracting: SOTS/outdoor/gt/0371.png  \n",
            "  inflating: SOTS/outdoor/gt/0372.png  \n",
            "  inflating: SOTS/outdoor/gt/0375.png  \n",
            "  inflating: SOTS/outdoor/gt/0380.png  \n",
            "  inflating: SOTS/outdoor/gt/0382.png  \n",
            " extracting: SOTS/outdoor/gt/0383.png  \n",
            " extracting: SOTS/outdoor/gt/0385.png  \n",
            " extracting: SOTS/outdoor/gt/0386.png  \n",
            " extracting: SOTS/outdoor/gt/0388.png  \n",
            " extracting: SOTS/outdoor/gt/0390.png  \n",
            " extracting: SOTS/outdoor/gt/0391.png  \n",
            "  inflating: SOTS/outdoor/gt/0392.png  \n",
            "  inflating: SOTS/outdoor/gt/0393.png  \n",
            "  inflating: SOTS/outdoor/gt/0395.png  \n",
            "  inflating: SOTS/outdoor/gt/0397.png  \n",
            "  inflating: SOTS/outdoor/gt/0399.png  \n",
            "  inflating: SOTS/outdoor/gt/0400.png  \n",
            "  inflating: SOTS/outdoor/gt/0402.png  \n",
            "  inflating: SOTS/outdoor/gt/0404.png  \n",
            "  inflating: SOTS/outdoor/gt/0405.png  \n",
            "  inflating: SOTS/outdoor/gt/0406.png  \n",
            "  inflating: SOTS/outdoor/gt/0407.png  \n",
            "  inflating: SOTS/outdoor/gt/0409.png  \n",
            "  inflating: SOTS/outdoor/gt/0411.png  \n",
            "  inflating: SOTS/outdoor/gt/0413.png  \n",
            "  inflating: SOTS/outdoor/gt/1001.png  \n",
            " extracting: SOTS/outdoor/gt/1002.png  \n",
            "  inflating: SOTS/outdoor/gt/1005.png  \n",
            " extracting: SOTS/outdoor/gt/1006.png  \n",
            " extracting: SOTS/outdoor/gt/1007.png  \n",
            "  inflating: SOTS/outdoor/gt/1008.png  \n",
            "  inflating: SOTS/outdoor/gt/1009.png  \n",
            "  inflating: SOTS/outdoor/gt/1010.png  \n",
            "  inflating: SOTS/outdoor/gt/1011.png  \n",
            "  inflating: SOTS/outdoor/gt/1012.png  \n",
            "  inflating: SOTS/outdoor/gt/1015.png  \n",
            "  inflating: SOTS/outdoor/gt/1016.png  \n",
            "  inflating: SOTS/outdoor/gt/1018.png  \n",
            " extracting: SOTS/outdoor/gt/1020.png  \n",
            " extracting: SOTS/outdoor/gt/1022.png  \n",
            "  inflating: SOTS/outdoor/gt/1027.png  \n",
            " extracting: SOTS/outdoor/gt/1030.png  \n",
            "  inflating: SOTS/outdoor/gt/1034.png  \n",
            "  inflating: SOTS/outdoor/gt/1037.png  \n",
            " extracting: SOTS/outdoor/gt/1038.png  \n",
            "  inflating: SOTS/outdoor/gt/1040.png  \n",
            " extracting: SOTS/outdoor/gt/1042.png  \n",
            " extracting: SOTS/outdoor/gt/1044.png  \n",
            "  inflating: SOTS/outdoor/gt/1046.png  \n",
            "  inflating: SOTS/outdoor/gt/1048.png  \n",
            "  inflating: SOTS/outdoor/gt/1050.png  \n",
            " extracting: SOTS/outdoor/gt/1051.png  \n",
            "  inflating: SOTS/outdoor/gt/1053.png  \n",
            "  inflating: SOTS/outdoor/gt/1055.png  \n",
            "  inflating: SOTS/outdoor/gt/1057.png  \n",
            "  inflating: SOTS/outdoor/gt/1059.png  \n",
            "  inflating: SOTS/outdoor/gt/1060.png  \n",
            "  inflating: SOTS/outdoor/gt/1063.png  \n",
            " extracting: SOTS/outdoor/gt/1709.png  \n",
            "  inflating: SOTS/outdoor/gt/1712.png  \n",
            "  inflating: SOTS/outdoor/gt/1713.png  \n",
            " extracting: SOTS/outdoor/gt/1714.png  \n",
            "  inflating: SOTS/outdoor/gt/1716.png  \n",
            " extracting: SOTS/outdoor/gt/1718.png  \n",
            "  inflating: SOTS/outdoor/gt/1722.png  \n",
            "  inflating: SOTS/outdoor/gt/1724.png  \n",
            "  inflating: SOTS/outdoor/gt/1726.png  \n",
            "  inflating: SOTS/outdoor/gt/1728.png  \n",
            "  inflating: SOTS/outdoor/gt/1730.png  \n",
            "  inflating: SOTS/outdoor/gt/1731.png  \n",
            "  inflating: SOTS/outdoor/gt/1732.png  \n",
            "  inflating: SOTS/outdoor/gt/1734.png  \n",
            "  inflating: SOTS/outdoor/gt/1736.png  \n",
            "  inflating: SOTS/outdoor/gt/1738.png  \n",
            "  inflating: SOTS/outdoor/gt/1741.png  \n",
            "  inflating: SOTS/outdoor/gt/1742.png  \n",
            "  inflating: SOTS/outdoor/gt/1743.png  \n",
            "  inflating: SOTS/outdoor/gt/1744.png  \n",
            "  inflating: SOTS/outdoor/gt/1747.png  \n",
            "  inflating: SOTS/outdoor/gt/1749.png  \n",
            "  inflating: SOTS/outdoor/gt/1753.png  \n",
            "  inflating: SOTS/outdoor/gt/1756.png  \n",
            "  inflating: SOTS/outdoor/gt/1757.png  \n",
            "  inflating: SOTS/outdoor/gt/1759.png  \n",
            "  inflating: SOTS/outdoor/gt/1760.png  \n",
            "  inflating: SOTS/outdoor/gt/1765.png  \n",
            " extracting: SOTS/outdoor/gt/1771.png  \n",
            " extracting: SOTS/outdoor/gt/1774.png  \n",
            "  inflating: SOTS/outdoor/gt/1778.png  \n",
            "  inflating: SOTS/outdoor/gt/1781.png  \n",
            "  inflating: SOTS/outdoor/gt/1784.png  \n",
            "  inflating: SOTS/outdoor/gt/1790.png  \n",
            "  inflating: SOTS/outdoor/gt/1800.png  \n",
            "  inflating: SOTS/outdoor/gt/1805.png  \n",
            " extracting: SOTS/outdoor/gt/1812.png  \n",
            "  inflating: SOTS/outdoor/gt/1815.png  \n",
            "  inflating: SOTS/outdoor/gt/1818.png  \n",
            " extracting: SOTS/outdoor/gt/1821.png  \n",
            "  inflating: SOTS/outdoor/gt/1822.png  \n",
            " extracting: SOTS/outdoor/gt/1824.png  \n",
            "  inflating: SOTS/outdoor/gt/1826.png  \n",
            "  inflating: SOTS/outdoor/gt/1828.png  \n",
            "  inflating: SOTS/outdoor/gt/1831.png  \n",
            "  inflating: SOTS/outdoor/gt/1832.png  \n",
            "  inflating: SOTS/outdoor/gt/1834.png  \n",
            "  inflating: SOTS/outdoor/gt/1837.png  \n",
            "  inflating: SOTS/outdoor/gt/1839.png  \n",
            "  inflating: SOTS/outdoor/gt/1840.png  \n",
            " extracting: SOTS/outdoor/gt/1843.png  \n",
            "  inflating: SOTS/outdoor/gt/1845.png  \n",
            "  inflating: SOTS/outdoor/gt/1846.png  \n",
            "  inflating: SOTS/outdoor/gt/1848.png  \n",
            "  inflating: SOTS/outdoor/gt/1849.png  \n",
            " extracting: SOTS/outdoor/gt/1851.png  \n",
            "  inflating: SOTS/outdoor/gt/1852.png  \n",
            "  inflating: SOTS/outdoor/gt/1853.png  \n",
            "  inflating: SOTS/outdoor/gt/1855.png  \n",
            "  inflating: SOTS/outdoor/gt/1857.png  \n",
            "  inflating: SOTS/outdoor/gt/1858.png  \n",
            "  inflating: SOTS/outdoor/gt/1859.png  \n",
            "  inflating: SOTS/outdoor/gt/1861.png  \n",
            "  inflating: SOTS/outdoor/gt/1862.png  \n",
            " extracting: SOTS/outdoor/gt/1863.png  \n",
            "  inflating: SOTS/outdoor/gt/1865.png  \n",
            "  inflating: SOTS/outdoor/gt/1867.png  \n",
            "  inflating: SOTS/outdoor/gt/1868.png  \n",
            "  inflating: SOTS/outdoor/gt/1869.png  \n",
            "  inflating: SOTS/outdoor/gt/1871.png  \n",
            "  inflating: SOTS/outdoor/gt/1872.png  \n",
            "  inflating: SOTS/outdoor/gt/1873.png  \n",
            "  inflating: SOTS/outdoor/gt/1874.png  \n",
            "  inflating: SOTS/outdoor/gt/1875.png  \n",
            "  inflating: SOTS/outdoor/gt/1876.png  \n",
            "  inflating: SOTS/outdoor/gt/1877.png  \n",
            "  inflating: SOTS/outdoor/gt/1878.png  \n",
            "  inflating: SOTS/outdoor/gt/1879.png  \n",
            "  inflating: SOTS/outdoor/gt/1880.png  \n",
            "  inflating: SOTS/outdoor/gt/1881.png  \n",
            "  inflating: SOTS/outdoor/gt/1882.png  \n",
            "  inflating: SOTS/outdoor/gt/1883.png  \n",
            "  inflating: SOTS/outdoor/gt/1887.png  \n",
            "  inflating: SOTS/outdoor/gt/1889.png  \n",
            "  inflating: SOTS/outdoor/gt/1891.png  \n",
            "  inflating: SOTS/outdoor/gt/1893.png  \n",
            "  inflating: SOTS/outdoor/gt/1896.png  \n",
            " extracting: SOTS/outdoor/gt/1898.png  \n",
            "  inflating: SOTS/outdoor/gt/1899.png  \n",
            "  inflating: SOTS/outdoor/gt/1900.png  \n",
            "  inflating: SOTS/outdoor/gt/1903.png  \n",
            "  inflating: SOTS/outdoor/gt/1909.png  \n",
            "  inflating: SOTS/outdoor/gt/1913.png  \n",
            "  inflating: SOTS/outdoor/gt/1915.png  \n",
            "  inflating: SOTS/outdoor/gt/1917.png  \n",
            "  inflating: SOTS/outdoor/gt/1919.png  \n",
            "  inflating: SOTS/outdoor/gt/1920.png  \n",
            "  inflating: SOTS/outdoor/gt/1921.png  \n",
            "  inflating: SOTS/outdoor/gt/1923.png  \n",
            "  inflating: SOTS/outdoor/gt/1924.png  \n",
            "  inflating: SOTS/outdoor/gt/1926.png  \n",
            "  inflating: SOTS/outdoor/gt/1927.png  \n",
            "  inflating: SOTS/outdoor/gt/1928.png  \n",
            " extracting: SOTS/outdoor/gt/1930.png  \n",
            "  inflating: SOTS/outdoor/gt/1931.png  \n",
            "  inflating: SOTS/outdoor/gt/1932.png  \n",
            "  inflating: SOTS/outdoor/gt/1933.png  \n",
            "  inflating: SOTS/outdoor/gt/1934.png  \n",
            "  inflating: SOTS/outdoor/gt/1936.png  \n",
            "  inflating: SOTS/outdoor/gt/1938.png  \n",
            "  inflating: SOTS/outdoor/gt/1940.png  \n",
            "  inflating: SOTS/outdoor/gt/1942.png  \n",
            " extracting: SOTS/outdoor/gt/1944.png  \n",
            "  inflating: SOTS/outdoor/gt/1945.png  \n",
            "  inflating: SOTS/outdoor/gt/1947.png  \n",
            "  inflating: SOTS/outdoor/gt/1949.png  \n",
            "  inflating: SOTS/outdoor/gt/1950.png  \n",
            "  inflating: SOTS/outdoor/gt/1953.png  \n",
            "  inflating: SOTS/outdoor/gt/1954.png  \n",
            "  inflating: SOTS/outdoor/gt/1956.png  \n",
            "  inflating: SOTS/outdoor/gt/1958.png  \n",
            "  inflating: SOTS/outdoor/gt/1960.png  \n",
            "  inflating: SOTS/outdoor/gt/1962.png  \n",
            "  inflating: SOTS/outdoor/gt/1964.png  \n",
            "  inflating: SOTS/outdoor/gt/1966.png  \n",
            "  inflating: SOTS/outdoor/gt/1968.png  \n",
            "  inflating: SOTS/outdoor/gt/1970.png  \n",
            " extracting: SOTS/outdoor/gt/1971.png  \n",
            "  inflating: SOTS/outdoor/gt/1973.png  \n",
            " extracting: SOTS/outdoor/gt/1975.png  \n",
            "  inflating: SOTS/outdoor/gt/1977.png  \n",
            "  inflating: SOTS/outdoor/gt/1981.png  \n",
            "  inflating: SOTS/outdoor/gt/1982.png  \n",
            " extracting: SOTS/outdoor/gt/1984.png  \n",
            "  inflating: SOTS/outdoor/gt/1986.png  \n",
            "  inflating: SOTS/outdoor/gt/1988.png  \n",
            "   creating: SOTS/outdoor/hazy/\n",
            "  inflating: SOTS/outdoor/hazy/0001_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0002_0.8_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0003_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0004_0.9_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0006_0.85_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0007_0.9_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0009_0.8_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0010_0.95_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0011_0.95_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0014_0.8_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0016_0.8_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0017_0.9_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0018_0.9_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0019_0.8_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0021_0.8_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0022_0.8_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0023_0.85_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0024_0.85_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0025_0.8_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0026_0.95_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0029_0.85_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0030_0.95_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0033_0.85_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0034_0.9_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0036_1_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0039_0.9_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0040_1_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0042_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0045_0.8_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0046_0.9_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0047_0.9_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0048_0.9_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0049_0.85_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0051_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0051_0.95_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0052_0.9_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0053_0.9_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0054_0.85_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0055_0.85_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0056_0.8_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0057_0.8_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0058_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0059_0.9_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0060_0.9_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0061_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0062_0.9_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0063_0.8_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0064_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0065_0.9_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0066_1_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0068_1_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0069_0.9_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0070_0.9_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0071_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0072_0.9_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0073_0.8_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0074_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0075_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0076_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0076_1_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0077_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0079_0.8_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0081_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0082_0.85_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0083_0.85_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0084_0.95_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0085_0.95_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0086_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0086_0.95_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0087_0.9_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0088_0.9_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0089_0.9_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0090_0.85_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0091_0.85_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0092_0.85_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0093_0.8_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0094_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0095_0.8_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0096_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0097_0.85_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0098_0.85_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0099_0.9_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0100_0.9_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0101_0.9_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0102_0.85_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0104_0.95_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0105_0.95_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0106_0.9_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0107_0.9_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0108_0.8_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0108_1_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0109_0.85_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0110_0.8_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0111_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0112_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0113_0.9_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0115_0.8_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0116_0.9_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0117_0.9_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0118_0.9_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0119_0.85_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0120_0.85_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0121_0.85_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0123_1_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0125_0.8_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0126_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0127_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0129_0.8_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0131_0.95_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0132_1_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0133_1_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0134_0.95_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0135_0.95_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0137_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0138_0.9_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0139_0.85_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0140_0.95_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0141_0.95_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0142_0.95_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0143_1_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0145_0.8_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0146_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0147_1_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0148_0.9_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0149_0.9_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0150_0.8_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0151_0.8_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0152_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0153_0.8_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0154_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0155_1_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0157_0.8_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0158_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0160_0.9_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0161_0.85_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0162_0.95_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0163_0.95_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0164_0.9_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0165_0.9_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0166_0.8_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0167_0.8_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0168_0.9_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0169_0.9_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0170_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0171_0.85_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0172_0.85_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0174_0.95_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0175_0.85_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0176_1_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0178_0.8_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0179_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0180_0.85_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0181_0.9_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0182_0.8_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0183_0.8_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0184_0.95_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0185_1_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0187_0.9_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0188_0.85_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0189_0.85_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0191_1_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0194_1_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0195_1_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0196_0.9_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0197_0.8_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0198_0.95_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0199_1_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0200_1_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0201_1_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0202_0.85_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0204_0.8_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0205_1_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0206_1_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0207_1_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0208_0.8_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0209_0.85_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0210_0.85_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0212_0.9_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0213_1_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0215_0.8_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0216_1_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0217_1_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0218_0.9_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0219_0.85_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0220_0.85_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0222_0.85_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0223_0.95_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0225_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0226_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0228_1_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0230_0.9_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0233_0.8_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0235_0.9_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0237_0.8_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0238_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0239_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0240_1_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0242_0.8_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0243_0.8_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0244_0.9_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0245_1_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0246_0.85_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0248_0.8_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0249_1_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0251_0.8_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0253_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0253_1_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0255_0.85_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0256_0.95_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0258_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0259_0.9_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0260_0.9_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0261_0.85_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0262_0.95_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0263_0.95_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0264_0.95_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0266_0.8_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0267_0.9_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0268_0.9_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0269_0.95_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0270_0.85_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0271_0.85_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0272_0.85_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0273_0.85_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0274_0.95_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0275_1_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0276_1_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0277_1_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0279_0.8_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0280_0.9_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0281_0.9_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0282_0.85_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0283_0.85_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0284_0.8_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0285_0.85_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0286_0.8_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0287_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0287_0.95_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0288_0.95_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0290_0.85_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0291_0.95_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0292_1_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0294_1_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0295_0.85_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0296_0.95_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0297_0.95_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0298_1_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0299_0.9_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0300_1_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0302_0.9_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0303_0.9_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0304_0.9_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0305_0.9_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0306_0.8_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0307_0.8_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0308_1_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0309_1_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0311_0.8_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0312_0.8_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0313_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0314_0.9_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0315_0.85_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0316_0.85_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0317_0.85_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0318_0.85_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0319_0.9_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0320_0.9_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0320_1_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0321_1_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0323_0.8_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0324_0.8_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0325_0.9_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0326_0.9_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0327_1_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0329_0.8_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0330_0.8_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0330_0.95_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0331_0.95_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0332_0.95_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0333_0.95_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0334_1_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0335_1_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0337_0.8_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0338_0.9_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0340_0.85_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0341_0.85_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0342_0.95_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0343_0.95_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0344_0.85_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0345_0.85_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0346_0.95_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0348_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0349_0.9_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0350_0.85_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0351_0.95_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0353_0.9_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0354_0.9_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0355_0.85_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0356_0.85_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0357_0.95_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0359_0.85_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0361_0.85_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0362_0.95_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0364_0.85_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0366_0.9_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0369_0.85_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0371_0.9_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0372_0.85_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0375_0.85_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0380_0.85_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0382_0.9_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0383_0.9_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0385_0.8_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0386_1_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0388_0.95_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0390_0.9_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0391_1_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0392_1_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0393_1_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0395_0.95_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0397_0.85_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0399_0.8_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0400_0.9_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0402_0.8_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0404_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0405_1_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0406_1_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0407_0.95_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0409_1_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0411_0.95_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/0413_0.95_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1001_0.8_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1002_1_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1005_0.8_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1006_1_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1007_0.95_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1008_0.9_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1009_0.85_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1010_0.85_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1011_0.95_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1012_0.95_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1015_0.8_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1016_0.9_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1018_0.85_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1020_0.95_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1022_0.85_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1027_0.8_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1030_0.95_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1034_0.9_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1037_0.8_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1038_1_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1040_1_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1042_0.95_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1044_0.85_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1046_0.9_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1048_0.9_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1050_0.8_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1051_1_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1053_0.95_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1055_0.95_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1057_0.95_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1059_0.95_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1060_0.85_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1063_0.9_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1709_0.95_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1712_0.8_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1713_1_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1714_1_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1716_0.95_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1718_0.9_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1722_0.95_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1724_0.95_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1726_0.85_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1728_0.9_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1730_0.85_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1731_0.85_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1732_0.95_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1734_1_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1736_0.8_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1738_1_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1741_0.85_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1742_0.85_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1743_0.85_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1744_0.85_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1747_0.8_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1749_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1753_0.9_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1756_0.9_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1757_0.85_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1759_0.8_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1760_1_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1765_0.85_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1771_1_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1774_0.95_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1778_0.9_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1781_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1784_0.8_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1790_1_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1800_1_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1805_0.85_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1812_1_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1815_1_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1818_0.85_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1821_0.8_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1822_1_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1824_0.95_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1826_0.9_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1828_0.85_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1831_0.8_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1832_0.95_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1834_1_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1837_0.9_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1839_0.8_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1840_0.85_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1843_0.8_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1845_0.9_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1846_0.9_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1848_0.8_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1849_1_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1851_0.85_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1852_0.85_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1853_0.85_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1855_0.9_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1857_0.8_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1858_0.95_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1859_0.85_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1861_0.85_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1862_0.8_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1863_1_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1865_0.85_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1867_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1868_1_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1869_1_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1871_0.95_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1872_0.95_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1873_0.85_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1874_0.85_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1875_0.85_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1876_1_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1877_0.9_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1878_0.9_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1879_0.9_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1880_0.9_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1881_0.8_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1882_0.85_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1883_0.8_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1887_0.95_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1889_0.85_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1891_0.9_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1893_0.9_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1896_0.9_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1898_0.85_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1899_0.8_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1900_0.8_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1903_0.95_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1909_0.85_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1913_0.95_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1915_1_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1917_0.95_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1919_1_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1920_0.95_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1921_0.95_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1923_0.9_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1924_0.85_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1926_0.9_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1927_0.9_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1928_0.9_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1930_0.8_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1931_0.8_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1932_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1933_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1934_1_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1936_0.95_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1938_0.85_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1940_0.9_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1942_0.85_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1944_0.9_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1945_0.9_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1947_0.8_0.16.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1949_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1950_1_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1953_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1954_1_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1956_0.8_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1958_0.9_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1960_0.85_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1962_0.9_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1964_0.85_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1966_0.9_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1968_0.8_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1970_0.9_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1971_1_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1973_0.95_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1975_0.85_0.12.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1977_0.8_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1981_0.8_0.2.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1982_1_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1984_0.85_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1986_0.9_0.08.jpg  \n",
            "  inflating: SOTS/outdoor/hazy/1988_0.8_0.12.jpg  \n"
          ]
        }
      ],
      "source": [
        "!cp drive/MyDrive/haze/data/SOTS.zip .\n",
        "!unzip SOTS.zip "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEIxYXUD3C8a",
        "outputId": "dabd858e-9041-40db-bb09-b66e18e2adec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ZID'...\n",
            "remote: Enumerating objects: 49, done.\u001b[K\n",
            "remote: Counting objects: 100% (49/49), done.\u001b[K\n",
            "remote: Compressing objects: 100% (41/41), done.\u001b[K\n",
            "remote: Total 49 (delta 9), reused 45 (delta 5), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (49/49), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/vfrantc/ZID.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDEiGMfM-O1f",
        "outputId": "270e6e81-032c-4665-da19-efc58d6d2611"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ZID\n"
          ]
        }
      ],
      "source": [
        "%cd ZID"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/REAL.zip ."
      ],
      "metadata": {
        "id": "JSVWsxk30XrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip REAL.zip "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "el7bzGBG0X3p",
        "outputId": "0fde5dd6-3ab1-451b-c4f2-ce3eba8b9fb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  REAL.zip\n",
            "   creating: REAL/\n",
            "  inflating: REAL/1.png              \n",
            "  inflating: REAL/3.png              \n",
            "  inflating: REAL/4.png              \n",
            "  inflating: REAL/5.png              \n",
            "  inflating: REAL/6.png              \n",
            "  inflating: REAL/7.png              \n",
            "  inflating: REAL/8.png              \n",
            "  inflating: REAL/9.png              \n",
            "  inflating: REAL/10.png             \n",
            "  inflating: REAL/11.png             \n",
            "  inflating: REAL/12.png             \n",
            "  inflating: REAL/13.png             \n",
            "  inflating: REAL/14.png             \n",
            "  inflating: REAL/15.png             \n",
            "  inflating: REAL/16.png             \n",
            "  inflating: REAL/17.png             \n",
            "  inflating: REAL/18.png             \n",
            "  inflating: REAL/19.png             \n",
            "  inflating: REAL/20.png             \n",
            "  inflating: REAL/21.png             \n",
            "  inflating: REAL/22.png             \n",
            "  inflating: REAL/23.png             \n",
            "  inflating: REAL/24.png             \n",
            "  inflating: REAL/25.png             \n",
            "  inflating: REAL/26.png             \n",
            "  inflating: REAL/27.png             \n",
            "  inflating: REAL/28.png             \n",
            "  inflating: REAL/29.png             \n",
            "  inflating: REAL/30.png             \n",
            "  inflating: REAL/31.png             \n",
            "  inflating: REAL/32.png             \n",
            "  inflating: REAL/33.png             \n",
            "  inflating: REAL/aerial_input.png   \n",
            "  inflating: REAL/buildings_input.png  \n",
            "  inflating: REAL/canon_input.png    \n",
            "  inflating: REAL/castle_input.png   \n",
            "  inflating: REAL/cityscape_input.png  \n",
            "  inflating: REAL/cliff_input.png    \n",
            "  inflating: REAL/cones_input.png    \n",
            "  inflating: REAL/dubai_input.png    \n",
            "  inflating: REAL/flags_input.png    \n",
            "  inflating: REAL/florence_input.png  \n",
            "  inflating: REAL/forest_input.png   \n",
            "  inflating: REAL/herzeliya_input.png  \n",
            "  inflating: REAL/hill.png           \n",
            "  inflating: REAL/hongkong_input.png  \n",
            "  inflating: REAL/house_input.png    \n",
            "  inflating: REAL/lviv_input.png     \n",
            "  inflating: REAL/mountain_input.png  \n",
            "  inflating: REAL/night_input.png    \n",
            "  inflating: REAL/ny12_input.png     \n",
            "  inflating: REAL/ny17_input.png     \n",
            "  inflating: REAL/pumpkins_input.png  \n",
            "  inflating: REAL/road_input.png     \n",
            "  inflating: REAL/schechner_input.png  \n",
            "  inflating: REAL/seoul.png          \n",
            "  inflating: REAL/snow_input.png     \n",
            "  inflating: REAL/stadium_input.png  \n",
            "  inflating: REAL/swan_input.png     \n",
            "  inflating: REAL/test_image1.png    \n",
            "  inflating: REAL/tiananmen_input.png  \n",
            "  inflating: REAL/towns.png          \n",
            "  inflating: REAL/train_input.png    \n",
            "  inflating: REAL/tree_input.png     \n",
            "  inflating: REAL/urbino_input.png   \n",
            "  inflating: REAL/y1_input.png       \n",
            "  inflating: REAL/y16_input.png      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python RW_dehazing.py -input /content/ZID/REAL/ -output out/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akrwGPtI0X6Q",
        "outputId": "525838fd-5821-4dae-b553-d026d70d6478"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Iteration 00010    Loss 0.125113  0.000003\n",
            "Iteration 00011    Loss 0.118406  0.000003\n",
            "Iteration 00012    Loss 0.112611  0.000003\n",
            "Iteration 00013    Loss 0.106853  0.000004\n",
            "Iteration 00014    Loss 0.100504  0.000005\n",
            "Iteration 00015    Loss 0.096035  0.000005\n",
            "Iteration 00016    Loss 0.093644  0.000005\n",
            "Iteration 00017    Loss 0.090092  0.000006\n",
            "Iteration 00018    Loss 0.088336  0.000006\n",
            "Iteration 00019    Loss 0.086627  0.000007\n",
            "Iteration 00020    Loss 0.084512  0.000007\n",
            "Iteration 00021    Loss 0.082719  0.000007\n",
            "Iteration 00022    Loss 0.081398  0.000008\n",
            "Iteration 00023    Loss 0.079865  0.000008\n",
            "Iteration 00024    Loss 0.078616  0.000007\n",
            "Iteration 00025    Loss 0.077025  0.000009\n",
            "Iteration 00026    Loss 0.075720  0.000009\n",
            "Iteration 00027    Loss 0.074528  0.000009\n",
            "Iteration 00028    Loss 0.073125  0.000010\n",
            "Iteration 00029    Loss 0.071943  0.000010\n",
            "Iteration 00030    Loss 0.070843  0.000011\n",
            "Iteration 00031    Loss 0.069681  0.000012\n",
            "Iteration 00032    Loss 0.068521  0.000011\n",
            "Iteration 00033    Loss 0.067454  0.000012\n",
            "Iteration 00034    Loss 0.066402  0.000015\n",
            "Iteration 00035    Loss 0.065370  0.000016\n",
            "Iteration 00036    Loss 0.064347  0.000016\n",
            "Iteration 00037    Loss 0.063282  0.000017\n",
            "Iteration 00038    Loss 0.062356  0.000019\n",
            "Iteration 00039    Loss 0.061330  0.000018\n",
            "Iteration 00040    Loss 0.060357  0.000017\n",
            "Iteration 00041    Loss 0.059424  0.000017\n",
            "Iteration 00042    Loss 0.058495  0.000018\n",
            "Iteration 00043    Loss 0.057596  0.000019\n",
            "Iteration 00044    Loss 0.056771  0.000019\n",
            "Iteration 00045    Loss 0.055808  0.000020\n",
            "Iteration 00046    Loss 0.054923  0.000026\n",
            "Iteration 00047    Loss 0.053989  0.000028\n",
            "Iteration 00048    Loss 0.053149  0.000022\n",
            "Iteration 00049    Loss 0.052279  0.000025\n",
            "Iteration 00050    Loss 0.051524  0.000026\n",
            "Iteration 00051    Loss 0.050675  0.000026\n",
            "Iteration 00052    Loss 0.049849  0.000029\n",
            "Iteration 00053    Loss 0.049003  0.000041\n",
            "Iteration 00054    Loss 0.048275  0.000048\n",
            "Iteration 00055    Loss 0.047430  0.000034\n",
            "Iteration 00056    Loss 0.046697  0.000029\n",
            "Iteration 00057    Loss 0.045898  0.000034\n",
            "Iteration 00058    Loss 0.045227  0.000037\n",
            "Iteration 00059    Loss 0.044442  0.000038\n",
            "Iteration 00060    Loss 0.043776  0.000035\n",
            "Iteration 00061    Loss 0.043030  0.000032\n",
            "Iteration 00062    Loss 0.042362  0.000036\n",
            "Iteration 00063    Loss 0.041676  0.000037\n",
            "Iteration 00064    Loss 0.040945  0.000034\n",
            "Iteration 00065    Loss 0.040248  0.000036\n",
            "Iteration 00066    Loss 0.039633  0.000039\n",
            "Iteration 00067    Loss 0.038986  0.000039\n",
            "Iteration 00068    Loss 0.038320  0.000046\n",
            "Iteration 00069    Loss 0.037671  0.000046\n",
            "Iteration 00070    Loss 0.036970  0.000045\n",
            "Iteration 00071    Loss 0.036313  0.000048\n",
            "Iteration 00072    Loss 0.035661  0.000044\n",
            "Iteration 00073    Loss 0.035042  0.000046\n",
            "Iteration 00074    Loss 0.034457  0.000050\n",
            "Iteration 00075    Loss 0.033845  0.000049\n",
            "Iteration 00076    Loss 0.033260  0.000050\n",
            "Iteration 00077    Loss 0.032652  0.000050\n",
            "Iteration 00078    Loss 0.032123  0.000049\n",
            "Iteration 00079    Loss 0.031472  0.000052\n",
            "Iteration 00080    Loss 0.030895  0.000054\n",
            "Iteration 00081    Loss 0.030330  0.000051\n",
            "Iteration 00082    Loss 0.029789  0.000051\n",
            "Iteration 00083    Loss 0.029234  0.000051\n",
            "Iteration 00084    Loss 0.028656  0.000052\n",
            "Iteration 00085    Loss 0.028186  0.000053\n",
            "Iteration 00086    Loss 0.027666  0.000058\n",
            "Iteration 00087    Loss 0.027101  0.000061\n",
            "Iteration 00088    Loss 0.026664  0.000058\n",
            "Iteration 00089    Loss 0.026124  0.000059\n",
            "Iteration 00090    Loss 0.025623  0.000060\n",
            "Iteration 00091    Loss 0.025071  0.000063\n",
            "Iteration 00092    Loss 0.024598  0.000064\n",
            "Iteration 00093    Loss 0.024123  0.000066\n",
            "Iteration 00094    Loss 0.023616  0.000062\n",
            "Iteration 00095    Loss 0.023149  0.000063\n",
            "Iteration 00096    Loss 0.022669  0.000072\n",
            "Iteration 00097    Loss 0.022290  0.000072\n",
            "Iteration 00098    Loss 0.021761  0.000068\n",
            "Iteration 00099    Loss 0.021341  0.000068\n",
            "Iteration 00100    Loss 0.020871  0.000068\n",
            "Iteration 00101    Loss 0.020323  0.000068\n",
            "Iteration 00102    Loss 0.019848  0.000070\n",
            "Iteration 00103    Loss 0.019423  0.000072\n",
            "Iteration 00104    Loss 0.018978  0.000073\n",
            "Iteration 00105    Loss 0.018473  0.000072\n",
            "Iteration 00106    Loss 0.018019  0.000075\n",
            "Iteration 00107    Loss 0.017617  0.000075\n",
            "Iteration 00108    Loss 0.017153  0.000073\n",
            "Iteration 00109    Loss 0.016610  0.000077\n",
            "Iteration 00110    Loss 0.016184  0.000084\n",
            "Iteration 00111    Loss 0.015672  0.000075\n",
            "Iteration 00112    Loss 0.015285  0.000077\n",
            "Iteration 00113    Loss 0.014864  0.000084\n",
            "Iteration 00114    Loss 0.014379  0.000081\n",
            "Iteration 00115    Loss 0.013965  0.000079\n",
            "Iteration 00116    Loss 0.013609  0.000082\n",
            "Iteration 00117    Loss 0.013203  0.000080\n",
            "Iteration 00118    Loss 0.012841  0.000083\n",
            "Iteration 00119    Loss 0.012572  0.000086\n",
            "Iteration 00120    Loss 0.011951  0.000081\n",
            "Iteration 00121    Loss 0.011590  0.000089\n",
            "Iteration 00122    Loss 0.011271  0.000096\n",
            "Iteration 00123    Loss 0.010892  0.000087\n",
            "Iteration 00124    Loss 0.010485  0.000088\n",
            "Iteration 00125    Loss 0.010077  0.000092\n",
            "Iteration 00126    Loss 0.009727  0.000088\n",
            "Iteration 00127    Loss 0.009379  0.000084\n",
            "Iteration 00128    Loss 0.009001  0.000093\n",
            "Iteration 00129    Loss 0.008608  0.000094\n",
            "Iteration 00130    Loss 0.008262  0.000097\n",
            "Iteration 00131    Loss 0.007932  0.000098\n",
            "Iteration 00132    Loss 0.007455  0.000101\n",
            "Iteration 00133    Loss 0.007118  0.000099\n",
            "Iteration 00134    Loss 0.006852  0.000100\n",
            "Iteration 00135    Loss 0.006431  0.000096\n",
            "Iteration 00136    Loss 0.006109  0.000106\n",
            "Iteration 00137    Loss 0.005839  0.000110\n",
            "Iteration 00138    Loss 0.005497  0.000094\n",
            "Iteration 00139    Loss 0.005181  0.000099\n",
            "Iteration 00140    Loss 0.004767  0.000104\n",
            "Iteration 00141    Loss 0.004639  0.000101\n",
            "Iteration 00142    Loss 0.004187  0.000108\n",
            "Iteration 00143    Loss 0.003887  0.000099\n",
            "Iteration 00144    Loss 0.003513  0.000105\n",
            "Iteration 00145    Loss 0.003233  0.000109\n",
            "Iteration 00146    Loss 0.003075  0.000094\n",
            "Iteration 00147    Loss 0.002730  0.000117\n",
            "Iteration 00148    Loss 0.002495  0.000128\n",
            "Iteration 00149    Loss 0.002209  0.000102\n",
            "Iteration 00150    Loss 0.001740  0.000097\n",
            "Iteration 00151    Loss 0.001527  0.000123\n",
            "Iteration 00152    Loss 0.001258  0.000113\n",
            "Iteration 00153    Loss 0.000879  0.000099\n",
            "Iteration 00154    Loss 0.000660  0.000123\n",
            "Iteration 00155    Loss 0.000340  0.000117\n",
            "Iteration 00156    Loss -0.000016  0.000113\n",
            "Iteration 00157    Loss -0.000293  0.000104\n",
            "Iteration 00158    Loss -0.000584  0.000107\n",
            "Iteration 00159    Loss -0.000701  0.000126\n",
            "Iteration 00160    Loss -0.001138  0.000121\n",
            "Iteration 00161    Loss -0.001539  0.000113\n",
            "Iteration 00162    Loss -0.001640  0.000125\n",
            "Iteration 00163    Loss -0.002001  0.000124\n",
            "Iteration 00164    Loss -0.002306  0.000115\n",
            "Iteration 00165    Loss -0.002534  0.000116\n",
            "Iteration 00166    Loss -0.002835  0.000119\n",
            "Iteration 00167    Loss -0.003096  0.000129\n",
            "Iteration 00168    Loss -0.003334  0.000133\n",
            "Iteration 00169    Loss -0.003618  0.000125\n",
            "Iteration 00170    Loss -0.003879  0.000134\n",
            "Iteration 00171    Loss -0.004114  0.000138\n",
            "Iteration 00172    Loss -0.004315  0.000126\n",
            "Iteration 00173    Loss -0.004532  0.000123\n",
            "Iteration 00174    Loss -0.004839  0.000130\n",
            "Iteration 00175    Loss -0.005087  0.000136\n",
            "Iteration 00176    Loss -0.005388  0.000136\n",
            "Iteration 00177    Loss -0.005635  0.000137\n",
            "Iteration 00178    Loss -0.005836  0.000143\n",
            "Iteration 00179    Loss -0.006037  0.000138\n",
            "Iteration 00180    Loss -0.006288  0.000135\n",
            "Iteration 00181    Loss -0.006641  0.000145\n",
            "Iteration 00182    Loss -0.006824  0.000132\n",
            "Iteration 00183    Loss -0.007026  0.000130\n",
            "Iteration 00184    Loss -0.007286  0.000141\n",
            "Iteration 00185    Loss -0.007441  0.000139\n",
            "Iteration 00186    Loss -0.007800  0.000142\n",
            "Iteration 00187    Loss -0.007894  0.000148\n",
            "Iteration 00188    Loss -0.008159  0.000147\n",
            "Iteration 00189    Loss -0.008450  0.000148\n",
            "Iteration 00190    Loss -0.008498  0.000144\n",
            "Iteration 00191    Loss -0.008885  0.000150\n",
            "Iteration 00192    Loss -0.009074  0.000146\n",
            "Iteration 00193    Loss -0.009194  0.000140\n",
            "Iteration 00194    Loss -0.009394  0.000161\n",
            "Iteration 00195    Loss -0.009715  0.000155\n",
            "Iteration 00196    Loss -0.009967  0.000135\n",
            "Iteration 00197    Loss -0.010197  0.000140\n",
            "Iteration 00198    Loss -0.010320  0.000156\n",
            "Iteration 00199    Loss -0.010586  0.000158\n",
            "Iteration 00200    Loss -0.010683  0.000149\n",
            "Iteration 00201    Loss -0.011007  0.000163\n",
            "Iteration 00202    Loss -0.011220  0.000159\n",
            "Iteration 00203    Loss -0.011367  0.000151\n",
            "Iteration 00204    Loss -0.011741  0.000149\n",
            "Iteration 00205    Loss -0.011741  0.000157\n",
            "Iteration 00206    Loss -0.012087  0.000162\n",
            "Iteration 00207    Loss -0.012280  0.000140\n",
            "Iteration 00208    Loss -0.012547  0.000154\n",
            "Iteration 00209    Loss -0.012784  0.000165\n",
            "Iteration 00210    Loss -0.012831  0.000162\n",
            "Iteration 00211    Loss -0.013107  0.000157\n",
            "Iteration 00212    Loss -0.013381  0.000173\n",
            "Iteration 00213    Loss -0.013590  0.000155\n",
            "Iteration 00214    Loss -0.013670  0.000155\n",
            "Iteration 00215    Loss -0.014017  0.000178\n",
            "Iteration 00216    Loss -0.014080  0.000159\n",
            "Iteration 00217    Loss -0.014386  0.000164\n",
            "Iteration 00218    Loss -0.014546  0.000167\n",
            "Iteration 00219    Loss -0.014519  0.000176\n",
            "Iteration 00220    Loss -0.014994  0.000170\n",
            "Iteration 00221    Loss -0.014834  0.000166\n",
            "Iteration 00222    Loss -0.015101  0.000184\n",
            "Iteration 00223    Loss -0.015512  0.000183\n",
            "Iteration 00224    Loss -0.015333  0.000154\n",
            "Iteration 00225    Loss -0.015961  0.000182\n",
            "Iteration 00226    Loss -0.015868  0.000173\n",
            "Iteration 00227    Loss -0.016397  0.000186\n",
            "Iteration 00228    Loss -0.016221  0.000182\n",
            "Iteration 00229    Loss -0.016828  0.000173\n",
            "Iteration 00230    Loss -0.016840  0.000191\n",
            "Iteration 00231    Loss -0.017138  0.000198\n",
            "Iteration 00232    Loss -0.016966  0.000165\n",
            "Iteration 00233    Loss -0.017473  0.000175\n",
            "Iteration 00234    Loss -0.017789  0.000188\n",
            "Iteration 00235    Loss -0.017984  0.000198\n",
            "Iteration 00236    Loss -0.018196  0.000186\n",
            "Iteration 00237    Loss -0.017866  0.000175\n",
            "Iteration 00238    Loss -0.017786  0.000195\n",
            "Iteration 00239    Loss -0.018268  0.000210\n",
            "Iteration 00240    Loss -0.017893  0.000176\n",
            "Iteration 00241    Loss -0.018552  0.000201\n",
            "Iteration 00242    Loss -0.018995  0.000186\n",
            "Iteration 00243    Loss -0.018651  0.000181\n",
            "Iteration 00244    Loss -0.019086  0.000202\n",
            "Iteration 00245    Loss -0.019385  0.000191\n",
            "Iteration 00246    Loss -0.019568  0.000186\n",
            "Iteration 00247    Loss -0.019908  0.000183\n",
            "Iteration 00248    Loss -0.019479  0.000194\n",
            "Iteration 00249    Loss -0.020152  0.000188\n",
            "Iteration 00250    Loss -0.019943  0.000194\n",
            "Iteration 00251    Loss -0.020661  0.000182\n",
            "Iteration 00252    Loss -0.020550  0.000187\n",
            "Iteration 00253    Loss -0.020610  0.000206\n",
            "Iteration 00254    Loss -0.021072  0.000177\n",
            "Iteration 00255    Loss -0.021195  0.000185\n",
            "Iteration 00256    Loss -0.021413  0.000205\n",
            "Iteration 00257    Loss -0.021310  0.000184\n",
            "Iteration 00258    Loss -0.021607  0.000192\n",
            "Iteration 00259    Loss -0.021645  0.000218\n",
            "Iteration 00260    Loss -0.021691  0.000190\n",
            "Iteration 00261    Loss -0.021544  0.000194\n",
            "Iteration 00262    Loss -0.022351  0.000217\n",
            "Iteration 00263    Loss -0.021612  0.000184\n",
            "Iteration 00264    Loss -0.021320  0.000216\n",
            "Iteration 00265    Loss -0.022679  0.000214\n",
            "Iteration 00266    Loss -0.022605  0.000193\n",
            "Iteration 00267    Loss -0.022935  0.000216\n",
            "Iteration 00268    Loss -0.022780  0.000216\n",
            "Iteration 00269    Loss -0.022667  0.000203\n",
            "Iteration 00270    Loss -0.023373  0.000225\n",
            "Iteration 00271    Loss -0.022493  0.000219\n",
            "Iteration 00272    Loss -0.023469  0.000213\n",
            "Iteration 00273    Loss -0.023593  0.000210\n",
            "Iteration 00274    Loss -0.023746  0.000230\n",
            "Iteration 00275    Loss -0.023877  0.000204\n",
            "Iteration 00276    Loss -0.023819  0.000194\n",
            "Iteration 00277    Loss -0.024244  0.000241\n",
            "Iteration 00278    Loss -0.024394  0.000187\n",
            "Iteration 00279    Loss -0.024752  0.000188\n",
            "Iteration 00280    Loss -0.025414  0.000197\n",
            "Iteration 00281    Loss -0.025498  0.000199\n",
            "Iteration 00282    Loss -0.025118  0.000218\n",
            "Iteration 00283    Loss -0.024557  0.000207\n",
            "Iteration 00284    Loss -0.026126  0.000189\n",
            "Iteration 00285    Loss -0.025359  0.000196\n",
            "Iteration 00286    Loss -0.026170  0.000230\n",
            "Iteration 00287    Loss -0.026298  0.000175\n",
            "Iteration 00288    Loss -0.026644  0.000165\n",
            "Iteration 00289    Loss -0.026012  0.000203\n",
            "Iteration 00290    Loss -0.026829  0.000222\n",
            "Iteration 00291    Loss -0.026738  0.000196\n",
            "Iteration 00292    Loss -0.027105  0.000205\n",
            "Iteration 00293    Loss -0.026896  0.000218\n",
            "Iteration 00294    Loss -0.027566  0.000211\n",
            "Iteration 00295    Loss -0.027540  0.000216\n",
            "Iteration 00296    Loss -0.027886  0.000222\n",
            "Iteration 00297    Loss -0.027957  0.000213\n",
            "Iteration 00298    Loss -0.027522  0.000214\n",
            "Iteration 00299    Loss -0.027501  0.000235\n",
            "Iteration 00300    Loss -0.028410  0.000229\n",
            "Iteration 00301    Loss -0.027889  0.000192\n",
            "Iteration 00302    Loss -0.028469  0.000226\n",
            "Iteration 00303    Loss -0.028343  0.000246\n",
            "Iteration 00304    Loss -0.028668  0.000212\n",
            "Iteration 00305    Loss -0.028683  0.000216\n",
            "Iteration 00306    Loss -0.028856  0.000233\n",
            "Iteration 00307    Loss -0.029174  0.000218\n",
            "Iteration 00308    Loss -0.029018  0.000227\n",
            "Iteration 00309    Loss -0.029371  0.000234\n",
            "Iteration 00310    Loss -0.029479  0.000238\n",
            "Iteration 00311    Loss -0.029663  0.000223\n",
            "Iteration 00312    Loss -0.029661  0.000222\n",
            "Iteration 00313    Loss -0.029747  0.000245\n",
            "Iteration 00314    Loss -0.029853  0.000216\n",
            "Iteration 00315    Loss -0.029537  0.000225\n",
            "Iteration 00316    Loss -0.029053  0.000265\n",
            "Iteration 00317    Loss -0.030294  0.000221\n",
            "Iteration 00318    Loss -0.030182  0.000198\n",
            "Iteration 00319    Loss -0.030454  0.000242\n",
            "Iteration 00320    Loss -0.030469  0.000258\n",
            "Iteration 00321    Loss -0.030361  0.000221\n",
            "Iteration 00322    Loss -0.030597  0.000225\n",
            "Iteration 00323    Loss -0.030807  0.000249\n",
            "Iteration 00324    Loss -0.030835  0.000243\n",
            "Iteration 00325    Loss -0.030702  0.000246\n",
            "Iteration 00326    Loss -0.031007  0.000257\n",
            "Iteration 00327    Loss -0.031172  0.000247\n",
            "Iteration 00328    Loss -0.031131  0.000253\n",
            "Iteration 00329    Loss -0.031368  0.000247\n",
            "Iteration 00330    Loss -0.030792  0.000256\n",
            "Iteration 00331    Loss -0.031613  0.000259\n",
            "Iteration 00332    Loss -0.031414  0.000237\n",
            "Iteration 00333    Loss -0.031569  0.000268\n",
            "Iteration 00334    Loss -0.031471  0.000253\n",
            "Iteration 00335    Loss -0.031756  0.000250\n",
            "Iteration 00336    Loss -0.031826  0.000260\n",
            "Iteration 00337    Loss -0.032038  0.000245\n",
            "Iteration 00338    Loss -0.031943  0.000270\n",
            "Iteration 00339    Loss -0.032204  0.000261\n",
            "Iteration 00340    Loss -0.032072  0.000256\n",
            "Iteration 00341    Loss -0.032368  0.000273\n",
            "Iteration 00342    Loss -0.031816  0.000251\n",
            "Iteration 00343    Loss -0.032370  0.000286\n",
            "Iteration 00344    Loss -0.032736  0.000267\n",
            "Iteration 00345    Loss -0.032642  0.000256\n",
            "Iteration 00346    Loss -0.032665  0.000267\n",
            "Iteration 00347    Loss -0.032885  0.000269\n",
            "Iteration 00348    Loss -0.032423  0.000268\n",
            "Iteration 00349    Loss -0.033076  0.000282\n",
            "Iteration 00350    Loss -0.033077  0.000261\n",
            "Iteration 00351    Loss -0.033159  0.000272\n",
            "Iteration 00352    Loss -0.033336  0.000289\n",
            "Iteration 00353    Loss -0.033267  0.000244\n",
            "Iteration 00354    Loss -0.033344  0.000275\n",
            "Iteration 00355    Loss -0.033519  0.000280\n",
            "Iteration 00356    Loss -0.033613  0.000271\n",
            "Iteration 00357    Loss -0.033659  0.000280\n",
            "Iteration 00358    Loss -0.033847  0.000275\n",
            "Iteration 00359    Loss -0.033801  0.000277\n",
            "Iteration 00360    Loss -0.033830  0.000287\n",
            "Iteration 00361    Loss -0.033544  0.000268\n",
            "Iteration 00362    Loss -0.033783  0.000288\n",
            "Iteration 00363    Loss -0.033343  0.000272\n",
            "Iteration 00364    Loss -0.034201  0.000278\n",
            "Iteration 00365    Loss -0.034251  0.000268\n",
            "Iteration 00366    Loss -0.034372  0.000292\n",
            "Iteration 00367    Loss -0.034316  0.000262\n",
            "Iteration 00368    Loss -0.034442  0.000271\n",
            "Iteration 00369    Loss -0.034500  0.000291\n",
            "Iteration 00370    Loss -0.034024  0.000255\n",
            "Iteration 00371    Loss -0.034219  0.000272\n",
            "Iteration 00372    Loss -0.034073  0.000289\n",
            "Iteration 00373    Loss -0.034819  0.000290\n",
            "Iteration 00374    Loss -0.034392  0.000248\n",
            "Iteration 00375    Loss -0.034735  0.000316\n",
            "Iteration 00376    Loss -0.034847  0.000243\n",
            "Iteration 00377    Loss -0.034894  0.000281\n",
            "Iteration 00378    Loss -0.034805  0.000265\n",
            "Iteration 00379    Loss -0.035092  0.000272\n",
            "Iteration 00380    Loss -0.035148  0.000307\n",
            "Iteration 00381    Loss -0.035330  0.000276\n",
            "Iteration 00382    Loss -0.035317  0.000265\n",
            "Iteration 00383    Loss -0.035363  0.000272\n",
            "Iteration 00384    Loss -0.035446  0.000276\n",
            "Iteration 00385    Loss -0.035562  0.000291\n",
            "Iteration 00386    Loss -0.035677  0.000306\n",
            "Iteration 00387    Loss -0.035729  0.000279\n",
            "Iteration 00388    Loss -0.035670  0.000290\n",
            "Iteration 00389    Loss -0.035789  0.000296\n",
            "Iteration 00390    Loss -0.035701  0.000300\n",
            "Iteration 00391    Loss -0.035967  0.000306\n",
            "Iteration 00392    Loss -0.035788  0.000284\n",
            "Iteration 00393    Loss -0.036023  0.000306\n",
            "Iteration 00394    Loss -0.036057  0.000293\n",
            "Iteration 00395    Loss -0.036206  0.000317\n",
            "Iteration 00396    Loss -0.035861  0.000314\n",
            "Iteration 00397    Loss -0.036295  0.000294\n",
            "Iteration 00398    Loss -0.036155  0.000308\n",
            "Iteration 00399    Loss -0.036421  0.000325\n",
            "Iteration 00400    Loss -0.035905  0.000293\n",
            "Iteration 00401    Loss -0.036361  0.000349\n",
            "Iteration 00402    Loss -0.036526  0.000268\n",
            "Iteration 00403    Loss -0.036437  0.000337\n",
            "Iteration 00404    Loss -0.036606  0.000298\n",
            "Iteration 00405    Loss -0.036612  0.000323\n",
            "Iteration 00406    Loss -0.035598  0.000311\n",
            "Iteration 00407    Loss -0.036730  0.000302\n",
            "Iteration 00408    Loss -0.036803  0.000324\n",
            "Iteration 00409    Loss -0.036888  0.000266\n",
            "Iteration 00410    Loss -0.036946  0.000324\n",
            "Iteration 00411    Loss -0.036520  0.000322\n",
            "Iteration 00412    Loss -0.037045  0.000264\n",
            "Iteration 00413    Loss -0.037133  0.000317\n",
            "Iteration 00414    Loss -0.037185  0.000324\n",
            "Iteration 00415    Loss -0.037079  0.000272\n",
            "Iteration 00416    Loss -0.037116  0.000310\n",
            "Iteration 00417    Loss -0.037301  0.000336\n",
            "Iteration 00418    Loss -0.037403  0.000277\n",
            "Iteration 00419    Loss -0.037431  0.000306\n",
            "Iteration 00420    Loss -0.037224  0.000314\n",
            "Iteration 00421    Loss -0.037355  0.000284\n",
            "Iteration 00422    Loss -0.037579  0.000348\n",
            "Iteration 00423    Loss -0.037671  0.000302\n",
            "Iteration 00424    Loss -0.037493  0.000309\n",
            "Iteration 00425    Loss -0.037526  0.000318\n",
            "Iteration 00426    Loss -0.037746  0.000346\n",
            "Iteration 00427    Loss -0.036995  0.000295\n",
            "Iteration 00428    Loss -0.037707  0.000329\n",
            "Iteration 00429    Loss -0.037978  0.000322\n",
            "Iteration 00430    Loss -0.037782  0.000310\n",
            "Iteration 00431    Loss -0.038020  0.000342\n",
            "Iteration 00432    Loss -0.038086  0.000302\n",
            "Iteration 00433    Loss -0.038073  0.000339\n",
            "Iteration 00434    Loss -0.038077  0.000297\n",
            "Iteration 00435    Loss -0.038157  0.000344\n",
            "Iteration 00436    Loss -0.038229  0.000340\n",
            "Iteration 00437    Loss -0.038329  0.000301\n",
            "Iteration 00438    Loss -0.038305  0.000350\n",
            "Iteration 00439    Loss -0.038314  0.000316\n",
            "Iteration 00440    Loss -0.038422  0.000336\n",
            "Iteration 00441    Loss -0.038552  0.000349\n",
            "Iteration 00442    Loss -0.038481  0.000310\n",
            "Iteration 00443    Loss -0.038474  0.000359\n",
            "Iteration 00444    Loss -0.038665  0.000324\n",
            "Iteration 00445    Loss -0.038397  0.000332\n",
            "Iteration 00446    Loss -0.038756  0.000376\n",
            "Iteration 00447    Loss -0.038664  0.000305\n",
            "Iteration 00448    Loss -0.038728  0.000360\n",
            "Iteration 00449    Loss -0.038909  0.000352\n",
            "Iteration 00450    Loss -0.038953  0.000338\n",
            "Iteration 00451    Loss -0.038959  0.000338\n",
            "Iteration 00452    Loss -0.038973  0.000334\n",
            "Iteration 00453    Loss -0.039103  0.000373\n",
            "Iteration 00454    Loss -0.039143  0.000321\n",
            "Iteration 00455    Loss -0.039060  0.000367\n",
            "Iteration 00456    Loss -0.039138  0.000340\n",
            "Iteration 00457    Loss -0.039186  0.000362\n",
            "Iteration 00458    Loss -0.039198  0.000308\n",
            "Iteration 00459    Loss -0.039209  0.000357\n",
            "Iteration 00460    Loss -0.039262  0.000336\n",
            "Iteration 00461    Loss -0.039345  0.000317\n",
            "Iteration 00462    Loss -0.039423  0.000377\n",
            "Iteration 00463    Loss -0.039400  0.000315\n",
            "Iteration 00464    Loss -0.039476  0.000347\n",
            "Iteration 00465    Loss -0.039567  0.000341\n",
            "Iteration 00466    Loss -0.039350  0.000319\n",
            "Iteration 00467    Loss -0.039674  0.000355\n",
            "Iteration 00468    Loss -0.039642  0.000333\n",
            "Iteration 00469    Loss -0.039497  0.000357\n",
            "Iteration 00470    Loss -0.039695  0.000340\n",
            "Iteration 00471    Loss -0.039780  0.000360\n",
            "Iteration 00472    Loss -0.039817  0.000323\n",
            "Iteration 00473    Loss -0.039843  0.000376\n",
            "Iteration 00474    Loss -0.039870  0.000345\n",
            "Iteration 00475    Loss -0.039882  0.000300\n",
            "Iteration 00476    Loss -0.040009  0.000371\n",
            "Iteration 00477    Loss -0.040022  0.000345\n",
            "Iteration 00478    Loss -0.039936  0.000331\n",
            "Iteration 00479    Loss -0.040114  0.000378\n",
            "Iteration 00480    Loss -0.040127  0.000311\n",
            "Iteration 00481    Loss -0.040186  0.000359\n",
            "Iteration 00482    Loss -0.040225  0.000361\n",
            "Iteration 00483    Loss -0.040307  0.000337\n",
            "Iteration 00484    Loss -0.040296  0.000376\n",
            "Iteration 00485    Loss -0.040352  0.000334\n",
            "Iteration 00486    Loss -0.040270  0.000364\n",
            "Iteration 00487    Loss -0.040419  0.000364\n",
            "Iteration 00488    Loss -0.040407  0.000365\n",
            "Iteration 00489    Loss -0.040485  0.000360\n",
            "Iteration 00490    Loss -0.040474  0.000355\n",
            "Iteration 00491    Loss -0.040537  0.000385\n",
            "Iteration 00492    Loss -0.040610  0.000338\n",
            "Iteration 00493    Loss -0.040638  0.000377\n",
            "Iteration 00494    Loss -0.040410  0.000347\n",
            "Iteration 00495    Loss -0.040607  0.000396\n",
            "Iteration 00496    Loss -0.040715  0.000343\n",
            "Iteration 00497    Loss -0.040787  0.000349\n",
            "Iteration 00498    Loss -0.040802  0.000381\n",
            "Iteration 00499    Loss -0.040769  0.000351\n",
            " 87% 58/67 [31:53<04:48, 32.08s/it]/content/ZID/REAL/tree_input.png\n",
            "Iteration 00000    Loss 0.473512  0.000021\n",
            "Iteration 00001    Loss 20.531460  0.000007\n",
            "Iteration 00002    Loss 0.339722  0.000004\n",
            "Iteration 00003    Loss 0.318339  0.000003\n",
            "Iteration 00004    Loss 0.250215  0.000002\n",
            "Iteration 00005    Loss 0.219373  0.000001\n",
            "Iteration 00006    Loss 0.184665  0.000001\n",
            "Iteration 00007    Loss 0.161229  0.000001\n",
            "Iteration 00008    Loss 0.141177  0.000001\n",
            "Iteration 00009    Loss 0.131159  0.000001\n",
            "Iteration 00010    Loss 0.112655  0.000001\n",
            "Iteration 00011    Loss 0.102403  0.000001\n",
            "Iteration 00012    Loss 0.092383  0.000001\n",
            "Iteration 00013    Loss 0.084258  0.000001\n",
            "Iteration 00014    Loss 0.078324  0.000001\n",
            "Iteration 00015    Loss 0.073723  0.000001\n",
            "Iteration 00016    Loss 0.069733  0.000001\n",
            "Iteration 00017    Loss 0.066185  0.000001\n",
            "Iteration 00018    Loss 0.062754  0.000001\n",
            "Iteration 00019    Loss 0.060041  0.000001\n",
            "Iteration 00020    Loss 0.058067  0.000001\n",
            "Iteration 00021    Loss 0.055500  0.000001\n",
            "Iteration 00022    Loss 0.052592  0.000001\n",
            "Iteration 00023    Loss 0.050948  0.000002\n",
            "Iteration 00024    Loss 0.048517  0.000002\n",
            "Iteration 00025    Loss 0.046396  0.000002\n",
            "Iteration 00026    Loss 0.044925  0.000002\n",
            "Iteration 00027    Loss 0.043543  0.000002\n",
            "Iteration 00028    Loss 0.042180  0.000002\n",
            "Iteration 00029    Loss 0.040717  0.000003\n",
            "Iteration 00030    Loss 0.039588  0.000003\n",
            "Iteration 00031    Loss 0.038556  0.000004\n",
            "Iteration 00032    Loss 0.037387  0.000004\n",
            "Iteration 00033    Loss 0.036265  0.000005\n",
            "Iteration 00034    Loss 0.035106  0.000005\n",
            "Iteration 00035    Loss 0.034087  0.000005\n",
            "Iteration 00036    Loss 0.033208  0.000006\n",
            "Iteration 00037    Loss 0.032244  0.000006\n",
            "Iteration 00038    Loss 0.031288  0.000007\n",
            "Iteration 00039    Loss 0.030533  0.000007\n",
            "Iteration 00040    Loss 0.029552  0.000007\n",
            "Iteration 00041    Loss 0.028778  0.000008\n",
            "Iteration 00042    Loss 0.027944  0.000008\n",
            "Iteration 00043    Loss 0.027210  0.000008\n",
            "Iteration 00044    Loss 0.026397  0.000008\n",
            "Iteration 00045    Loss 0.025588  0.000008\n",
            "Iteration 00046    Loss 0.024841  0.000009\n",
            "Iteration 00047    Loss 0.024090  0.000009\n",
            "Iteration 00048    Loss 0.023457  0.000009\n",
            "Iteration 00049    Loss 0.022672  0.000010\n",
            "Iteration 00050    Loss 0.022019  0.000009\n",
            "Iteration 00051    Loss 0.021251  0.000010\n",
            "Iteration 00052    Loss 0.020668  0.000010\n",
            "Iteration 00053    Loss 0.020027  0.000010\n",
            "Iteration 00054    Loss 0.019349  0.000010\n",
            "Iteration 00055    Loss 0.018702  0.000011\n",
            "Iteration 00056    Loss 0.018030  0.000012\n",
            "Iteration 00057    Loss 0.017409  0.000012\n",
            "Iteration 00058    Loss 0.016818  0.000013\n",
            "Iteration 00059    Loss 0.016211  0.000013\n",
            "Iteration 00060    Loss 0.015598  0.000013\n",
            "Iteration 00061    Loss 0.015026  0.000013\n",
            "Iteration 00062    Loss 0.014418  0.000014\n",
            "Iteration 00063    Loss 0.013823  0.000015\n",
            "Iteration 00064    Loss 0.013214  0.000014\n",
            "Iteration 00065    Loss 0.012759  0.000015\n",
            "Iteration 00066    Loss 0.012372  0.000015\n",
            "Iteration 00067    Loss 0.011846  0.000015\n",
            "Iteration 00068    Loss 0.011254  0.000016\n",
            "Iteration 00069    Loss 0.010786  0.000016\n",
            "Iteration 00070    Loss 0.010089  0.000016\n",
            "Iteration 00071    Loss 0.009533  0.000016\n",
            "Iteration 00072    Loss 0.009080  0.000016\n",
            "Iteration 00073    Loss 0.008587  0.000017\n",
            "Iteration 00074    Loss 0.007960  0.000017\n",
            "Iteration 00075    Loss 0.007469  0.000018\n",
            "Iteration 00076    Loss 0.007001  0.000017\n",
            "Iteration 00077    Loss 0.006451  0.000017\n",
            "Iteration 00078    Loss 0.005954  0.000017\n",
            "Iteration 00079    Loss 0.005475  0.000018\n",
            "Iteration 00080    Loss 0.004962  0.000019\n",
            "Iteration 00081    Loss 0.004539  0.000020\n",
            "Iteration 00082    Loss 0.004067  0.000020\n",
            "Iteration 00083    Loss 0.003560  0.000022\n",
            "Iteration 00084    Loss 0.003106  0.000021\n",
            "Iteration 00085    Loss 0.002705  0.000021\n",
            "Iteration 00086    Loss 0.002228  0.000022\n",
            "Iteration 00087    Loss 0.001814  0.000022\n",
            "Iteration 00088    Loss 0.001370  0.000023\n",
            "Iteration 00089    Loss 0.000938  0.000024\n",
            "Iteration 00090    Loss 0.000516  0.000024\n",
            "Iteration 00091    Loss 0.000083  0.000024\n",
            "Iteration 00092    Loss -0.000257  0.000025\n",
            "Iteration 00093    Loss -0.000689  0.000025\n",
            "Iteration 00094    Loss -0.001070  0.000026\n",
            "Iteration 00095    Loss -0.001509  0.000027\n",
            "Iteration 00096    Loss -0.001884  0.000026\n",
            "Iteration 00097    Loss -0.002286  0.000027\n",
            "Iteration 00098    Loss -0.002655  0.000027\n",
            "Iteration 00099    Loss -0.002983  0.000028\n",
            "Iteration 00100    Loss -0.003370  0.000027\n",
            "Iteration 00101    Loss -0.003765  0.000028\n",
            "Iteration 00102    Loss -0.004153  0.000029\n",
            "Iteration 00103    Loss -0.004510  0.000030\n",
            "Iteration 00104    Loss -0.004860  0.000031\n",
            "Iteration 00105    Loss -0.005212  0.000030\n",
            "Iteration 00106    Loss -0.005582  0.000033\n",
            "Iteration 00107    Loss -0.005869  0.000032\n",
            "Iteration 00108    Loss -0.006211  0.000034\n",
            "Iteration 00109    Loss -0.006618  0.000032\n",
            "Iteration 00110    Loss -0.006947  0.000033\n",
            "Iteration 00111    Loss -0.007232  0.000034\n",
            "Iteration 00112    Loss -0.007531  0.000033\n",
            "Iteration 00113    Loss -0.007851  0.000033\n",
            "Iteration 00114    Loss -0.008165  0.000036\n",
            "Iteration 00115    Loss -0.008437  0.000032\n",
            "Iteration 00116    Loss -0.008796  0.000035\n",
            "Iteration 00117    Loss -0.009114  0.000034\n",
            "Iteration 00118    Loss -0.009386  0.000034\n",
            "Iteration 00119    Loss -0.009694  0.000037\n",
            "Iteration 00120    Loss -0.009996  0.000035\n",
            "Iteration 00121    Loss -0.010286  0.000036\n",
            "Iteration 00122    Loss -0.010595  0.000038\n",
            "Iteration 00123    Loss -0.010886  0.000035\n",
            "Iteration 00124    Loss -0.011177  0.000036\n",
            "Iteration 00125    Loss -0.011438  0.000039\n",
            "Iteration 00126    Loss -0.011719  0.000037\n",
            "Iteration 00127    Loss -0.011979  0.000037\n",
            "Iteration 00128    Loss -0.012257  0.000040\n",
            "Iteration 00129    Loss -0.012563  0.000040\n",
            "Iteration 00130    Loss -0.012823  0.000040\n",
            "Iteration 00131    Loss -0.013080  0.000042\n",
            "Iteration 00132    Loss -0.013331  0.000041\n",
            "Iteration 00133    Loss -0.013622  0.000041\n",
            "Iteration 00134    Loss -0.013851  0.000042\n",
            "Iteration 00135    Loss -0.014097  0.000041\n",
            "Iteration 00136    Loss -0.014351  0.000042\n",
            "Iteration 00137    Loss -0.014589  0.000044\n",
            "Iteration 00138    Loss -0.014827  0.000044\n",
            "Iteration 00139    Loss -0.015079  0.000044\n",
            "Iteration 00140    Loss -0.015308  0.000043\n",
            "Iteration 00141    Loss -0.015554  0.000043\n",
            "Iteration 00142    Loss -0.015750  0.000044\n",
            "Iteration 00143    Loss -0.016009  0.000046\n",
            "Iteration 00144    Loss -0.016228  0.000046\n",
            "Iteration 00145    Loss -0.016441  0.000046\n",
            "Iteration 00146    Loss -0.016663  0.000045\n",
            "Iteration 00147    Loss -0.016881  0.000046\n",
            "Iteration 00148    Loss -0.017065  0.000049\n",
            "Iteration 00149    Loss -0.017237  0.000045\n",
            "Iteration 00150    Loss -0.017462  0.000048\n",
            "Iteration 00151    Loss -0.017654  0.000045\n",
            "Iteration 00152    Loss -0.017894  0.000045\n",
            "Iteration 00153    Loss -0.018086  0.000049\n",
            "Iteration 00154    Loss -0.018309  0.000045\n",
            "Iteration 00155    Loss -0.018554  0.000049\n",
            "Iteration 00156    Loss -0.018715  0.000049\n",
            "Iteration 00157    Loss -0.018897  0.000044\n",
            "Iteration 00158    Loss -0.019108  0.000048\n",
            "Iteration 00159    Loss -0.019266  0.000052\n",
            "Iteration 00160    Loss -0.019509  0.000049\n",
            "Iteration 00161    Loss -0.019701  0.000049\n",
            "Iteration 00162    Loss -0.019881  0.000050\n",
            "Iteration 00163    Loss -0.020065  0.000047\n",
            "Iteration 00164    Loss -0.020261  0.000048\n",
            "Iteration 00165    Loss -0.020424  0.000050\n",
            "Iteration 00166    Loss -0.020594  0.000051\n",
            "Iteration 00167    Loss -0.020817  0.000049\n",
            "Iteration 00168    Loss -0.020968  0.000049\n",
            "Iteration 00169    Loss -0.021168  0.000050\n",
            "Iteration 00170    Loss -0.021335  0.000053\n",
            "Iteration 00171    Loss -0.021490  0.000053\n",
            "Iteration 00172    Loss -0.021673  0.000049\n",
            "Iteration 00173    Loss -0.021865  0.000051\n",
            "Iteration 00174    Loss -0.022016  0.000051\n",
            "Iteration 00175    Loss -0.022200  0.000050\n",
            "Iteration 00176    Loss -0.022364  0.000051\n",
            "Iteration 00177    Loss -0.022526  0.000052\n",
            "Iteration 00178    Loss -0.022681  0.000051\n",
            "Iteration 00179    Loss -0.022833  0.000053\n",
            "Iteration 00180    Loss -0.023040  0.000053\n",
            "Iteration 00181    Loss -0.023141  0.000053\n",
            "Iteration 00182    Loss -0.023334  0.000052\n",
            "Iteration 00183    Loss -0.023481  0.000052\n",
            "Iteration 00184    Loss -0.023606  0.000053\n",
            "Iteration 00185    Loss -0.023793  0.000053\n",
            "Iteration 00186    Loss -0.023952  0.000054\n",
            "Iteration 00187    Loss -0.024104  0.000056\n",
            "Iteration 00188    Loss -0.024200  0.000051\n",
            "Iteration 00189    Loss -0.024330  0.000055\n",
            "Iteration 00190    Loss -0.024495  0.000054\n",
            "Iteration 00191    Loss -0.024631  0.000053\n",
            "Iteration 00192    Loss -0.024787  0.000050\n",
            "Iteration 00193    Loss -0.024936  0.000049\n",
            "Iteration 00194    Loss -0.025087  0.000054\n",
            "Iteration 00195    Loss -0.025223  0.000053\n",
            "Iteration 00196    Loss -0.025379  0.000051\n",
            "Iteration 00197    Loss -0.025492  0.000053\n",
            "Iteration 00198    Loss -0.025666  0.000054\n",
            "Iteration 00199    Loss -0.025756  0.000054\n",
            "Iteration 00200    Loss -0.025918  0.000055\n",
            "Iteration 00201    Loss -0.026035  0.000051\n",
            "Iteration 00202    Loss -0.026201  0.000052\n",
            "Iteration 00203    Loss -0.026301  0.000052\n",
            "Iteration 00204    Loss -0.026438  0.000054\n",
            "Iteration 00205    Loss -0.026587  0.000053\n",
            "Iteration 00206    Loss -0.026684  0.000052\n",
            "Iteration 00207    Loss -0.026853  0.000053\n",
            "Iteration 00208    Loss -0.027010  0.000051\n",
            "Iteration 00209    Loss -0.027126  0.000051\n",
            "Iteration 00210    Loss -0.027190  0.000053\n",
            "Iteration 00211    Loss -0.027355  0.000055\n",
            "Iteration 00212    Loss -0.027482  0.000051\n",
            "Iteration 00213    Loss -0.027575  0.000052\n",
            "Iteration 00214    Loss -0.027688  0.000053\n",
            "Iteration 00215    Loss -0.027836  0.000053\n",
            "Iteration 00216    Loss -0.027980  0.000053\n",
            "Iteration 00217    Loss -0.028037  0.000051\n",
            "Iteration 00218    Loss -0.028140  0.000055\n",
            "Iteration 00219    Loss -0.028175  0.000045\n",
            "Iteration 00220    Loss -0.028388  0.000052\n",
            "Iteration 00221    Loss -0.028476  0.000054\n",
            "Iteration 00222    Loss -0.028566  0.000049\n",
            "Iteration 00223    Loss -0.028687  0.000050\n",
            "Iteration 00224    Loss -0.028866  0.000048\n",
            "Iteration 00225    Loss -0.028943  0.000048\n",
            "Iteration 00226    Loss -0.029076  0.000049\n",
            "Iteration 00227    Loss -0.029187  0.000049\n",
            "Iteration 00228    Loss -0.029284  0.000050\n",
            "Iteration 00229    Loss -0.029437  0.000049\n",
            "Iteration 00230    Loss -0.029515  0.000047\n",
            "Iteration 00231    Loss -0.029649  0.000051\n",
            "Iteration 00232    Loss -0.029691  0.000053\n",
            "Iteration 00233    Loss -0.029851  0.000049\n",
            "Iteration 00234    Loss -0.029955  0.000050\n",
            "Iteration 00235    Loss -0.030066  0.000051\n",
            "Iteration 00236    Loss -0.030163  0.000050\n",
            "Iteration 00237    Loss -0.030328  0.000049\n",
            "Iteration 00238    Loss -0.030378  0.000048\n",
            "Iteration 00239    Loss -0.030466  0.000049\n",
            "Iteration 00240    Loss -0.030523  0.000049\n",
            "Iteration 00241    Loss -0.030664  0.000051\n",
            "Iteration 00242    Loss -0.030747  0.000050\n",
            "Iteration 00243    Loss -0.030825  0.000049\n",
            "Iteration 00244    Loss -0.030971  0.000051\n",
            "Iteration 00245    Loss -0.031030  0.000051\n",
            "Iteration 00246    Loss -0.031166  0.000050\n",
            "Iteration 00247    Loss -0.031227  0.000051\n",
            "Iteration 00248    Loss -0.031349  0.000051\n",
            "Iteration 00249    Loss -0.031446  0.000052\n",
            "Iteration 00250    Loss -0.031567  0.000052\n",
            "Iteration 00251    Loss -0.031605  0.000051\n",
            "Iteration 00252    Loss -0.031769  0.000053\n",
            "Iteration 00253    Loss -0.031878  0.000052\n",
            "Iteration 00254    Loss -0.032020  0.000051\n",
            "Iteration 00255    Loss -0.032075  0.000053\n",
            "Iteration 00256    Loss -0.032184  0.000053\n",
            "Iteration 00257    Loss -0.032150  0.000054\n",
            "Iteration 00258    Loss -0.032177  0.000056\n",
            "Iteration 00259    Loss -0.032380  0.000052\n",
            "Iteration 00260    Loss -0.032528  0.000057\n",
            "Iteration 00261    Loss -0.032557  0.000057\n",
            "Iteration 00262    Loss -0.032650  0.000052\n",
            "Iteration 00263    Loss -0.032709  0.000058\n",
            "Iteration 00264    Loss -0.032847  0.000055\n",
            "Iteration 00265    Loss -0.032829  0.000056\n",
            "Iteration 00266    Loss -0.032940  0.000053\n",
            "Iteration 00267    Loss -0.033001  0.000053\n",
            "Iteration 00268    Loss -0.033130  0.000057\n",
            "Iteration 00269    Loss -0.033242  0.000053\n",
            "Iteration 00270    Loss -0.033368  0.000051\n",
            "Iteration 00271    Loss -0.033450  0.000056\n",
            "Iteration 00272    Loss -0.033548  0.000062\n",
            "Iteration 00273    Loss -0.033592  0.000055\n",
            "Iteration 00274    Loss -0.033644  0.000054\n",
            "Iteration 00275    Loss -0.033868  0.000053\n",
            "Iteration 00276    Loss -0.033749  0.000054\n",
            "Iteration 00277    Loss -0.033849  0.000061\n",
            "Iteration 00278    Loss -0.033824  0.000052\n",
            "Iteration 00279    Loss -0.033956  0.000053\n",
            "Iteration 00280    Loss -0.034010  0.000055\n",
            "Iteration 00281    Loss -0.034127  0.000054\n",
            "Iteration 00282    Loss -0.034194  0.000058\n",
            "Iteration 00283    Loss -0.034339  0.000053\n",
            "Iteration 00284    Loss -0.034453  0.000051\n",
            "Iteration 00285    Loss -0.034429  0.000055\n",
            "Iteration 00286    Loss -0.034587  0.000059\n",
            "Iteration 00287    Loss -0.034672  0.000056\n",
            "Iteration 00288    Loss -0.034772  0.000053\n",
            "Iteration 00289    Loss -0.034778  0.000052\n",
            "Iteration 00290    Loss -0.034876  0.000053\n",
            "Iteration 00291    Loss -0.034848  0.000056\n",
            "Iteration 00292    Loss -0.035086  0.000057\n",
            "Iteration 00293    Loss -0.035169  0.000056\n",
            "Iteration 00294    Loss -0.035218  0.000055\n",
            "Iteration 00295    Loss -0.035188  0.000054\n",
            "Iteration 00296    Loss -0.035508  0.000058\n",
            "Iteration 00297    Loss -0.035339  0.000056\n",
            "Iteration 00298    Loss -0.035390  0.000058\n",
            "Iteration 00299    Loss -0.035469  0.000059\n",
            "Iteration 00300    Loss -0.035708  0.000059\n",
            "Iteration 00301    Loss -0.035710  0.000058\n",
            "Iteration 00302    Loss -0.035792  0.000054\n",
            "Iteration 00303    Loss -0.035917  0.000058\n",
            "Iteration 00304    Loss -0.035912  0.000062\n",
            "Iteration 00305    Loss -0.035912  0.000060\n",
            "Iteration 00306    Loss -0.036020  0.000061\n",
            "Iteration 00307    Loss -0.036154  0.000057\n",
            "Iteration 00308    Loss -0.036339  0.000060\n",
            "Iteration 00309    Loss -0.036076  0.000059\n",
            "Iteration 00310    Loss -0.036344  0.000059\n",
            "Iteration 00311    Loss -0.036597  0.000061\n",
            "Iteration 00312    Loss -0.036321  0.000058\n",
            "Iteration 00313    Loss -0.036460  0.000061\n",
            "Iteration 00314    Loss -0.036754  0.000061\n",
            "Iteration 00315    Loss -0.036437  0.000061\n",
            "Iteration 00316    Loss -0.036622  0.000063\n",
            "Iteration 00317    Loss -0.036666  0.000054\n",
            "Iteration 00318    Loss -0.036682  0.000057\n",
            "Iteration 00319    Loss -0.036904  0.000064\n",
            "Iteration 00320    Loss -0.036963  0.000058\n",
            "Iteration 00321    Loss -0.036820  0.000061\n",
            "Iteration 00322    Loss -0.036972  0.000066\n",
            "Iteration 00323    Loss -0.037098  0.000056\n",
            "Iteration 00324    Loss -0.037110  0.000059\n",
            "Iteration 00325    Loss -0.037337  0.000062\n",
            "Iteration 00326    Loss -0.037428  0.000061\n",
            "Iteration 00327    Loss -0.037421  0.000061\n",
            "Iteration 00328    Loss -0.037331  0.000058\n",
            "Iteration 00329    Loss -0.037648  0.000058\n",
            "Iteration 00330    Loss -0.037540  0.000061\n",
            "Iteration 00331    Loss -0.037448  0.000063\n",
            "Iteration 00332    Loss -0.037716  0.000060\n",
            "Iteration 00333    Loss -0.037863  0.000058\n",
            "Iteration 00334    Loss -0.037786  0.000062\n",
            "Iteration 00335    Loss -0.037976  0.000066\n",
            "Iteration 00336    Loss -0.037915  0.000060\n",
            "Iteration 00337    Loss -0.037840  0.000061\n",
            "Iteration 00338    Loss -0.038178  0.000064\n",
            "Iteration 00339    Loss -0.038043  0.000060\n",
            "Iteration 00340    Loss -0.037898  0.000057\n",
            "Iteration 00341    Loss -0.038049  0.000065\n",
            "Iteration 00342    Loss -0.038205  0.000064\n",
            "Iteration 00343    Loss -0.038445  0.000059\n",
            "Iteration 00344    Loss -0.038353  0.000061\n",
            "Iteration 00345    Loss -0.038546  0.000065\n",
            "Iteration 00346    Loss -0.037892  0.000068\n",
            "Iteration 00347    Loss -0.038494  0.000067\n",
            "Iteration 00348    Loss -0.038653  0.000062\n",
            "Iteration 00349    Loss -0.038715  0.000064\n",
            "Iteration 00350    Loss -0.038827  0.000066\n",
            "Iteration 00351    Loss -0.038694  0.000066\n",
            "Iteration 00352    Loss -0.038557  0.000066\n",
            "Iteration 00353    Loss -0.039093  0.000063\n",
            "Iteration 00354    Loss -0.038993  0.000067\n",
            "Iteration 00355    Loss -0.039089  0.000066\n",
            "Iteration 00356    Loss -0.038794  0.000065\n",
            "Iteration 00357    Loss -0.039135  0.000068\n",
            "Iteration 00358    Loss -0.038934  0.000065\n",
            "Iteration 00359    Loss -0.038994  0.000067\n",
            "Iteration 00360    Loss -0.039226  0.000066\n",
            "Iteration 00361    Loss -0.039067  0.000069\n",
            "Iteration 00362    Loss -0.039372  0.000068\n",
            "Iteration 00363    Loss -0.039456  0.000060\n",
            "Iteration 00364    Loss -0.039572  0.000065\n",
            "Iteration 00365    Loss -0.039573  0.000068\n",
            "Iteration 00366    Loss -0.039339  0.000063\n",
            "Iteration 00367    Loss -0.039419  0.000064\n",
            "Iteration 00368    Loss -0.039499  0.000067\n",
            "Iteration 00369    Loss -0.039712  0.000065\n",
            "Iteration 00370    Loss -0.039818  0.000066\n",
            "Iteration 00371    Loss -0.039776  0.000064\n",
            "Iteration 00372    Loss -0.039896  0.000065\n",
            "Iteration 00373    Loss -0.039601  0.000067\n",
            "Iteration 00374    Loss -0.039913  0.000066\n",
            "Iteration 00375    Loss -0.039869  0.000067\n",
            "Iteration 00376    Loss -0.040018  0.000066\n",
            "Iteration 00377    Loss -0.040175  0.000063\n",
            "Iteration 00378    Loss -0.040147  0.000070\n",
            "Iteration 00379    Loss -0.040208  0.000074\n",
            "Iteration 00380    Loss -0.040297  0.000060\n",
            "Iteration 00381    Loss -0.040116  0.000062\n",
            "Iteration 00382    Loss -0.040263  0.000068\n",
            "Iteration 00383    Loss -0.040259  0.000066\n",
            "Iteration 00384    Loss -0.040312  0.000067\n",
            "Iteration 00385    Loss -0.040457  0.000066\n",
            "Iteration 00386    Loss -0.040347  0.000061\n",
            "Iteration 00387    Loss -0.040444  0.000066\n",
            "Iteration 00388    Loss -0.040671  0.000071\n",
            "Iteration 00389    Loss -0.040619  0.000068\n",
            "Iteration 00390    Loss -0.040491  0.000067\n",
            "Iteration 00391    Loss -0.039768  0.000065\n",
            "Iteration 00392    Loss -0.040523  0.000068\n",
            "Iteration 00393    Loss -0.040833  0.000067\n",
            "Iteration 00394    Loss -0.040627  0.000067\n",
            "Iteration 00395    Loss -0.040656  0.000067\n",
            "Iteration 00396    Loss -0.040869  0.000066\n",
            "Iteration 00397    Loss -0.040980  0.000069\n",
            "Iteration 00398    Loss -0.040749  0.000070\n",
            "Iteration 00399    Loss -0.041071  0.000066\n",
            "Iteration 00400    Loss -0.041121  0.000071\n",
            "Iteration 00401    Loss -0.041115  0.000072\n",
            "Iteration 00402    Loss -0.041144  0.000068\n",
            "Iteration 00403    Loss -0.041204  0.000067\n",
            "Iteration 00404    Loss -0.041267  0.000071\n",
            "Iteration 00405    Loss -0.041319  0.000068\n",
            "Iteration 00406    Loss -0.041202  0.000068\n",
            "Iteration 00407    Loss -0.040959  0.000074\n",
            "Iteration 00408    Loss -0.041349  0.000071\n",
            "Iteration 00409    Loss -0.041345  0.000063\n",
            "Iteration 00410    Loss -0.041464  0.000069\n",
            "Iteration 00411    Loss -0.041401  0.000072\n",
            "Iteration 00412    Loss -0.041550  0.000070\n",
            "Iteration 00413    Loss -0.041365  0.000069\n",
            "Iteration 00414    Loss -0.041358  0.000071\n",
            "Iteration 00415    Loss -0.041605  0.000067\n",
            "Iteration 00416    Loss -0.041699  0.000071\n",
            "Iteration 00417    Loss -0.041745  0.000071\n",
            "Iteration 00418    Loss -0.041620  0.000070\n",
            "Iteration 00419    Loss -0.041394  0.000071\n",
            "Iteration 00420    Loss -0.041639  0.000067\n",
            "Iteration 00421    Loss -0.041837  0.000071\n",
            "Iteration 00422    Loss -0.041925  0.000072\n",
            "Iteration 00423    Loss -0.041371  0.000071\n",
            "Iteration 00424    Loss -0.041946  0.000070\n",
            "Iteration 00425    Loss -0.041872  0.000070\n",
            "Iteration 00426    Loss -0.042022  0.000075\n",
            "Iteration 00427    Loss -0.042083  0.000069\n",
            "Iteration 00428    Loss -0.042033  0.000067\n",
            "Iteration 00429    Loss -0.041862  0.000073\n",
            "Iteration 00430    Loss -0.042161  0.000069\n",
            "Iteration 00431    Loss -0.042188  0.000072\n",
            "Iteration 00432    Loss -0.042255  0.000073\n",
            "Iteration 00433    Loss -0.042251  0.000069\n",
            "Iteration 00434    Loss -0.042196  0.000071\n",
            "Iteration 00435    Loss -0.042361  0.000072\n",
            "Iteration 00436    Loss -0.042383  0.000074\n",
            "Iteration 00437    Loss -0.042191  0.000073\n",
            "Iteration 00438    Loss -0.042332  0.000070\n",
            "Iteration 00439    Loss -0.042360  0.000075\n",
            "Iteration 00440    Loss -0.042275  0.000074\n",
            "Iteration 00441    Loss -0.042491  0.000075\n",
            "Iteration 00442    Loss -0.042255  0.000066\n",
            "Iteration 00443    Loss -0.042497  0.000075\n",
            "Iteration 00444    Loss -0.042031  0.000070\n",
            "Iteration 00445    Loss -0.042641  0.000066\n",
            "Iteration 00446    Loss -0.042547  0.000074\n",
            "Iteration 00447    Loss -0.042669  0.000071\n",
            "Iteration 00448    Loss -0.042627  0.000064\n",
            "Iteration 00449    Loss -0.042657  0.000071\n",
            "Iteration 00450    Loss -0.042733  0.000075\n",
            "Iteration 00451    Loss -0.042676  0.000068\n",
            "Iteration 00452    Loss -0.042674  0.000069\n",
            "Iteration 00453    Loss -0.042773  0.000071\n",
            "Iteration 00454    Loss -0.042884  0.000072\n",
            "Iteration 00455    Loss -0.042880  0.000072\n",
            "Iteration 00456    Loss -0.042891  0.000071\n",
            "Iteration 00457    Loss -0.042938  0.000071\n",
            "Iteration 00458    Loss -0.042903  0.000076\n",
            "Iteration 00459    Loss -0.043046  0.000072\n",
            "Iteration 00460    Loss -0.043072  0.000069\n",
            "Iteration 00461    Loss -0.043034  0.000075\n",
            "Iteration 00462    Loss -0.042976  0.000074\n",
            "Iteration 00463    Loss -0.042685  0.000073\n",
            "Iteration 00464    Loss -0.043179  0.000074\n",
            "Iteration 00465    Loss -0.043130  0.000076\n",
            "Iteration 00466    Loss -0.043149  0.000068\n",
            "Iteration 00467    Loss -0.043212  0.000073\n",
            "Iteration 00468    Loss -0.043296  0.000077\n",
            "Iteration 00469    Loss -0.043046  0.000072\n",
            "Iteration 00470    Loss -0.043302  0.000073\n",
            "Iteration 00471    Loss -0.043346  0.000078\n",
            "Iteration 00472    Loss -0.043344  0.000069\n",
            "Iteration 00473    Loss -0.043437  0.000075\n",
            "Iteration 00474    Loss -0.043149  0.000077\n",
            "Iteration 00475    Loss -0.043471  0.000073\n",
            "Iteration 00476    Loss -0.043474  0.000070\n",
            "Iteration 00477    Loss -0.043498  0.000074\n",
            "Iteration 00478    Loss -0.043415  0.000079\n",
            "Iteration 00479    Loss -0.043590  0.000069\n",
            "Iteration 00480    Loss -0.043553  0.000077\n",
            "Iteration 00481    Loss -0.043635  0.000077\n",
            "Iteration 00482    Loss -0.043629  0.000068\n",
            "Iteration 00483    Loss -0.043689  0.000074\n",
            "Iteration 00484    Loss -0.043638  0.000077\n",
            "Iteration 00485    Loss -0.043634  0.000073\n",
            "Iteration 00486    Loss -0.043737  0.000071\n",
            "Iteration 00487    Loss -0.043638  0.000075\n",
            "Iteration 00488    Loss -0.043759  0.000074\n",
            "Iteration 00489    Loss -0.043835  0.000072\n",
            "Iteration 00490    Loss -0.043795  0.000078\n",
            "Iteration 00491    Loss -0.043873  0.000074\n",
            "Iteration 00492    Loss -0.043905  0.000070\n",
            "Iteration 00493    Loss -0.043884  0.000075\n",
            "Iteration 00494    Loss -0.043981  0.000077\n",
            "Iteration 00495    Loss -0.043993  0.000076\n",
            "Iteration 00496    Loss -0.044016  0.000073\n",
            "Iteration 00497    Loss -0.044047  0.000075\n",
            "Iteration 00498    Loss -0.044063  0.000078\n",
            "Iteration 00499    Loss -0.044000  0.000074\n",
            " 88% 59/67 [32:28<04:22, 32.80s/it]/content/ZID/REAL/5.png\n",
            "Iteration 00000    Loss 0.411445  0.000036\n",
            "Iteration 00001    Loss 11.142017  0.000008\n",
            "Iteration 00002    Loss 0.497239  0.000007\n",
            "Iteration 00003    Loss 0.688308  0.000004\n",
            "Iteration 00004    Loss 0.694523  0.000002\n",
            "Iteration 00005    Loss 0.222432  0.000001\n",
            "Iteration 00006    Loss 0.166931  0.000001\n",
            "Iteration 00007    Loss 0.151709  0.000001\n",
            "Iteration 00008    Loss 0.132012  0.000000\n",
            "Iteration 00009    Loss 0.119355  0.000000\n",
            "Iteration 00010    Loss 0.110058  0.000001\n",
            "Iteration 00011    Loss 0.102067  0.000001\n",
            "Iteration 00012    Loss 0.094973  0.000001\n",
            "Iteration 00013    Loss 0.097599  0.000000\n",
            "Iteration 00014    Loss 0.085846  0.000000\n",
            "Iteration 00015    Loss 0.082916  0.000000\n",
            "Iteration 00016    Loss 0.079800  0.000000\n",
            "Iteration 00017    Loss 0.077321  0.000000\n",
            "Iteration 00018    Loss 0.075147  0.000001\n",
            "Iteration 00019    Loss 0.073128  0.000001\n",
            "Iteration 00020    Loss 0.070773  0.000001\n",
            "Iteration 00021    Loss 0.069221  0.000001\n",
            "Iteration 00022    Loss 0.067292  0.000001\n",
            "Iteration 00023    Loss 0.065711  0.000001\n",
            "Iteration 00024    Loss 0.064315  0.000001\n",
            "Iteration 00025    Loss 0.062425  0.000001\n",
            "Iteration 00026    Loss 0.061146  0.000001\n",
            "Iteration 00027    Loss 0.059638  0.000001\n",
            "Iteration 00028    Loss 0.058069  0.000001\n",
            "Iteration 00029    Loss 0.056674  0.000001\n",
            "Iteration 00030    Loss 0.055110  0.000001\n",
            "Iteration 00031    Loss 0.053777  0.000000\n",
            "Iteration 00032    Loss 0.052341  0.000000\n",
            "Iteration 00033    Loss 0.051124  0.000000\n",
            "Iteration 00034    Loss 0.049670  0.000000\n",
            "Iteration 00035    Loss 0.048515  0.000000\n",
            "Iteration 00036    Loss 0.047181  0.000000\n",
            "Iteration 00037    Loss 0.046127  0.000000\n",
            "Iteration 00038    Loss 0.044932  0.000000\n",
            "Iteration 00039    Loss 0.043731  0.000000\n",
            "Iteration 00040    Loss 0.042813  0.000000\n",
            "Iteration 00041    Loss 0.041718  0.000000\n",
            "Iteration 00042    Loss 0.040577  0.000000\n",
            "Iteration 00043    Loss 0.039538  0.000000\n",
            "Iteration 00044    Loss 0.038537  0.000000\n",
            "Iteration 00045    Loss 0.037434  0.000000\n",
            "Iteration 00046    Loss 0.036402  0.000000\n",
            "Iteration 00047    Loss 0.035527  0.000000\n",
            "Iteration 00048    Loss 0.034563  0.000000\n",
            "Iteration 00049    Loss 0.033624  0.000000\n",
            "Iteration 00050    Loss 0.032794  0.000000\n",
            "Iteration 00051    Loss 0.031849  0.000001\n",
            "Iteration 00052    Loss 0.030807  0.000001\n",
            "Iteration 00053    Loss 0.030132  0.000001\n",
            "Iteration 00054    Loss 0.029248  0.000001\n",
            "Iteration 00055    Loss 0.028370  0.000001\n",
            "Iteration 00056    Loss 0.027629  0.000001\n",
            "Iteration 00057    Loss 0.026768  0.000001\n",
            "Iteration 00058    Loss 0.026090  0.000001\n",
            "Iteration 00059    Loss 0.025344  0.000001\n",
            "Iteration 00060    Loss 0.024611  0.000001\n",
            "Iteration 00061    Loss 0.023817  0.000001\n",
            "Iteration 00062    Loss 0.023139  0.000001\n",
            "Iteration 00063    Loss 0.022392  0.000001\n",
            "Iteration 00064    Loss 0.021736  0.000001\n",
            "Iteration 00065    Loss 0.020868  0.000001\n",
            "Iteration 00066    Loss 0.020283  0.000001\n",
            "Iteration 00067    Loss 0.019617  0.000001\n",
            "Iteration 00068    Loss 0.018790  0.000001\n",
            "Iteration 00069    Loss 0.018118  0.000001\n",
            "Iteration 00070    Loss 0.017438  0.000001\n",
            "Iteration 00071    Loss 0.016544  0.000001\n",
            "Iteration 00072    Loss 0.015922  0.000001\n",
            "Iteration 00073    Loss 0.015368  0.000001\n",
            "Iteration 00074    Loss 0.014949  0.000001\n",
            "Iteration 00075    Loss 0.014528  0.000001\n",
            "Iteration 00076    Loss 0.013738  0.000001\n",
            "Iteration 00077    Loss 0.012954  0.000001\n",
            "Iteration 00078    Loss 0.012381  0.000001\n",
            "Iteration 00079    Loss 0.011595  0.000001\n",
            "Iteration 00080    Loss 0.011114  0.000001\n",
            "Iteration 00081    Loss 0.010501  0.000001\n",
            "Iteration 00082    Loss 0.009964  0.000002\n",
            "Iteration 00083    Loss 0.009332  0.000002\n",
            "Iteration 00084    Loss 0.008711  0.000002\n",
            "Iteration 00085    Loss 0.008111  0.000002\n",
            "Iteration 00086    Loss 0.007549  0.000002\n",
            "Iteration 00087    Loss 0.006557  0.000002\n",
            "Iteration 00088    Loss 0.005847  0.000001\n",
            "Iteration 00089    Loss 0.005238  0.000001\n",
            "Iteration 00090    Loss 0.004774  0.000001\n",
            "Iteration 00091    Loss 0.004247  0.000002\n",
            "Iteration 00092    Loss 0.003665  0.000002\n",
            "Iteration 00093    Loss 0.003003  0.000002\n",
            "Iteration 00094    Loss 0.002573  0.000002\n",
            "Iteration 00095    Loss 0.002126  0.000002\n",
            "Iteration 00096    Loss 0.001859  0.000002\n",
            "Iteration 00097    Loss 0.001072  0.000002\n",
            "Iteration 00098    Loss 0.000652  0.000002\n",
            "Iteration 00099    Loss 0.000002  0.000002\n",
            "Iteration 00100    Loss -0.000539  0.000002\n",
            "Iteration 00101    Loss -0.001022  0.000002\n",
            "Iteration 00102    Loss -0.001378  0.000002\n",
            "Iteration 00103    Loss -0.001972  0.000002\n",
            "Iteration 00104    Loss -0.002531  0.000002\n",
            "Iteration 00105    Loss -0.002882  0.000002\n",
            "Iteration 00106    Loss -0.003354  0.000002\n",
            "Iteration 00107    Loss -0.003809  0.000002\n",
            "Iteration 00108    Loss -0.004313  0.000002\n",
            "Iteration 00109    Loss -0.004775  0.000002\n",
            "Iteration 00110    Loss -0.005138  0.000002\n",
            "Iteration 00111    Loss -0.005564  0.000002\n",
            "Iteration 00112    Loss -0.005947  0.000003\n",
            "Iteration 00113    Loss -0.006349  0.000003\n",
            "Iteration 00114    Loss -0.006728  0.000003\n",
            "Iteration 00115    Loss -0.007078  0.000003\n",
            "Iteration 00116    Loss -0.007498  0.000003\n",
            "Iteration 00117    Loss -0.007884  0.000004\n",
            "Iteration 00118    Loss -0.008233  0.000004\n",
            "Iteration 00119    Loss -0.008713  0.000003\n",
            "Iteration 00120    Loss -0.009027  0.000003\n",
            "Iteration 00121    Loss -0.009337  0.000003\n",
            "Iteration 00122    Loss -0.009665  0.000003\n",
            "Iteration 00123    Loss -0.010032  0.000003\n",
            "Iteration 00124    Loss -0.010355  0.000004\n",
            "Iteration 00125    Loss -0.010598  0.000004\n",
            "Iteration 00126    Loss -0.010929  0.000004\n",
            "Iteration 00127    Loss -0.011409  0.000003\n",
            "Iteration 00128    Loss -0.011697  0.000003\n",
            "Iteration 00129    Loss -0.011973  0.000003\n",
            "Iteration 00130    Loss -0.012328  0.000003\n",
            "Iteration 00131    Loss -0.012753  0.000003\n",
            "Iteration 00132    Loss -0.013055  0.000004\n",
            "Iteration 00133    Loss -0.013296  0.000004\n",
            "Iteration 00134    Loss -0.013586  0.000004\n",
            "Iteration 00135    Loss -0.013960  0.000004\n",
            "Iteration 00136    Loss -0.014192  0.000004\n",
            "Iteration 00137    Loss -0.014481  0.000004\n",
            "Iteration 00138    Loss -0.015057  0.000004\n",
            "Iteration 00139    Loss -0.015322  0.000004\n",
            "Iteration 00140    Loss -0.015620  0.000004\n",
            "Iteration 00141    Loss -0.015936  0.000004\n",
            "Iteration 00142    Loss -0.016168  0.000004\n",
            "Iteration 00143    Loss -0.016484  0.000004\n",
            "Iteration 00144    Loss -0.016731  0.000004\n",
            "Iteration 00145    Loss -0.017056  0.000004\n",
            "Iteration 00146    Loss -0.017249  0.000004\n",
            "Iteration 00147    Loss -0.017510  0.000004\n",
            "Iteration 00148    Loss -0.017727  0.000004\n",
            "Iteration 00149    Loss -0.018007  0.000004\n",
            "Iteration 00150    Loss -0.018297  0.000004\n",
            "Iteration 00151    Loss -0.018530  0.000005\n",
            "Iteration 00152    Loss -0.018832  0.000005\n",
            "Iteration 00153    Loss -0.019051  0.000005\n",
            "Iteration 00154    Loss -0.019328  0.000005\n",
            "Iteration 00155    Loss -0.019529  0.000005\n",
            "Iteration 00156    Loss -0.019667  0.000005\n",
            "Iteration 00157    Loss -0.019736  0.000005\n",
            "Iteration 00158    Loss -0.020173  0.000005\n",
            "Iteration 00159    Loss -0.020331  0.000005\n",
            "Iteration 00160    Loss -0.020560  0.000006\n",
            "Iteration 00161    Loss -0.020747  0.000006\n",
            "Iteration 00162    Loss -0.020995  0.000006\n",
            "Iteration 00163    Loss -0.021227  0.000006\n",
            "Iteration 00164    Loss -0.021452  0.000006\n",
            "Iteration 00165    Loss -0.021705  0.000006\n",
            "Iteration 00166    Loss -0.021857  0.000006\n",
            "Iteration 00167    Loss -0.021916  0.000007\n",
            "Iteration 00168    Loss -0.022209  0.000007\n",
            "Iteration 00169    Loss -0.022525  0.000007\n",
            "Iteration 00170    Loss -0.022751  0.000007\n",
            "Iteration 00171    Loss -0.022942  0.000007\n",
            "Iteration 00172    Loss -0.023091  0.000007\n",
            "Iteration 00173    Loss -0.023219  0.000007\n",
            "Iteration 00174    Loss -0.023449  0.000007\n",
            "Iteration 00175    Loss -0.023749  0.000006\n",
            "Iteration 00176    Loss -0.023885  0.000007\n",
            "Iteration 00177    Loss -0.024031  0.000007\n",
            "Iteration 00178    Loss -0.024217  0.000007\n",
            "Iteration 00179    Loss -0.024394  0.000007\n",
            "Iteration 00180    Loss -0.024546  0.000008\n",
            "Iteration 00181    Loss -0.024808  0.000007\n",
            "Iteration 00182    Loss -0.024997  0.000008\n",
            "Iteration 00183    Loss -0.025176  0.000008\n",
            "Iteration 00184    Loss -0.025400  0.000009\n",
            "Iteration 00185    Loss -0.025445  0.000009\n",
            "Iteration 00186    Loss -0.025694  0.000008\n",
            "Iteration 00187    Loss -0.025909  0.000009\n",
            "Iteration 00188    Loss -0.025973  0.000009\n",
            "Iteration 00189    Loss -0.026165  0.000010\n",
            "Iteration 00190    Loss -0.026295  0.000010\n",
            "Iteration 00191    Loss -0.026444  0.000010\n",
            "Iteration 00192    Loss -0.026605  0.000010\n",
            "Iteration 00193    Loss -0.026767  0.000009\n",
            "Iteration 00194    Loss -0.027006  0.000010\n",
            "Iteration 00195    Loss -0.027091  0.000011\n",
            "Iteration 00196    Loss -0.027210  0.000011\n",
            "Iteration 00197    Loss -0.027417  0.000010\n",
            "Iteration 00198    Loss -0.027541  0.000009\n",
            "Iteration 00199    Loss -0.027748  0.000010\n",
            "Iteration 00200    Loss -0.027809  0.000011\n",
            "Iteration 00201    Loss -0.027985  0.000011\n",
            "Iteration 00202    Loss -0.028219  0.000012\n",
            "Iteration 00203    Loss -0.028304  0.000012\n",
            "Iteration 00204    Loss -0.028406  0.000011\n",
            "Iteration 00205    Loss -0.028627  0.000011\n",
            "Iteration 00206    Loss -0.028754  0.000013\n",
            "Iteration 00207    Loss -0.028921  0.000013\n",
            "Iteration 00208    Loss -0.029076  0.000013\n",
            "Iteration 00209    Loss -0.029153  0.000014\n",
            "Iteration 00210    Loss -0.029276  0.000013\n",
            "Iteration 00211    Loss -0.029347  0.000012\n",
            "Iteration 00212    Loss -0.029520  0.000014\n",
            "Iteration 00213    Loss -0.029615  0.000014\n",
            "Iteration 00214    Loss -0.029642  0.000013\n",
            "Iteration 00215    Loss -0.029969  0.000013\n",
            "Iteration 00216    Loss -0.029968  0.000013\n",
            "Iteration 00217    Loss -0.030242  0.000013\n",
            "Iteration 00218    Loss -0.030165  0.000014\n",
            "Iteration 00219    Loss -0.030404  0.000015\n",
            "Iteration 00220    Loss -0.030592  0.000015\n",
            "Iteration 00221    Loss -0.030773  0.000016\n",
            "Iteration 00222    Loss -0.030697  0.000016\n",
            "Iteration 00223    Loss -0.030977  0.000017\n",
            "Iteration 00224    Loss -0.031002  0.000017\n",
            "Iteration 00225    Loss -0.031063  0.000016\n",
            "Iteration 00226    Loss -0.031259  0.000017\n",
            "Iteration 00227    Loss -0.031415  0.000017\n",
            "Iteration 00228    Loss -0.031513  0.000016\n",
            "Iteration 00229    Loss -0.031637  0.000017\n",
            "Iteration 00230    Loss -0.031707  0.000018\n",
            "Iteration 00231    Loss -0.031845  0.000018\n",
            "Iteration 00232    Loss -0.031781  0.000018\n",
            "Iteration 00233    Loss -0.032222  0.000018\n",
            "Iteration 00234    Loss -0.032235  0.000019\n",
            "Iteration 00235    Loss -0.032372  0.000019\n",
            "Iteration 00236    Loss -0.032316  0.000019\n",
            "Iteration 00237    Loss -0.032462  0.000020\n",
            "Iteration 00238    Loss -0.032684  0.000020\n",
            "Iteration 00239    Loss -0.032608  0.000022\n",
            "Iteration 00240    Loss -0.032829  0.000023\n",
            "Iteration 00241    Loss -0.032779  0.000022\n",
            "Iteration 00242    Loss -0.033033  0.000022\n",
            "Iteration 00243    Loss -0.033111  0.000021\n",
            "Iteration 00244    Loss -0.033192  0.000020\n",
            "Iteration 00245    Loss -0.033389  0.000019\n",
            "Iteration 00246    Loss -0.033375  0.000019\n",
            "Iteration 00247    Loss -0.033429  0.000022\n",
            "Iteration 00248    Loss -0.033411  0.000022\n",
            "Iteration 00249    Loss -0.033712  0.000020\n",
            "Iteration 00250    Loss -0.033615  0.000021\n",
            "Iteration 00251    Loss -0.033819  0.000022\n",
            "Iteration 00252    Loss -0.033978  0.000023\n",
            "Iteration 00253    Loss -0.034186  0.000020\n",
            "Iteration 00254    Loss -0.034223  0.000019\n",
            "Iteration 00255    Loss -0.034338  0.000022\n",
            "Iteration 00256    Loss -0.034457  0.000024\n",
            "Iteration 00257    Loss -0.034278  0.000023\n",
            "Iteration 00258    Loss -0.034664  0.000021\n",
            "Iteration 00259    Loss -0.034529  0.000023\n",
            "Iteration 00260    Loss -0.034795  0.000026\n",
            "Iteration 00261    Loss -0.034771  0.000027\n",
            "Iteration 00262    Loss -0.034990  0.000028\n",
            "Iteration 00263    Loss -0.034873  0.000028\n",
            "Iteration 00264    Loss -0.035086  0.000027\n",
            "Iteration 00265    Loss -0.035236  0.000028\n",
            "Iteration 00266    Loss -0.035325  0.000028\n",
            "Iteration 00267    Loss -0.035417  0.000027\n",
            "Iteration 00268    Loss -0.035533  0.000027\n",
            "Iteration 00269    Loss -0.035508  0.000029\n",
            "Iteration 00270    Loss -0.035698  0.000027\n",
            "Iteration 00271    Loss -0.035667  0.000026\n",
            "Iteration 00272    Loss -0.035828  0.000029\n",
            "Iteration 00273    Loss -0.035935  0.000031\n",
            "Iteration 00274    Loss -0.035947  0.000033\n",
            "Iteration 00275    Loss -0.036048  0.000031\n",
            "Iteration 00276    Loss -0.036112  0.000031\n",
            "Iteration 00277    Loss -0.036173  0.000031\n",
            "Iteration 00278    Loss -0.036435  0.000032\n",
            "Iteration 00279    Loss -0.036377  0.000033\n",
            "Iteration 00280    Loss -0.036547  0.000032\n",
            "Iteration 00281    Loss -0.036595  0.000035\n",
            "Iteration 00282    Loss -0.036546  0.000035\n",
            "Iteration 00283    Loss -0.036475  0.000036\n",
            "Iteration 00284    Loss -0.036840  0.000034\n",
            "Iteration 00285    Loss -0.036585  0.000036\n",
            "Iteration 00286    Loss -0.036939  0.000035\n",
            "Iteration 00287    Loss -0.036886  0.000034\n",
            "Iteration 00288    Loss -0.037069  0.000032\n",
            "Iteration 00289    Loss -0.037136  0.000039\n",
            "Iteration 00290    Loss -0.036957  0.000034\n",
            "Iteration 00291    Loss -0.037307  0.000035\n",
            "Iteration 00292    Loss -0.037118  0.000037\n",
            "Iteration 00293    Loss -0.037354  0.000038\n",
            "Iteration 00294    Loss -0.037498  0.000037\n",
            "Iteration 00295    Loss -0.037491  0.000039\n",
            "Iteration 00296    Loss -0.037616  0.000039\n",
            "Iteration 00297    Loss -0.037667  0.000038\n",
            "Iteration 00298    Loss -0.037857  0.000039\n",
            "Iteration 00299    Loss -0.037871  0.000039\n",
            "Iteration 00300    Loss -0.037663  0.000037\n",
            "Iteration 00301    Loss -0.037626  0.000037\n",
            "Iteration 00302    Loss -0.037855  0.000040\n",
            "Iteration 00303    Loss -0.037851  0.000039\n",
            "Iteration 00304    Loss -0.038066  0.000037\n",
            "Iteration 00305    Loss -0.038082  0.000039\n",
            "Iteration 00306    Loss -0.038163  0.000039\n",
            "Iteration 00307    Loss -0.038284  0.000038\n",
            "Iteration 00308    Loss -0.038390  0.000040\n",
            "Iteration 00309    Loss -0.038261  0.000038\n",
            "Iteration 00310    Loss -0.038530  0.000037\n",
            "Iteration 00311    Loss -0.038286  0.000037\n",
            "Iteration 00312    Loss -0.038707  0.000035\n",
            "Iteration 00313    Loss -0.038736  0.000038\n",
            "Iteration 00314    Loss -0.038772  0.000039\n",
            "Iteration 00315    Loss -0.038837  0.000042\n",
            "Iteration 00316    Loss -0.038909  0.000042\n",
            "Iteration 00317    Loss -0.038772  0.000040\n",
            "Iteration 00318    Loss -0.039054  0.000044\n",
            "Iteration 00319    Loss -0.038969  0.000042\n",
            "Iteration 00320    Loss -0.039035  0.000042\n",
            "Iteration 00321    Loss -0.039144  0.000043\n",
            "Iteration 00322    Loss -0.039288  0.000045\n",
            "Iteration 00323    Loss -0.039338  0.000045\n",
            "Iteration 00324    Loss -0.039360  0.000044\n",
            "Iteration 00325    Loss -0.039206  0.000039\n",
            "Iteration 00326    Loss -0.039463  0.000045\n",
            "Iteration 00327    Loss -0.039524  0.000050\n",
            "Iteration 00328    Loss -0.039611  0.000046\n",
            "Iteration 00329    Loss -0.039592  0.000042\n",
            "Iteration 00330    Loss -0.039561  0.000044\n",
            "Iteration 00331    Loss -0.039469  0.000049\n",
            "Iteration 00332    Loss -0.039754  0.000048\n",
            "Iteration 00333    Loss -0.039942  0.000047\n",
            "Iteration 00334    Loss -0.039856  0.000051\n",
            "Iteration 00335    Loss -0.039858  0.000055\n",
            "Iteration 00336    Loss -0.039790  0.000053\n",
            "Iteration 00337    Loss -0.040007  0.000053\n",
            "Iteration 00338    Loss -0.040036  0.000053\n",
            "Iteration 00339    Loss -0.040119  0.000048\n",
            "Iteration 00340    Loss -0.040052  0.000047\n",
            "Iteration 00341    Loss -0.040225  0.000049\n",
            "Iteration 00342    Loss -0.040236  0.000049\n",
            "Iteration 00343    Loss -0.040393  0.000047\n",
            "Iteration 00344    Loss -0.040365  0.000047\n",
            "Iteration 00345    Loss -0.040355  0.000049\n",
            "Iteration 00346    Loss -0.040440  0.000050\n",
            "Iteration 00347    Loss -0.040583  0.000049\n",
            "Iteration 00348    Loss -0.040571  0.000054\n",
            "Iteration 00349    Loss -0.040428  0.000051\n",
            "Iteration 00350    Loss -0.040538  0.000050\n",
            "Iteration 00351    Loss -0.040573  0.000054\n",
            "Iteration 00352    Loss -0.040698  0.000052\n",
            "Iteration 00353    Loss -0.040574  0.000046\n",
            "Iteration 00354    Loss -0.040775  0.000047\n",
            "Iteration 00355    Loss -0.040600  0.000051\n",
            "Iteration 00356    Loss -0.040934  0.000052\n",
            "Iteration 00357    Loss -0.040583  0.000047\n",
            "Iteration 00358    Loss -0.041062  0.000047\n",
            "Iteration 00359    Loss -0.040977  0.000051\n",
            "Iteration 00360    Loss -0.041054  0.000051\n",
            "Iteration 00361    Loss -0.041146  0.000052\n",
            "Iteration 00362    Loss -0.041210  0.000051\n",
            "Iteration 00363    Loss -0.041204  0.000053\n",
            "Iteration 00364    Loss -0.041030  0.000057\n",
            "Iteration 00365    Loss -0.041305  0.000055\n",
            "Iteration 00366    Loss -0.041172  0.000056\n",
            "Iteration 00367    Loss -0.041446  0.000047\n",
            "Iteration 00368    Loss -0.041471  0.000047\n",
            "Iteration 00369    Loss -0.041514  0.000050\n",
            "Iteration 00370    Loss -0.041441  0.000053\n",
            "Iteration 00371    Loss -0.041581  0.000056\n",
            "Iteration 00372    Loss -0.041207  0.000054\n",
            "Iteration 00373    Loss -0.041571  0.000054\n",
            "Iteration 00374    Loss -0.041532  0.000053\n",
            "Iteration 00375    Loss -0.041651  0.000049\n",
            "Iteration 00376    Loss -0.041529  0.000056\n",
            "Iteration 00377    Loss -0.041718  0.000056\n",
            "Iteration 00378    Loss -0.041777  0.000055\n",
            "Iteration 00379    Loss -0.041875  0.000053\n",
            "Iteration 00380    Loss -0.041901  0.000053\n",
            "Iteration 00381    Loss -0.041877  0.000055\n",
            "Iteration 00382    Loss -0.041785  0.000053\n",
            "Iteration 00383    Loss -0.041661  0.000056\n",
            "Iteration 00384    Loss -0.042030  0.000055\n",
            "Iteration 00385    Loss -0.042092  0.000051\n",
            "Iteration 00386    Loss -0.042035  0.000054\n",
            "Iteration 00387    Loss -0.041982  0.000052\n",
            "Iteration 00388    Loss -0.042205  0.000057\n",
            "Iteration 00389    Loss -0.042162  0.000054\n",
            "Iteration 00390    Loss -0.042179  0.000057\n",
            "Iteration 00391    Loss -0.042256  0.000055\n",
            "Iteration 00392    Loss -0.042353  0.000054\n",
            "Iteration 00393    Loss -0.042320  0.000058\n",
            "Iteration 00394    Loss -0.042380  0.000058\n",
            "Iteration 00395    Loss -0.042383  0.000054\n",
            "Iteration 00396    Loss -0.042502  0.000058\n",
            "Iteration 00397    Loss -0.042396  0.000055\n",
            "Iteration 00398    Loss -0.042580  0.000059\n",
            "Iteration 00399    Loss -0.042559  0.000063\n",
            "Iteration 00400    Loss -0.042573  0.000062\n",
            "Iteration 00401    Loss -0.042581  0.000064\n",
            "Iteration 00402    Loss -0.042701  0.000053\n",
            "Iteration 00403    Loss -0.042708  0.000054\n",
            "Iteration 00404    Loss -0.042772  0.000059\n",
            "Iteration 00405    Loss -0.042790  0.000059\n",
            "Iteration 00406    Loss -0.042688  0.000059\n",
            "Iteration 00407    Loss -0.042787  0.000059\n",
            "Iteration 00408    Loss -0.042805  0.000058\n",
            "Iteration 00409    Loss -0.042748  0.000058\n",
            "Iteration 00410    Loss -0.042884  0.000054\n",
            "Iteration 00411    Loss -0.042812  0.000055\n",
            "Iteration 00412    Loss -0.042782  0.000060\n",
            "Iteration 00413    Loss -0.042982  0.000060\n",
            "Iteration 00414    Loss -0.043012  0.000061\n",
            "Iteration 00415    Loss -0.042993  0.000058\n",
            "Iteration 00416    Loss -0.043027  0.000049\n",
            "Iteration 00417    Loss -0.043150  0.000058\n",
            "Iteration 00418    Loss -0.043170  0.000061\n",
            "Iteration 00419    Loss -0.043117  0.000059\n",
            "Iteration 00420    Loss -0.043089  0.000053\n",
            "Iteration 00421    Loss -0.043241  0.000056\n",
            "Iteration 00422    Loss -0.043258  0.000063\n",
            "Iteration 00423    Loss -0.043315  0.000057\n",
            "Iteration 00424    Loss -0.043316  0.000057\n",
            "Iteration 00425    Loss -0.043339  0.000064\n",
            "Iteration 00426    Loss -0.043350  0.000062\n",
            "Iteration 00427    Loss -0.043431  0.000064\n",
            "Iteration 00428    Loss -0.043318  0.000066\n",
            "Iteration 00429    Loss -0.043479  0.000062\n",
            "Iteration 00430    Loss -0.043515  0.000061\n",
            "Iteration 00431    Loss -0.043553  0.000062\n",
            "Iteration 00432    Loss -0.043522  0.000064\n",
            "Iteration 00433    Loss -0.043555  0.000061\n",
            "Iteration 00434    Loss -0.043662  0.000064\n",
            "Iteration 00435    Loss -0.043656  0.000067\n",
            "Iteration 00436    Loss -0.043638  0.000061\n",
            "Iteration 00437    Loss -0.043615  0.000067\n",
            "Iteration 00438    Loss -0.043611  0.000063\n",
            "Iteration 00439    Loss -0.043702  0.000060\n",
            "Iteration 00440    Loss -0.043767  0.000058\n",
            "Iteration 00441    Loss -0.043675  0.000062\n",
            "Iteration 00442    Loss -0.043775  0.000067\n",
            "Iteration 00443    Loss -0.043866  0.000066\n",
            "Iteration 00444    Loss -0.043863  0.000066\n",
            "Iteration 00445    Loss -0.043865  0.000070\n",
            "Iteration 00446    Loss -0.043837  0.000062\n",
            "Iteration 00447    Loss -0.043833  0.000062\n",
            "Iteration 00448    Loss -0.043836  0.000059\n",
            "Iteration 00449    Loss -0.043908  0.000058\n",
            "Iteration 00450    Loss -0.043932  0.000057\n",
            "Iteration 00451    Loss -0.043981  0.000061\n",
            "Iteration 00452    Loss -0.043954  0.000064\n",
            "Iteration 00453    Loss -0.043971  0.000048\n",
            "Iteration 00454    Loss -0.043825  0.000043\n",
            "Iteration 00455    Loss -0.044077  0.000043\n",
            "Iteration 00456    Loss -0.044030  0.000054\n",
            "Iteration 00457    Loss -0.043996  0.000046\n",
            "Iteration 00458    Loss -0.044089  0.000052\n",
            "Iteration 00459    Loss -0.044025  0.000057\n",
            "Iteration 00460    Loss -0.044166  0.000043\n",
            "Iteration 00461    Loss -0.044082  0.000040\n",
            "Iteration 00462    Loss -0.044143  0.000049\n",
            "Iteration 00463    Loss -0.044210  0.000067\n",
            "Iteration 00464    Loss -0.044052  0.000031\n",
            "Iteration 00465    Loss -0.043919  0.000025\n",
            "Iteration 00466    Loss -0.044187  0.000028\n",
            "Iteration 00467    Loss -0.044211  0.000034\n",
            "Iteration 00468    Loss -0.044154  0.000040\n",
            "Iteration 00469    Loss -0.044251  0.000042\n",
            "Iteration 00470    Loss -0.044194  0.000035\n",
            "Iteration 00471    Loss -0.044337  0.000030\n",
            "Iteration 00472    Loss -0.044283  0.000030\n",
            "Iteration 00473    Loss -0.044276  0.000037\n",
            "Iteration 00474    Loss -0.044381  0.000043\n",
            "Iteration 00475    Loss -0.044428  0.000045\n",
            "Iteration 00476    Loss -0.044483  0.000037\n",
            "Iteration 00477    Loss -0.044494  0.000035\n",
            "Iteration 00478    Loss -0.044429  0.000039\n",
            "Iteration 00479    Loss -0.044555  0.000041\n",
            "Iteration 00480    Loss -0.044531  0.000039\n",
            "Iteration 00481    Loss -0.044583  0.000039\n",
            "Iteration 00482    Loss -0.044574  0.000040\n",
            "Iteration 00483    Loss -0.044604  0.000046\n",
            "Iteration 00484    Loss -0.044621  0.000054\n",
            "Iteration 00485    Loss -0.044473  0.000045\n",
            "Iteration 00486    Loss -0.044648  0.000044\n",
            "Iteration 00487    Loss -0.044692  0.000040\n",
            "Iteration 00488    Loss -0.044711  0.000037\n",
            "Iteration 00489    Loss -0.044700  0.000037\n",
            "Iteration 00490    Loss -0.044762  0.000045\n",
            "Iteration 00491    Loss -0.044745  0.000052\n",
            "Iteration 00492    Loss -0.044724  0.000058\n",
            "Iteration 00493    Loss -0.044725  0.000060\n",
            "Iteration 00494    Loss -0.044787  0.000057\n",
            "Iteration 00495    Loss -0.044837  0.000058\n",
            "Iteration 00496    Loss -0.044860  0.000060\n",
            "Iteration 00497    Loss -0.044870  0.000058\n",
            "Iteration 00498    Loss -0.044765  0.000059\n",
            "Iteration 00499    Loss -0.044860  0.000059\n",
            " 90% 60/67 [33:00<03:48, 32.61s/it]/content/ZID/REAL/train_input.png\n",
            "Iteration 00000    Loss 0.422251  0.000019\n",
            "Iteration 00001    Loss 13.929593  0.000005\n",
            "Iteration 00002    Loss 0.315005  0.000002\n",
            "Iteration 00003    Loss 0.757046  0.000002\n",
            "Iteration 00004    Loss 0.236051  0.000001\n",
            "Iteration 00005    Loss 0.199070  0.000002\n",
            "Iteration 00006    Loss 0.175948  0.000001\n",
            "Iteration 00007    Loss 0.158161  0.000002\n",
            "Iteration 00008    Loss 0.144120  0.000002\n",
            "Iteration 00009    Loss 0.134229  0.000002\n",
            "Iteration 00010    Loss 0.124636  0.000003\n",
            "Iteration 00011    Loss 0.117460  0.000003\n",
            "Iteration 00012    Loss 0.111982  0.000003\n",
            "Iteration 00013    Loss 0.107682  0.000004\n",
            "Iteration 00014    Loss 0.103649  0.000003\n",
            "Iteration 00015    Loss 0.100469  0.000003\n",
            "Iteration 00016    Loss 0.097741  0.000003\n",
            "Iteration 00017    Loss 0.095172  0.000003\n",
            "Iteration 00018    Loss 0.093353  0.000002\n",
            "Iteration 00019    Loss 0.091687  0.000003\n",
            "Iteration 00020    Loss 0.090016  0.000003\n",
            "Iteration 00021    Loss 0.088466  0.000003\n",
            "Iteration 00022    Loss 0.087047  0.000003\n",
            "Iteration 00023    Loss 0.085916  0.000003\n",
            "Iteration 00024    Loss 0.084520  0.000003\n",
            "Iteration 00025    Loss 0.083287  0.000003\n",
            "Iteration 00026    Loss 0.082372  0.000004\n",
            "Iteration 00027    Loss 0.081122  0.000003\n",
            "Iteration 00028    Loss 0.080027  0.000004\n",
            "Iteration 00029    Loss 0.078970  0.000004\n",
            "Iteration 00030    Loss 0.077836  0.000004\n",
            "Iteration 00031    Loss 0.076775  0.000004\n",
            "Iteration 00032    Loss 0.075744  0.000004\n",
            "Iteration 00033    Loss 0.074709  0.000004\n",
            "Iteration 00034    Loss 0.073696  0.000005\n",
            "Iteration 00035    Loss 0.072642  0.000005\n",
            "Iteration 00036    Loss 0.071692  0.000005\n",
            "Iteration 00037    Loss 0.070681  0.000006\n",
            "Iteration 00038    Loss 0.069676  0.000006\n",
            "Iteration 00039    Loss 0.068736  0.000007\n",
            "Iteration 00040    Loss 0.067770  0.000007\n",
            "Iteration 00041    Loss 0.066770  0.000008\n",
            "Iteration 00042    Loss 0.065899  0.000006\n",
            "Iteration 00043    Loss 0.065003  0.000007\n",
            "Iteration 00044    Loss 0.063934  0.000006\n",
            "Iteration 00045    Loss 0.063016  0.000007\n",
            "Iteration 00046    Loss 0.062154  0.000009\n",
            "Iteration 00047    Loss 0.061238  0.000008\n",
            "Iteration 00048    Loss 0.060289  0.000007\n",
            "Iteration 00049    Loss 0.059424  0.000008\n",
            "Iteration 00050    Loss 0.058480  0.000008\n",
            "Iteration 00051    Loss 0.057638  0.000009\n",
            "Iteration 00052    Loss 0.056737  0.000009\n",
            "Iteration 00053    Loss 0.055898  0.000010\n",
            "Iteration 00054    Loss 0.055046  0.000010\n",
            "Iteration 00055    Loss 0.054221  0.000010\n",
            "Iteration 00056    Loss 0.053366  0.000012\n",
            "Iteration 00057    Loss 0.052452  0.000013\n",
            "Iteration 00058    Loss 0.051688  0.000014\n",
            "Iteration 00059    Loss 0.050911  0.000015\n",
            "Iteration 00060    Loss 0.050080  0.000014\n",
            "Iteration 00061    Loss 0.049247  0.000014\n",
            "Iteration 00062    Loss 0.048480  0.000015\n",
            "Iteration 00063    Loss 0.047679  0.000014\n",
            "Iteration 00064    Loss 0.046748  0.000015\n",
            "Iteration 00065    Loss 0.045715  0.000015\n",
            "Iteration 00066    Loss 0.044656  0.000015\n",
            "Iteration 00067    Loss 0.043742  0.000015\n",
            "Iteration 00068    Loss 0.042836  0.000015\n",
            "Iteration 00069    Loss 0.041907  0.000015\n",
            "Iteration 00070    Loss 0.040949  0.000015\n",
            "Iteration 00071    Loss 0.040014  0.000015\n",
            "Iteration 00072    Loss 0.039106  0.000015\n",
            "Iteration 00073    Loss 0.038158  0.000015\n",
            "Iteration 00074    Loss 0.037270  0.000016\n",
            "Iteration 00075    Loss 0.036414  0.000016\n",
            "Iteration 00076    Loss 0.035574  0.000016\n",
            "Iteration 00077    Loss 0.034751  0.000017\n",
            "Iteration 00078    Loss 0.033954  0.000016\n",
            "Iteration 00079    Loss 0.033188  0.000017\n",
            "Iteration 00080    Loss 0.032424  0.000017\n",
            "Iteration 00081    Loss 0.031649  0.000017\n",
            "Iteration 00082    Loss 0.030909  0.000017\n",
            "Iteration 00083    Loss 0.030163  0.000017\n",
            "Iteration 00084    Loss 0.029382  0.000017\n",
            "Iteration 00085    Loss 0.028689  0.000017\n",
            "Iteration 00086    Loss 0.027988  0.000017\n",
            "Iteration 00087    Loss 0.027308  0.000018\n",
            "Iteration 00088    Loss 0.026575  0.000017\n",
            "Iteration 00089    Loss 0.025873  0.000018\n",
            "Iteration 00090    Loss 0.025243  0.000018\n",
            "Iteration 00091    Loss 0.024606  0.000018\n",
            "Iteration 00092    Loss 0.023881  0.000018\n",
            "Iteration 00093    Loss 0.023221  0.000019\n",
            "Iteration 00094    Loss 0.022610  0.000019\n",
            "Iteration 00095    Loss 0.021912  0.000019\n",
            "Iteration 00096    Loss 0.021254  0.000019\n",
            "Iteration 00097    Loss 0.020644  0.000019\n",
            "Iteration 00098    Loss 0.020015  0.000020\n",
            "Iteration 00099    Loss 0.019452  0.000019\n",
            "Iteration 00100    Loss 0.018793  0.000021\n",
            "Iteration 00101    Loss 0.018193  0.000021\n",
            "Iteration 00102    Loss 0.017564  0.000021\n",
            "Iteration 00103    Loss 0.017010  0.000021\n",
            "Iteration 00104    Loss 0.016397  0.000021\n",
            "Iteration 00105    Loss 0.015769  0.000021\n",
            "Iteration 00106    Loss 0.015260  0.000022\n",
            "Iteration 00107    Loss 0.014633  0.000022\n",
            "Iteration 00108    Loss 0.014063  0.000023\n",
            "Iteration 00109    Loss 0.013482  0.000023\n",
            "Iteration 00110    Loss 0.012894  0.000023\n",
            "Iteration 00111    Loss 0.012386  0.000023\n",
            "Iteration 00112    Loss 0.011804  0.000023\n",
            "Iteration 00113    Loss 0.011258  0.000024\n",
            "Iteration 00114    Loss 0.010707  0.000024\n",
            "Iteration 00115    Loss 0.010196  0.000024\n",
            "Iteration 00116    Loss 0.009665  0.000025\n",
            "Iteration 00117    Loss 0.009125  0.000025\n",
            "Iteration 00118    Loss 0.008627  0.000024\n",
            "Iteration 00119    Loss 0.008102  0.000026\n",
            "Iteration 00120    Loss 0.007618  0.000026\n",
            "Iteration 00121    Loss 0.007061  0.000026\n",
            "Iteration 00122    Loss 0.006558  0.000028\n",
            "Iteration 00123    Loss 0.006149  0.000027\n",
            "Iteration 00124    Loss 0.005603  0.000028\n",
            "Iteration 00125    Loss 0.005126  0.000030\n",
            "Iteration 00126    Loss 0.004630  0.000027\n",
            "Iteration 00127    Loss 0.004180  0.000029\n",
            "Iteration 00128    Loss 0.003715  0.000028\n",
            "Iteration 00129    Loss 0.003265  0.000028\n",
            "Iteration 00130    Loss 0.002793  0.000030\n",
            "Iteration 00131    Loss 0.002384  0.000030\n",
            "Iteration 00132    Loss 0.001868  0.000031\n",
            "Iteration 00133    Loss 0.001324  0.000032\n",
            "Iteration 00134    Loss 0.000937  0.000032\n",
            "Iteration 00135    Loss 0.000436  0.000034\n",
            "Iteration 00136    Loss -0.000053  0.000035\n",
            "Iteration 00137    Loss -0.000482  0.000032\n",
            "Iteration 00138    Loss -0.000917  0.000034\n",
            "Iteration 00139    Loss -0.001380  0.000036\n",
            "Iteration 00140    Loss -0.001794  0.000036\n",
            "Iteration 00141    Loss -0.002207  0.000041\n",
            "Iteration 00142    Loss -0.002739  0.000040\n",
            "Iteration 00143    Loss -0.003106  0.000042\n",
            "Iteration 00144    Loss -0.003518  0.000037\n",
            "Iteration 00145    Loss -0.003921  0.000044\n",
            "Iteration 00146    Loss -0.004323  0.000039\n",
            "Iteration 00147    Loss -0.004671  0.000046\n",
            "Iteration 00148    Loss -0.004791  0.000038\n",
            "Iteration 00149    Loss -0.005380  0.000033\n",
            "Iteration 00150    Loss -0.005702  0.000039\n",
            "Iteration 00151    Loss -0.006041  0.000040\n",
            "Iteration 00152    Loss -0.006588  0.000036\n",
            "Iteration 00153    Loss -0.006891  0.000036\n",
            "Iteration 00154    Loss -0.007286  0.000038\n",
            "Iteration 00155    Loss -0.007684  0.000044\n",
            "Iteration 00156    Loss -0.008033  0.000042\n",
            "Iteration 00157    Loss -0.008446  0.000041\n",
            "Iteration 00158    Loss -0.008836  0.000042\n",
            "Iteration 00159    Loss -0.009151  0.000047\n",
            "Iteration 00160    Loss -0.009479  0.000049\n",
            "Iteration 00161    Loss -0.009876  0.000045\n",
            "Iteration 00162    Loss -0.010234  0.000045\n",
            "Iteration 00163    Loss -0.010554  0.000048\n",
            "Iteration 00164    Loss -0.010902  0.000048\n",
            "Iteration 00165    Loss -0.011244  0.000051\n",
            "Iteration 00166    Loss -0.011551  0.000052\n",
            "Iteration 00167    Loss -0.011909  0.000053\n",
            "Iteration 00168    Loss -0.012217  0.000056\n",
            "Iteration 00169    Loss -0.012537  0.000055\n",
            "Iteration 00170    Loss -0.012877  0.000059\n",
            "Iteration 00171    Loss -0.013178  0.000060\n",
            "Iteration 00172    Loss -0.013454  0.000060\n",
            "Iteration 00173    Loss -0.013830  0.000059\n",
            "Iteration 00174    Loss -0.014092  0.000064\n",
            "Iteration 00175    Loss -0.014404  0.000064\n",
            "Iteration 00176    Loss -0.014675  0.000064\n",
            "Iteration 00177    Loss -0.014980  0.000065\n",
            "Iteration 00178    Loss -0.015219  0.000070\n",
            "Iteration 00179    Loss -0.015553  0.000070\n",
            "Iteration 00180    Loss -0.015837  0.000074\n",
            "Iteration 00181    Loss -0.016075  0.000066\n",
            "Iteration 00182    Loss -0.016370  0.000085\n",
            "Iteration 00183    Loss -0.016586  0.000055\n",
            "Iteration 00184    Loss -0.016870  0.000078\n",
            "Iteration 00185    Loss -0.017121  0.000095\n",
            "Iteration 00186    Loss -0.017327  0.000057\n",
            "Iteration 00187    Loss -0.017658  0.000062\n",
            "Iteration 00188    Loss -0.017883  0.000091\n",
            "Iteration 00189    Loss -0.018154  0.000087\n",
            "Iteration 00190    Loss -0.018386  0.000071\n",
            "Iteration 00191    Loss -0.018610  0.000077\n",
            "Iteration 00192    Loss -0.018886  0.000088\n",
            "Iteration 00193    Loss -0.019071  0.000086\n",
            "Iteration 00194    Loss -0.019271  0.000085\n",
            "Iteration 00195    Loss -0.019575  0.000085\n",
            "Iteration 00196    Loss -0.019915  0.000089\n",
            "Iteration 00197    Loss -0.020085  0.000093\n",
            "Iteration 00198    Loss -0.020298  0.000088\n",
            "Iteration 00199    Loss -0.020531  0.000087\n",
            "Iteration 00200    Loss -0.020808  0.000093\n",
            "Iteration 00201    Loss -0.021011  0.000105\n",
            "Iteration 00202    Loss -0.021230  0.000098\n",
            "Iteration 00203    Loss -0.021481  0.000102\n",
            "Iteration 00204    Loss -0.021641  0.000102\n",
            "Iteration 00205    Loss -0.021900  0.000101\n",
            "Iteration 00206    Loss -0.022104  0.000124\n",
            "Iteration 00207    Loss -0.022263  0.000082\n",
            "Iteration 00208    Loss -0.022503  0.000108\n",
            "Iteration 00209    Loss -0.022677  0.000155\n",
            "Iteration 00210    Loss -0.022774  0.000067\n",
            "Iteration 00211    Loss -0.022988  0.000066\n",
            "Iteration 00212    Loss -0.023203  0.000086\n",
            "Iteration 00213    Loss -0.023464  0.000111\n",
            "Iteration 00214    Loss -0.023630  0.000114\n",
            "Iteration 00215    Loss -0.023801  0.000101\n",
            "Iteration 00216    Loss -0.024010  0.000104\n",
            "Iteration 00217    Loss -0.024237  0.000105\n",
            "Iteration 00218    Loss -0.024422  0.000103\n",
            "Iteration 00219    Loss -0.024603  0.000116\n",
            "Iteration 00220    Loss -0.024783  0.000114\n",
            "Iteration 00221    Loss -0.024962  0.000107\n",
            "Iteration 00222    Loss -0.025152  0.000115\n",
            "Iteration 00223    Loss -0.025299  0.000124\n",
            "Iteration 00224    Loss -0.025524  0.000116\n",
            "Iteration 00225    Loss -0.025683  0.000121\n",
            "Iteration 00226    Loss -0.025866  0.000131\n",
            "Iteration 00227    Loss -0.026048  0.000134\n",
            "Iteration 00228    Loss -0.026215  0.000135\n",
            "Iteration 00229    Loss -0.026344  0.000139\n",
            "Iteration 00230    Loss -0.026534  0.000141\n",
            "Iteration 00231    Loss -0.026706  0.000143\n",
            "Iteration 00232    Loss -0.026861  0.000151\n",
            "Iteration 00233    Loss -0.027011  0.000143\n",
            "Iteration 00234    Loss -0.027199  0.000160\n",
            "Iteration 00235    Loss -0.027343  0.000137\n",
            "Iteration 00236    Loss -0.027485  0.000190\n",
            "Iteration 00237    Loss -0.027556  0.000104\n",
            "Iteration 00238    Loss -0.027755  0.000167\n",
            "Iteration 00239    Loss -0.027837  0.000175\n",
            "Iteration 00240    Loss -0.027866  0.000123\n",
            "Iteration 00241    Loss -0.028094  0.000155\n",
            "Iteration 00242    Loss -0.028242  0.000173\n",
            "Iteration 00243    Loss -0.028406  0.000125\n",
            "Iteration 00244    Loss -0.028572  0.000157\n",
            "Iteration 00245    Loss -0.028752  0.000176\n",
            "Iteration 00246    Loss -0.028870  0.000127\n",
            "Iteration 00247    Loss -0.029043  0.000155\n",
            "Iteration 00248    Loss -0.029182  0.000184\n",
            "Iteration 00249    Loss -0.029310  0.000138\n",
            "Iteration 00250    Loss -0.029463  0.000146\n",
            "Iteration 00251    Loss -0.029565  0.000178\n",
            "Iteration 00252    Loss -0.029773  0.000160\n",
            "Iteration 00253    Loss -0.029896  0.000156\n",
            "Iteration 00254    Loss -0.030019  0.000171\n",
            "Iteration 00255    Loss -0.030148  0.000170\n",
            "Iteration 00256    Loss -0.030300  0.000172\n",
            "Iteration 00257    Loss -0.030442  0.000169\n",
            "Iteration 00258    Loss -0.030566  0.000190\n",
            "Iteration 00259    Loss -0.030686  0.000166\n",
            "Iteration 00260    Loss -0.030799  0.000198\n",
            "Iteration 00261    Loss -0.030896  0.000163\n",
            "Iteration 00262    Loss -0.031007  0.000219\n",
            "Iteration 00263    Loss -0.031098  0.000136\n",
            "Iteration 00264    Loss -0.031269  0.000189\n",
            "Iteration 00265    Loss -0.031363  0.000216\n",
            "Iteration 00266    Loss -0.031469  0.000142\n",
            "Iteration 00267    Loss -0.031629  0.000169\n",
            "Iteration 00268    Loss -0.031723  0.000232\n",
            "Iteration 00269    Loss -0.031836  0.000134\n",
            "Iteration 00270    Loss -0.031995  0.000138\n",
            "Iteration 00271    Loss -0.032112  0.000203\n",
            "Iteration 00272    Loss -0.032227  0.000223\n",
            "Iteration 00273    Loss -0.032369  0.000151\n",
            "Iteration 00274    Loss -0.032477  0.000151\n",
            "Iteration 00275    Loss -0.032631  0.000200\n",
            "Iteration 00276    Loss -0.032719  0.000208\n",
            "Iteration 00277    Loss -0.032885  0.000172\n",
            "Iteration 00278    Loss -0.032989  0.000174\n",
            "Iteration 00279    Loss -0.033094  0.000195\n",
            "Iteration 00280    Loss -0.033214  0.000202\n",
            "Iteration 00281    Loss -0.033326  0.000190\n",
            "Iteration 00282    Loss -0.033435  0.000189\n",
            "Iteration 00283    Loss -0.033529  0.000198\n",
            "Iteration 00284    Loss -0.033645  0.000208\n",
            "Iteration 00285    Loss -0.033769  0.000198\n",
            "Iteration 00286    Loss -0.033900  0.000206\n",
            "Iteration 00287    Loss -0.034013  0.000212\n",
            "Iteration 00288    Loss -0.034092  0.000198\n",
            "Iteration 00289    Loss -0.034171  0.000214\n",
            "Iteration 00290    Loss -0.034244  0.000225\n",
            "Iteration 00291    Loss -0.034380  0.000203\n",
            "Iteration 00292    Loss -0.034481  0.000213\n",
            "Iteration 00293    Loss -0.034566  0.000225\n",
            "Iteration 00294    Loss -0.034669  0.000178\n",
            "Iteration 00295    Loss -0.034797  0.000231\n",
            "Iteration 00296    Loss -0.034926  0.000214\n",
            "Iteration 00297    Loss -0.035025  0.000191\n",
            "Iteration 00298    Loss -0.035078  0.000234\n",
            "Iteration 00299    Loss -0.035182  0.000210\n",
            "Iteration 00300    Loss -0.035315  0.000215\n",
            "Iteration 00301    Loss -0.035391  0.000217\n",
            "Iteration 00302    Loss -0.035496  0.000226\n",
            "Iteration 00303    Loss -0.035599  0.000222\n",
            "Iteration 00304    Loss -0.035653  0.000225\n",
            "Iteration 00305    Loss -0.035648  0.000205\n",
            "Iteration 00306    Loss -0.035550  0.000278\n",
            "Iteration 00307    Loss -0.035727  0.000158\n",
            "Iteration 00308    Loss -0.035748  0.000207\n",
            "Iteration 00309    Loss -0.035962  0.000256\n",
            "Iteration 00310    Loss -0.035942  0.000182\n",
            "Iteration 00311    Loss -0.036122  0.000187\n",
            "Iteration 00312    Loss -0.036183  0.000227\n",
            "Iteration 00313    Loss -0.036336  0.000208\n",
            "Iteration 00314    Loss -0.036405  0.000183\n",
            "Iteration 00315    Loss -0.036535  0.000200\n",
            "Iteration 00316    Loss -0.036614  0.000226\n",
            "Iteration 00317    Loss -0.036697  0.000200\n",
            "Iteration 00318    Loss -0.036807  0.000206\n",
            "Iteration 00319    Loss -0.036890  0.000215\n",
            "Iteration 00320    Loss -0.036985  0.000207\n",
            "Iteration 00321    Loss -0.037067  0.000210\n",
            "Iteration 00322    Loss -0.037161  0.000219\n",
            "Iteration 00323    Loss -0.037234  0.000204\n",
            "Iteration 00324    Loss -0.037312  0.000208\n",
            "Iteration 00325    Loss -0.037400  0.000227\n",
            "Iteration 00326    Loss -0.037481  0.000216\n",
            "Iteration 00327    Loss -0.037559  0.000216\n",
            "Iteration 00328    Loss -0.037647  0.000223\n",
            "Iteration 00329    Loss -0.037725  0.000218\n",
            "Iteration 00330    Loss -0.037798  0.000223\n",
            "Iteration 00331    Loss -0.037882  0.000231\n",
            "Iteration 00332    Loss -0.037959  0.000240\n",
            "Iteration 00333    Loss -0.038031  0.000212\n",
            "Iteration 00334    Loss -0.038098  0.000245\n",
            "Iteration 00335    Loss -0.038170  0.000220\n",
            "Iteration 00336    Loss -0.038237  0.000254\n",
            "Iteration 00337    Loss -0.038305  0.000213\n",
            "Iteration 00338    Loss -0.038367  0.000257\n",
            "Iteration 00339    Loss -0.038438  0.000211\n",
            "Iteration 00340    Loss -0.038518  0.000248\n",
            "Iteration 00341    Loss -0.038596  0.000230\n",
            "Iteration 00342    Loss -0.038658  0.000236\n",
            "Iteration 00343    Loss -0.038697  0.000262\n",
            "Iteration 00344    Loss -0.038777  0.000220\n",
            "Iteration 00345    Loss -0.038845  0.000252\n",
            "Iteration 00346    Loss -0.038915  0.000229\n",
            "Iteration 00347    Loss -0.039001  0.000243\n",
            "Iteration 00348    Loss -0.039056  0.000254\n",
            "Iteration 00349    Loss -0.039107  0.000232\n",
            "Iteration 00350    Loss -0.039178  0.000263\n",
            "Iteration 00351    Loss -0.039214  0.000213\n",
            "Iteration 00352    Loss -0.039285  0.000271\n",
            "Iteration 00353    Loss -0.039315  0.000201\n",
            "Iteration 00354    Loss -0.039353  0.000266\n",
            "Iteration 00355    Loss -0.039394  0.000249\n",
            "Iteration 00356    Loss -0.039417  0.000192\n",
            "Iteration 00357    Loss -0.039529  0.000255\n",
            "Iteration 00358    Loss -0.039574  0.000225\n",
            "Iteration 00359    Loss -0.039627  0.000219\n",
            "Iteration 00360    Loss -0.039703  0.000264\n",
            "Iteration 00361    Loss -0.039775  0.000218\n",
            "Iteration 00362    Loss -0.039818  0.000223\n",
            "Iteration 00363    Loss -0.039889  0.000241\n",
            "Iteration 00364    Loss -0.039955  0.000234\n",
            "Iteration 00365    Loss -0.040001  0.000241\n",
            "Iteration 00366    Loss -0.040058  0.000226\n",
            "Iteration 00367    Loss -0.040113  0.000247\n",
            "Iteration 00368    Loss -0.040186  0.000244\n",
            "Iteration 00369    Loss -0.040224  0.000225\n",
            "Iteration 00370    Loss -0.040299  0.000245\n",
            "Iteration 00371    Loss -0.040347  0.000247\n",
            "Iteration 00372    Loss -0.040400  0.000241\n",
            "Iteration 00373    Loss -0.040451  0.000244\n",
            "Iteration 00374    Loss -0.040512  0.000249\n",
            "Iteration 00375    Loss -0.040556  0.000235\n",
            "Iteration 00376    Loss -0.040585  0.000270\n",
            "Iteration 00377    Loss -0.040604  0.000213\n",
            "Iteration 00378    Loss -0.040643  0.000254\n",
            "Iteration 00379    Loss -0.040669  0.000233\n",
            "Iteration 00380    Loss -0.040735  0.000240\n",
            "Iteration 00381    Loss -0.040839  0.000242\n",
            "Iteration 00382    Loss -0.040883  0.000248\n",
            "Iteration 00383    Loss -0.040900  0.000228\n",
            "Iteration 00384    Loss -0.040953  0.000247\n",
            "Iteration 00385    Loss -0.041022  0.000241\n",
            "Iteration 00386    Loss -0.041067  0.000248\n",
            "Iteration 00387    Loss -0.041101  0.000235\n",
            "Iteration 00388    Loss -0.041169  0.000256\n",
            "Iteration 00389    Loss -0.041220  0.000243\n",
            "Iteration 00390    Loss -0.041260  0.000244\n",
            "Iteration 00391    Loss -0.041296  0.000249\n",
            "Iteration 00392    Loss -0.041349  0.000238\n",
            "Iteration 00393    Loss -0.041364  0.000248\n",
            "Iteration 00394    Loss -0.041420  0.000246\n",
            "Iteration 00395    Loss -0.041458  0.000238\n",
            "Iteration 00396    Loss -0.041521  0.000248\n",
            "Iteration 00397    Loss -0.041568  0.000236\n",
            "Iteration 00398    Loss -0.041615  0.000273\n",
            "Iteration 00399    Loss -0.041649  0.000221\n",
            "Iteration 00400    Loss -0.041688  0.000270\n",
            "Iteration 00401    Loss -0.041730  0.000219\n",
            "Iteration 00402    Loss -0.041773  0.000271\n",
            "Iteration 00403    Loss -0.041833  0.000236\n",
            "Iteration 00404    Loss -0.041866  0.000256\n",
            "Iteration 00405    Loss -0.041896  0.000251\n",
            "Iteration 00406    Loss -0.041948  0.000244\n",
            "Iteration 00407    Loss -0.041984  0.000254\n",
            "Iteration 00408    Loss -0.042004  0.000251\n",
            "Iteration 00409    Loss -0.042012  0.000239\n",
            "Iteration 00410    Loss -0.042020  0.000268\n",
            "Iteration 00411    Loss -0.042003  0.000209\n",
            "Iteration 00412    Loss -0.042092  0.000232\n",
            "Iteration 00413    Loss -0.042159  0.000276\n",
            "Iteration 00414    Loss -0.042187  0.000199\n",
            "Iteration 00415    Loss -0.042262  0.000235\n",
            "Iteration 00416    Loss -0.042266  0.000283\n",
            "Iteration 00417    Loss -0.042311  0.000213\n",
            "Iteration 00418    Loss -0.042370  0.000229\n",
            "Iteration 00419    Loss -0.042406  0.000260\n",
            "Iteration 00420    Loss -0.042450  0.000242\n",
            "Iteration 00421    Loss -0.042493  0.000224\n",
            "Iteration 00422    Loss -0.042525  0.000241\n",
            "Iteration 00423    Loss -0.042566  0.000253\n",
            "Iteration 00424    Loss -0.042608  0.000238\n",
            "Iteration 00425    Loss -0.042645  0.000245\n",
            "Iteration 00426    Loss -0.042687  0.000246\n",
            "Iteration 00427    Loss -0.042726  0.000253\n",
            "Iteration 00428    Loss -0.042761  0.000245\n",
            "Iteration 00429    Loss -0.042802  0.000250\n",
            "Iteration 00430    Loss -0.042840  0.000248\n",
            "Iteration 00431    Loss -0.042872  0.000261\n",
            "Iteration 00432    Loss -0.042916  0.000254\n",
            "Iteration 00433    Loss -0.042948  0.000251\n",
            "Iteration 00434    Loss -0.042983  0.000256\n",
            "Iteration 00435    Loss -0.043013  0.000245\n",
            "Iteration 00436    Loss -0.043042  0.000279\n",
            "Iteration 00437    Loss -0.043066  0.000231\n",
            "Iteration 00438    Loss -0.043086  0.000272\n",
            "Iteration 00439    Loss -0.043105  0.000233\n",
            "Iteration 00440    Loss -0.043148  0.000269\n",
            "Iteration 00441    Loss -0.043190  0.000253\n",
            "Iteration 00442    Loss -0.043218  0.000231\n",
            "Iteration 00443    Loss -0.043239  0.000287\n",
            "Iteration 00444    Loss -0.043278  0.000214\n",
            "Iteration 00445    Loss -0.043331  0.000271\n",
            "Iteration 00446    Loss -0.043343  0.000284\n",
            "Iteration 00447    Loss -0.043353  0.000207\n",
            "Iteration 00448    Loss -0.043394  0.000274\n",
            "Iteration 00449    Loss -0.043445  0.000270\n",
            "Iteration 00450    Loss -0.043454  0.000226\n",
            "Iteration 00451    Loss -0.043483  0.000276\n",
            "Iteration 00452    Loss -0.043515  0.000261\n",
            "Iteration 00453    Loss -0.043527  0.000229\n",
            "Iteration 00454    Loss -0.043553  0.000271\n",
            "Iteration 00455    Loss -0.043602  0.000243\n",
            "Iteration 00456    Loss -0.043655  0.000250\n",
            "Iteration 00457    Loss -0.043676  0.000277\n",
            "Iteration 00458    Loss -0.043702  0.000241\n",
            "Iteration 00459    Loss -0.043727  0.000263\n",
            "Iteration 00460    Loss -0.043751  0.000268\n",
            "Iteration 00461    Loss -0.043785  0.000245\n",
            "Iteration 00462    Loss -0.043801  0.000283\n",
            "Iteration 00463    Loss -0.043797  0.000234\n",
            "Iteration 00464    Loss -0.043827  0.000256\n",
            "Iteration 00465    Loss -0.043782  0.000285\n",
            "Iteration 00466    Loss -0.043810  0.000217\n",
            "Iteration 00467    Loss -0.043878  0.000231\n",
            "Iteration 00468    Loss -0.043926  0.000265\n",
            "Iteration 00469    Loss -0.043971  0.000235\n",
            "Iteration 00470    Loss -0.043987  0.000256\n",
            "Iteration 00471    Loss -0.044023  0.000246\n",
            "Iteration 00472    Loss -0.044058  0.000242\n",
            "Iteration 00473    Loss -0.044092  0.000265\n",
            "Iteration 00474    Loss -0.044106  0.000245\n",
            "Iteration 00475    Loss -0.044154  0.000261\n",
            "Iteration 00476    Loss -0.044186  0.000266\n",
            "Iteration 00477    Loss -0.044205  0.000240\n",
            "Iteration 00478    Loss -0.044235  0.000268\n",
            "Iteration 00479    Loss -0.044259  0.000270\n",
            "Iteration 00480    Loss -0.044299  0.000257\n",
            "Iteration 00481    Loss -0.044321  0.000256\n",
            "Iteration 00482    Loss -0.044355  0.000258\n",
            "Iteration 00483    Loss -0.044372  0.000271\n",
            "Iteration 00484    Loss -0.044399  0.000262\n",
            "Iteration 00485    Loss -0.044418  0.000269\n",
            "Iteration 00486    Loss -0.044436  0.000244\n",
            "Iteration 00487    Loss -0.044448  0.000291\n",
            "Iteration 00488    Loss -0.044458  0.000232\n",
            "Iteration 00489    Loss -0.044486  0.000268\n",
            "Iteration 00490    Loss -0.044504  0.000273\n",
            "Iteration 00491    Loss -0.044535  0.000237\n",
            "Iteration 00492    Loss -0.044585  0.000270\n",
            "Iteration 00493    Loss -0.044613  0.000274\n",
            "Iteration 00494    Loss -0.044626  0.000249\n",
            "Iteration 00495    Loss -0.044649  0.000277\n",
            "Iteration 00496    Loss -0.044669  0.000262\n",
            "Iteration 00497    Loss -0.044687  0.000262\n",
            "Iteration 00498    Loss -0.044719  0.000283\n",
            "Iteration 00499    Loss -0.044741  0.000244\n",
            " 91% 61/67 [33:32<03:14, 32.46s/it]/content/ZID/REAL/22.png\n",
            "Iteration 00000    Loss 0.312077  0.000022\n",
            "Iteration 00001    Loss 13.668237  0.000007\n",
            "Iteration 00002    Loss 0.607175  0.000002\n",
            "Iteration 00003    Loss 0.180134  0.000001\n",
            "Iteration 00004    Loss 0.294738  0.000001\n",
            "Iteration 00005    Loss 0.132283  0.000000\n",
            "Iteration 00006    Loss 0.113507  0.000000\n",
            "Iteration 00007    Loss 0.102200  0.000000\n",
            "Iteration 00008    Loss 0.094232  0.000000\n",
            "Iteration 00009    Loss 0.088393  0.000000\n",
            "Iteration 00010    Loss 0.083971  0.000000\n",
            "Iteration 00011    Loss 0.080272  0.000000\n",
            "Iteration 00012    Loss 0.087738  0.000000\n",
            "Iteration 00013    Loss 0.076275  0.000000\n",
            "Iteration 00014    Loss 0.075060  0.000000\n",
            "Iteration 00015    Loss 0.074045  0.000000\n",
            "Iteration 00016    Loss 0.073313  0.000000\n",
            "Iteration 00017    Loss 0.072673  0.000001\n",
            "Iteration 00018    Loss 0.071928  0.000000\n",
            "Iteration 00019    Loss 0.070959  0.000001\n",
            "Iteration 00020    Loss 0.070086  0.000001\n",
            "Iteration 00021    Loss 0.069029  0.000002\n",
            "Iteration 00022    Loss 0.068025  0.000002\n",
            "Iteration 00023    Loss 0.067175  0.000002\n",
            "Iteration 00024    Loss 0.065708  0.000003\n",
            "Iteration 00025    Loss 0.064690  0.000004\n",
            "Iteration 00026    Loss 0.063797  0.000004\n",
            "Iteration 00027    Loss 0.062544  0.000005\n",
            "Iteration 00028    Loss 0.061447  0.000007\n",
            "Iteration 00029    Loss 0.060111  0.000008\n",
            "Iteration 00030    Loss 0.059080  0.000009\n",
            "Iteration 00031    Loss 0.057853  0.000011\n",
            "Iteration 00032    Loss 0.056814  0.000014\n",
            "Iteration 00033    Loss 0.055218  0.000015\n",
            "Iteration 00034    Loss 0.054161  0.000016\n",
            "Iteration 00035    Loss 0.052812  0.000018\n",
            "Iteration 00036    Loss 0.051755  0.000022\n",
            "Iteration 00037    Loss 0.050611  0.000020\n",
            "Iteration 00038    Loss 0.048999  0.000021\n",
            "Iteration 00039    Loss 0.048095  0.000022\n",
            "Iteration 00040    Loss 0.047085  0.000018\n",
            "Iteration 00041    Loss 0.045983  0.000021\n",
            "Iteration 00042    Loss 0.044825  0.000025\n",
            "Iteration 00043    Loss 0.043920  0.000024\n",
            "Iteration 00044    Loss 0.042837  0.000025\n",
            "Iteration 00045    Loss 0.041952  0.000031\n",
            "Iteration 00046    Loss 0.040969  0.000033\n",
            "Iteration 00047    Loss 0.040360  0.000032\n",
            "Iteration 00048    Loss 0.039185  0.000031\n",
            "Iteration 00049    Loss 0.038295  0.000034\n",
            "Iteration 00050    Loss 0.037623  0.000035\n",
            "Iteration 00051    Loss 0.036755  0.000034\n",
            "Iteration 00052    Loss 0.036103  0.000036\n",
            "Iteration 00053    Loss 0.035204  0.000043\n",
            "Iteration 00054    Loss 0.034611  0.000043\n",
            "Iteration 00055    Loss 0.033685  0.000039\n",
            "Iteration 00056    Loss 0.033020  0.000041\n",
            "Iteration 00057    Loss 0.032168  0.000045\n",
            "Iteration 00058    Loss 0.031611  0.000044\n",
            "Iteration 00059    Loss 0.030783  0.000044\n",
            "Iteration 00060    Loss 0.030454  0.000051\n",
            "Iteration 00061    Loss 0.029808  0.000054\n",
            "Iteration 00062    Loss 0.028983  0.000051\n",
            "Iteration 00063    Loss 0.028052  0.000054\n",
            "Iteration 00064    Loss 0.027618  0.000059\n",
            "Iteration 00065    Loss 0.026916  0.000061\n",
            "Iteration 00066    Loss 0.026375  0.000063\n",
            "Iteration 00067    Loss 0.025565  0.000068\n",
            "Iteration 00068    Loss 0.025004  0.000061\n",
            "Iteration 00069    Loss 0.024328  0.000069\n",
            "Iteration 00070    Loss 0.023897  0.000079\n",
            "Iteration 00071    Loss 0.023413  0.000078\n",
            "Iteration 00072    Loss 0.022766  0.000080\n",
            "Iteration 00073    Loss 0.022050  0.000086\n",
            "Iteration 00074    Loss 0.021610  0.000084\n",
            "Iteration 00075    Loss 0.021076  0.000093\n",
            "Iteration 00076    Loss 0.020669  0.000103\n",
            "Iteration 00077    Loss 0.019963  0.000104\n",
            "Iteration 00078    Loss 0.019405  0.000109\n",
            "Iteration 00079    Loss 0.018773  0.000106\n",
            "Iteration 00080    Loss 0.018360  0.000118\n",
            "Iteration 00081    Loss 0.017752  0.000128\n",
            "Iteration 00082    Loss 0.017270  0.000140\n",
            "Iteration 00083    Loss 0.016561  0.000126\n",
            "Iteration 00084    Loss 0.016254  0.000132\n",
            "Iteration 00085    Loss 0.015963  0.000130\n",
            "Iteration 00086    Loss 0.015266  0.000139\n",
            "Iteration 00087    Loss 0.014673  0.000156\n",
            "Iteration 00088    Loss 0.014248  0.000148\n",
            "Iteration 00089    Loss 0.013779  0.000143\n",
            "Iteration 00090    Loss 0.013451  0.000156\n",
            "Iteration 00091    Loss 0.012850  0.000147\n",
            "Iteration 00092    Loss 0.012125  0.000161\n",
            "Iteration 00093    Loss 0.011792  0.000165\n",
            "Iteration 00094    Loss 0.011156  0.000171\n",
            "Iteration 00095    Loss 0.010925  0.000189\n",
            "Iteration 00096    Loss 0.010665  0.000194\n",
            "Iteration 00097    Loss 0.010085  0.000177\n",
            "Iteration 00098    Loss 0.009471  0.000195\n",
            "Iteration 00099    Loss 0.009202  0.000190\n",
            "Iteration 00100    Loss 0.008780  0.000197\n",
            "Iteration 00101    Loss 0.008317  0.000186\n",
            "Iteration 00102    Loss 0.008158  0.000208\n",
            "Iteration 00103    Loss 0.007545  0.000181\n",
            "Iteration 00104    Loss 0.007289  0.000218\n",
            "Iteration 00105    Loss 0.006638  0.000212\n",
            "Iteration 00106    Loss 0.006405  0.000208\n",
            "Iteration 00107    Loss 0.006003  0.000214\n",
            "Iteration 00108    Loss 0.005513  0.000220\n",
            "Iteration 00109    Loss 0.004971  0.000211\n",
            "Iteration 00110    Loss 0.004529  0.000227\n",
            "Iteration 00111    Loss 0.005223  0.000224\n",
            "Iteration 00112    Loss 0.004282  0.000238\n",
            "Iteration 00113    Loss 0.003727  0.000248\n",
            "Iteration 00114    Loss 0.003617  0.000199\n",
            "Iteration 00115    Loss 0.002749  0.000227\n",
            "Iteration 00116    Loss 0.002967  0.000270\n",
            "Iteration 00117    Loss 0.002393  0.000236\n",
            "Iteration 00118    Loss 0.001563  0.000245\n",
            "Iteration 00119    Loss 0.001682  0.000247\n",
            "Iteration 00120    Loss 0.001217  0.000239\n",
            "Iteration 00121    Loss 0.000907  0.000248\n",
            "Iteration 00122    Loss 0.000608  0.000264\n",
            "Iteration 00123    Loss 0.000130  0.000260\n",
            "Iteration 00124    Loss 0.000072  0.000281\n",
            "Iteration 00125    Loss -0.000313  0.000269\n",
            "Iteration 00126    Loss -0.000902  0.000276\n",
            "Iteration 00127    Loss -0.000823  0.000289\n",
            "Iteration 00128    Loss -0.001538  0.000285\n",
            "Iteration 00129    Loss -0.002156  0.000280\n",
            "Iteration 00130    Loss -0.002149  0.000304\n",
            "Iteration 00131    Loss -0.002407  0.000256\n",
            "Iteration 00132    Loss -0.003116  0.000311\n",
            "Iteration 00133    Loss -0.003053  0.000321\n",
            "Iteration 00134    Loss -0.003362  0.000268\n",
            "Iteration 00135    Loss -0.003856  0.000287\n",
            "Iteration 00136    Loss -0.003950  0.000311\n",
            "Iteration 00137    Loss -0.004475  0.000300\n",
            "Iteration 00138    Loss -0.004395  0.000306\n",
            "Iteration 00139    Loss -0.005447  0.000323\n",
            "Iteration 00140    Loss -0.005155  0.000340\n",
            "Iteration 00141    Loss -0.005745  0.000328\n",
            "Iteration 00142    Loss -0.005571  0.000300\n",
            "Iteration 00143    Loss -0.006256  0.000317\n",
            "Iteration 00144    Loss -0.005976  0.000335\n",
            "Iteration 00145    Loss -0.006634  0.000321\n",
            "Iteration 00146    Loss -0.007073  0.000310\n",
            "Iteration 00147    Loss -0.007276  0.000345\n",
            "Iteration 00148    Loss -0.007251  0.000374\n",
            "Iteration 00149    Loss -0.008316  0.000336\n",
            "Iteration 00150    Loss -0.008132  0.000356\n",
            "Iteration 00151    Loss -0.008765  0.000335\n",
            "Iteration 00152    Loss -0.008372  0.000327\n",
            "Iteration 00153    Loss -0.008754  0.000351\n",
            "Iteration 00154    Loss -0.009038  0.000337\n",
            "Iteration 00155    Loss -0.008913  0.000294\n",
            "Iteration 00156    Loss -0.010154  0.000335\n",
            "Iteration 00157    Loss -0.009911  0.000371\n",
            "Iteration 00158    Loss -0.010107  0.000369\n",
            "Iteration 00159    Loss -0.010307  0.000348\n",
            "Iteration 00160    Loss -0.011214  0.000357\n",
            "Iteration 00161    Loss -0.011376  0.000372\n",
            "Iteration 00162    Loss -0.011370  0.000381\n",
            "Iteration 00163    Loss -0.011400  0.000395\n",
            "Iteration 00164    Loss -0.011669  0.000380\n",
            "Iteration 00165    Loss -0.011369  0.000348\n",
            "Iteration 00166    Loss -0.012326  0.000363\n",
            "Iteration 00167    Loss -0.012236  0.000394\n",
            "Iteration 00168    Loss -0.012837  0.000406\n",
            "Iteration 00169    Loss -0.012887  0.000420\n",
            "Iteration 00170    Loss -0.012767  0.000348\n",
            "Iteration 00171    Loss -0.012160  0.000386\n",
            "Iteration 00172    Loss -0.013806  0.000430\n",
            "Iteration 00173    Loss -0.014301  0.000404\n",
            "Iteration 00174    Loss -0.014526  0.000392\n",
            "Iteration 00175    Loss -0.014642  0.000419\n",
            "Iteration 00176    Loss -0.014820  0.000423\n",
            "Iteration 00177    Loss -0.015175  0.000438\n",
            "Iteration 00178    Loss -0.015016  0.000420\n",
            "Iteration 00179    Loss -0.015464  0.000416\n",
            "Iteration 00180    Loss -0.015796  0.000390\n",
            "Iteration 00181    Loss -0.015832  0.000429\n",
            "Iteration 00182    Loss -0.016010  0.000475\n",
            "Iteration 00183    Loss -0.016180  0.000424\n",
            "Iteration 00184    Loss -0.016091  0.000448\n",
            "Iteration 00185    Loss -0.016526  0.000451\n",
            "Iteration 00186    Loss -0.016613  0.000379\n",
            "Iteration 00187    Loss -0.016747  0.000462\n",
            "Iteration 00188    Loss -0.017362  0.000500\n",
            "Iteration 00189    Loss -0.017602  0.000411\n",
            "Iteration 00190    Loss -0.017713  0.000480\n",
            "Iteration 00191    Loss -0.017793  0.000489\n",
            "Iteration 00192    Loss -0.018182  0.000440\n",
            "Iteration 00193    Loss -0.017841  0.000465\n",
            "Iteration 00194    Loss -0.017537  0.000487\n",
            "Iteration 00195    Loss -0.018716  0.000512\n",
            "Iteration 00196    Loss -0.018727  0.000430\n",
            "Iteration 00197    Loss -0.019054  0.000457\n",
            "Iteration 00198    Loss -0.018817  0.000544\n",
            "Iteration 00199    Loss -0.018803  0.000466\n",
            "Iteration 00200    Loss -0.019586  0.000413\n",
            "Iteration 00201    Loss -0.019642  0.000461\n",
            "Iteration 00202    Loss -0.019742  0.000536\n",
            "Iteration 00203    Loss -0.020220  0.000525\n",
            "Iteration 00204    Loss -0.020269  0.000508\n",
            "Iteration 00205    Loss -0.020236  0.000486\n",
            "Iteration 00206    Loss -0.020361  0.000505\n",
            "Iteration 00207    Loss -0.020851  0.000494\n",
            "Iteration 00208    Loss -0.020619  0.000490\n",
            "Iteration 00209    Loss -0.021336  0.000552\n",
            "Iteration 00210    Loss -0.021367  0.000570\n",
            "Iteration 00211    Loss -0.021050  0.000504\n",
            "Iteration 00212    Loss -0.021705  0.000501\n",
            "Iteration 00213    Loss -0.021912  0.000566\n",
            "Iteration 00214    Loss -0.021936  0.000535\n",
            "Iteration 00215    Loss -0.022274  0.000529\n",
            "Iteration 00216    Loss -0.021932  0.000541\n",
            "Iteration 00217    Loss -0.022544  0.000581\n",
            "Iteration 00218    Loss -0.022797  0.000529\n",
            "Iteration 00219    Loss -0.022864  0.000547\n",
            "Iteration 00220    Loss -0.023109  0.000589\n",
            "Iteration 00221    Loss -0.023159  0.000542\n",
            "Iteration 00222    Loss -0.022856  0.000563\n",
            "Iteration 00223    Loss -0.023554  0.000575\n",
            "Iteration 00224    Loss -0.022446  0.000564\n",
            "Iteration 00225    Loss -0.023907  0.000549\n",
            "Iteration 00226    Loss -0.024126  0.000586\n",
            "Iteration 00227    Loss -0.024197  0.000565\n",
            "Iteration 00228    Loss -0.024123  0.000568\n",
            "Iteration 00229    Loss -0.024395  0.000594\n",
            "Iteration 00230    Loss -0.024673  0.000573\n",
            "Iteration 00231    Loss -0.023961  0.000600\n",
            "Iteration 00232    Loss -0.024838  0.000617\n",
            "Iteration 00233    Loss -0.024790  0.000567\n",
            "Iteration 00234    Loss -0.025219  0.000589\n",
            "Iteration 00235    Loss -0.024847  0.000624\n",
            "Iteration 00236    Loss -0.025317  0.000612\n",
            "Iteration 00237    Loss -0.025398  0.000615\n",
            "Iteration 00238    Loss -0.025591  0.000584\n",
            "Iteration 00239    Loss -0.025440  0.000627\n",
            "Iteration 00240    Loss -0.025336  0.000574\n",
            "Iteration 00241    Loss -0.025788  0.000673\n",
            "Iteration 00242    Loss -0.026137  0.000518\n",
            "Iteration 00243    Loss -0.026293  0.000581\n",
            "Iteration 00244    Loss -0.026371  0.000633\n",
            "Iteration 00245    Loss -0.026568  0.000581\n",
            "Iteration 00246    Loss -0.026732  0.000631\n",
            "Iteration 00247    Loss -0.026680  0.000596\n",
            "Iteration 00248    Loss -0.026667  0.000581\n",
            "Iteration 00249    Loss -0.027066  0.000670\n",
            "Iteration 00250    Loss -0.027266  0.000614\n",
            "Iteration 00251    Loss -0.027354  0.000573\n",
            "Iteration 00252    Loss -0.027506  0.000640\n",
            "Iteration 00253    Loss -0.027697  0.000627\n",
            "Iteration 00254    Loss -0.027579  0.000653\n",
            "Iteration 00255    Loss -0.027673  0.000660\n",
            "Iteration 00256    Loss -0.027960  0.000626\n",
            "Iteration 00257    Loss -0.028164  0.000661\n",
            "Iteration 00258    Loss -0.028246  0.000653\n",
            "Iteration 00259    Loss -0.028322  0.000624\n",
            "Iteration 00260    Loss -0.028474  0.000663\n",
            "Iteration 00261    Loss -0.027412  0.000688\n",
            "Iteration 00262    Loss -0.028687  0.000674\n",
            "Iteration 00263    Loss -0.028368  0.000646\n",
            "Iteration 00264    Loss -0.028863  0.000669\n",
            "Iteration 00265    Loss -0.028977  0.000661\n",
            "Iteration 00266    Loss -0.029070  0.000612\n",
            "Iteration 00267    Loss -0.028962  0.000728\n",
            "Iteration 00268    Loss -0.029327  0.000621\n",
            "Iteration 00269    Loss -0.029446  0.000680\n",
            "Iteration 00270    Loss -0.029024  0.000668\n",
            "Iteration 00271    Loss -0.029338  0.000686\n",
            "Iteration 00272    Loss -0.029484  0.000666\n",
            "Iteration 00273    Loss -0.029680  0.000692\n",
            "Iteration 00274    Loss -0.029759  0.000622\n",
            "Iteration 00275    Loss -0.029822  0.000701\n",
            "Iteration 00276    Loss -0.029937  0.000667\n",
            "Iteration 00277    Loss -0.029032  0.000636\n",
            "Iteration 00278    Loss -0.030178  0.000674\n",
            "Iteration 00279    Loss -0.030043  0.000660\n",
            "Iteration 00280    Loss -0.030638  0.000729\n",
            "Iteration 00281    Loss -0.030701  0.000668\n",
            "Iteration 00282    Loss -0.030129  0.000660\n",
            "Iteration 00283    Loss -0.030719  0.000711\n",
            "Iteration 00284    Loss -0.030994  0.000763\n",
            "Iteration 00285    Loss -0.030951  0.000691\n",
            "Iteration 00286    Loss -0.031113  0.000665\n",
            "Iteration 00287    Loss -0.030858  0.000667\n",
            "Iteration 00288    Loss -0.031307  0.000611\n",
            "Iteration 00289    Loss -0.031154  0.000669\n",
            "Iteration 00290    Loss -0.031561  0.000699\n",
            "Iteration 00291    Loss -0.031594  0.000672\n",
            "Iteration 00292    Loss -0.031409  0.000637\n",
            "Iteration 00293    Loss -0.031391  0.000702\n",
            "Iteration 00294    Loss -0.031836  0.000742\n",
            "Iteration 00295    Loss -0.031878  0.000674\n",
            "Iteration 00296    Loss -0.032101  0.000685\n",
            "Iteration 00297    Loss -0.032007  0.000680\n",
            "Iteration 00298    Loss -0.032054  0.000671\n",
            "Iteration 00299    Loss -0.032213  0.000670\n",
            "Iteration 00300    Loss -0.032314  0.000685\n",
            "Iteration 00301    Loss -0.032355  0.000737\n",
            "Iteration 00302    Loss -0.032560  0.000682\n",
            "Iteration 00303    Loss -0.032464  0.000770\n",
            "Iteration 00304    Loss -0.032796  0.000709\n",
            "Iteration 00305    Loss -0.032788  0.000691\n",
            "Iteration 00306    Loss -0.033015  0.000767\n",
            "Iteration 00307    Loss -0.032600  0.000720\n",
            "Iteration 00308    Loss -0.033099  0.000706\n",
            "Iteration 00309    Loss -0.033275  0.000755\n",
            "Iteration 00310    Loss -0.033296  0.000776\n",
            "Iteration 00311    Loss -0.033401  0.000715\n",
            "Iteration 00312    Loss -0.033483  0.000749\n",
            "Iteration 00313    Loss -0.033515  0.000750\n",
            "Iteration 00314    Loss -0.033557  0.000754\n",
            "Iteration 00315    Loss -0.033660  0.000754\n",
            "Iteration 00316    Loss -0.033809  0.000731\n",
            "Iteration 00317    Loss -0.033926  0.000706\n",
            "Iteration 00318    Loss -0.034002  0.000809\n",
            "Iteration 00319    Loss -0.034123  0.000741\n",
            "Iteration 00320    Loss -0.033876  0.000737\n",
            "Iteration 00321    Loss -0.034231  0.000806\n",
            "Iteration 00322    Loss -0.034242  0.000728\n",
            "Iteration 00323    Loss -0.034269  0.000706\n",
            "Iteration 00324    Loss -0.034457  0.000800\n",
            "Iteration 00325    Loss -0.034589  0.000782\n",
            "Iteration 00326    Loss -0.034618  0.000809\n",
            "Iteration 00327    Loss -0.034712  0.000804\n",
            "Iteration 00328    Loss -0.034691  0.000727\n",
            "Iteration 00329    Loss -0.034811  0.000795\n",
            "Iteration 00330    Loss -0.034766  0.000805\n",
            "Iteration 00331    Loss -0.034933  0.000805\n",
            "Iteration 00332    Loss -0.034845  0.000741\n",
            "Iteration 00333    Loss -0.034851  0.000816\n",
            "Iteration 00334    Loss -0.034923  0.000798\n",
            "Iteration 00335    Loss -0.035254  0.000821\n",
            "Iteration 00336    Loss -0.035131  0.000744\n",
            "Iteration 00337    Loss -0.035294  0.000794\n",
            "Iteration 00338    Loss -0.035247  0.000823\n",
            "Iteration 00339    Loss -0.035585  0.000783\n",
            "Iteration 00340    Loss -0.035615  0.000821\n",
            "Iteration 00341    Loss -0.035655  0.000769\n",
            "Iteration 00342    Loss -0.035633  0.000829\n",
            "Iteration 00343    Loss -0.035757  0.000823\n",
            "Iteration 00344    Loss -0.035760  0.000789\n",
            "Iteration 00345    Loss -0.035817  0.000825\n",
            "Iteration 00346    Loss -0.035960  0.000818\n",
            "Iteration 00347    Loss -0.036126  0.000818\n",
            "Iteration 00348    Loss -0.036020  0.000825\n",
            "Iteration 00349    Loss -0.035856  0.000823\n",
            "Iteration 00350    Loss -0.036054  0.000847\n",
            "Iteration 00351    Loss -0.036267  0.000852\n",
            "Iteration 00352    Loss -0.036322  0.000750\n",
            "Iteration 00353    Loss -0.036208  0.000819\n",
            "Iteration 00354    Loss -0.036250  0.000854\n",
            "Iteration 00355    Loss -0.036436  0.000821\n",
            "Iteration 00356    Loss -0.036614  0.000838\n",
            "Iteration 00357    Loss -0.036666  0.000865\n",
            "Iteration 00358    Loss -0.036581  0.000817\n",
            "Iteration 00359    Loss -0.036717  0.000828\n",
            "Iteration 00360    Loss -0.036639  0.000878\n",
            "Iteration 00361    Loss -0.036857  0.000764\n",
            "Iteration 00362    Loss -0.036775  0.000813\n",
            "Iteration 00363    Loss -0.036752  0.000786\n",
            "Iteration 00364    Loss -0.037084  0.000857\n",
            "Iteration 00365    Loss -0.037087  0.000848\n",
            "Iteration 00366    Loss -0.036958  0.000758\n",
            "Iteration 00367    Loss -0.036872  0.000837\n",
            "Iteration 00368    Loss -0.037223  0.000894\n",
            "Iteration 00369    Loss -0.037300  0.000809\n",
            "Iteration 00370    Loss -0.037329  0.000829\n",
            "Iteration 00371    Loss -0.037384  0.000905\n",
            "Iteration 00372    Loss -0.037393  0.000844\n",
            "Iteration 00373    Loss -0.037453  0.000802\n",
            "Iteration 00374    Loss -0.037336  0.000885\n",
            "Iteration 00375    Loss -0.037603  0.000844\n",
            "Iteration 00376    Loss -0.037681  0.000900\n",
            "Iteration 00377    Loss -0.037735  0.000908\n",
            "Iteration 00378    Loss -0.037685  0.000808\n",
            "Iteration 00379    Loss -0.037876  0.000881\n",
            "Iteration 00380    Loss -0.037932  0.000899\n",
            "Iteration 00381    Loss -0.037965  0.000825\n",
            "Iteration 00382    Loss -0.038031  0.000917\n",
            "Iteration 00383    Loss -0.038080  0.000867\n",
            "Iteration 00384    Loss -0.038100  0.000879\n",
            "Iteration 00385    Loss -0.038105  0.000857\n",
            "Iteration 00386    Loss -0.038171  0.000877\n",
            "Iteration 00387    Loss -0.038299  0.000845\n",
            "Iteration 00388    Loss -0.038281  0.000889\n",
            "Iteration 00389    Loss -0.038372  0.000859\n",
            "Iteration 00390    Loss -0.038320  0.000926\n",
            "Iteration 00391    Loss -0.038430  0.000833\n",
            "Iteration 00392    Loss -0.038422  0.000818\n",
            "Iteration 00393    Loss -0.038573  0.000930\n",
            "Iteration 00394    Loss -0.038518  0.000888\n",
            "Iteration 00395    Loss -0.038599  0.000822\n",
            "Iteration 00396    Loss -0.038725  0.000860\n",
            "Iteration 00397    Loss -0.038699  0.000913\n",
            "Iteration 00398    Loss -0.038771  0.000861\n",
            "Iteration 00399    Loss -0.038758  0.000871\n",
            "Iteration 00400    Loss -0.038707  0.000914\n",
            "Iteration 00401    Loss -0.038839  0.000855\n",
            "Iteration 00402    Loss -0.038825  0.000867\n",
            "Iteration 00403    Loss -0.038953  0.000889\n",
            "Iteration 00404    Loss -0.039023  0.000925\n",
            "Iteration 00405    Loss -0.039026  0.000892\n",
            "Iteration 00406    Loss -0.039156  0.000910\n",
            "Iteration 00407    Loss -0.039207  0.000876\n",
            "Iteration 00408    Loss -0.039308  0.000949\n",
            "Iteration 00409    Loss -0.039070  0.000923\n",
            "Iteration 00410    Loss -0.039356  0.000869\n",
            "Iteration 00411    Loss -0.039328  0.000978\n",
            "Iteration 00412    Loss -0.039356  0.000873\n",
            "Iteration 00413    Loss -0.039304  0.000940\n",
            "Iteration 00414    Loss -0.039396  0.000930\n",
            "Iteration 00415    Loss -0.039469  0.000798\n",
            "Iteration 00416    Loss -0.039532  0.000960\n",
            "Iteration 00417    Loss -0.039173  0.000931\n",
            "Iteration 00418    Loss -0.039540  0.000905\n",
            "Iteration 00419    Loss -0.039666  0.000916\n",
            "Iteration 00420    Loss -0.039674  0.000894\n",
            "Iteration 00421    Loss -0.039488  0.001039\n",
            "Iteration 00422    Loss -0.039671  0.000823\n",
            "Iteration 00423    Loss -0.039702  0.000765\n",
            "Iteration 00424    Loss -0.039821  0.000922\n",
            "Iteration 00425    Loss -0.039865  0.000910\n",
            "Iteration 00426    Loss -0.039905  0.000883\n",
            "Iteration 00427    Loss -0.039835  0.000893\n",
            "Iteration 00428    Loss -0.039958  0.000849\n",
            "Iteration 00429    Loss -0.040004  0.000887\n",
            "Iteration 00430    Loss -0.039962  0.000941\n",
            "Iteration 00431    Loss -0.040165  0.000890\n",
            "Iteration 00432    Loss -0.039988  0.000866\n",
            "Iteration 00433    Loss -0.040239  0.000908\n",
            "Iteration 00434    Loss -0.040265  0.000932\n",
            "Iteration 00435    Loss -0.040185  0.000933\n",
            "Iteration 00436    Loss -0.040307  0.000897\n",
            "Iteration 00437    Loss -0.040388  0.000950\n",
            "Iteration 00438    Loss -0.040454  0.000936\n",
            "Iteration 00439    Loss -0.040389  0.000922\n",
            "Iteration 00440    Loss -0.040544  0.000953\n",
            "Iteration 00441    Loss -0.040601  0.000924\n",
            "Iteration 00442    Loss -0.040635  0.000913\n",
            "Iteration 00443    Loss -0.040626  0.000997\n",
            "Iteration 00444    Loss -0.040690  0.000943\n",
            "Iteration 00445    Loss -0.040757  0.000986\n",
            "Iteration 00446    Loss -0.040649  0.000905\n",
            "Iteration 00447    Loss -0.040813  0.000975\n",
            "Iteration 00448    Loss -0.040741  0.000974\n",
            "Iteration 00449    Loss -0.040844  0.000939\n",
            "Iteration 00450    Loss -0.040861  0.000933\n",
            "Iteration 00451    Loss -0.040796  0.000972\n",
            "Iteration 00452    Loss -0.040961  0.000939\n",
            "Iteration 00453    Loss -0.040980  0.001027\n",
            "Iteration 00454    Loss -0.041056  0.000923\n",
            "Iteration 00455    Loss -0.041068  0.000937\n",
            "Iteration 00456    Loss -0.041119  0.001045\n",
            "Iteration 00457    Loss -0.041044  0.000967\n",
            "Iteration 00458    Loss -0.041052  0.001025\n",
            "Iteration 00459    Loss -0.041179  0.001032\n",
            "Iteration 00460    Loss -0.041178  0.000914\n",
            "Iteration 00461    Loss -0.041250  0.001044\n",
            "Iteration 00462    Loss -0.041235  0.001015\n",
            "Iteration 00463    Loss -0.041012  0.000869\n",
            "Iteration 00464    Loss -0.041235  0.001036\n",
            "Iteration 00465    Loss -0.041291  0.000961\n",
            "Iteration 00466    Loss -0.041354  0.000952\n",
            "Iteration 00467    Loss -0.041355  0.000982\n",
            "Iteration 00468    Loss -0.041414  0.000997\n",
            "Iteration 00469    Loss -0.041491  0.000941\n",
            "Iteration 00470    Loss -0.041537  0.000978\n",
            "Iteration 00471    Loss -0.041548  0.001008\n",
            "Iteration 00472    Loss -0.041549  0.000956\n",
            "Iteration 00473    Loss -0.041540  0.000989\n",
            "Iteration 00474    Loss -0.041625  0.001017\n",
            "Iteration 00475    Loss -0.041584  0.000975\n",
            "Iteration 00476    Loss -0.041686  0.001063\n",
            "Iteration 00477    Loss -0.041673  0.000896\n",
            "Iteration 00478    Loss -0.041627  0.001074\n",
            "Iteration 00479    Loss -0.041722  0.000921\n",
            "Iteration 00480    Loss -0.041746  0.000934\n",
            "Iteration 00481    Loss -0.041855  0.001053\n",
            "Iteration 00482    Loss -0.041835  0.000933\n",
            "Iteration 00483    Loss -0.041888  0.000930\n",
            "Iteration 00484    Loss -0.041926  0.000996\n",
            "Iteration 00485    Loss -0.041984  0.000995\n",
            "Iteration 00486    Loss -0.041863  0.001060\n",
            "Iteration 00487    Loss -0.041993  0.001024\n",
            "Iteration 00488    Loss -0.041989  0.000977\n",
            "Iteration 00489    Loss -0.041752  0.000988\n",
            "Iteration 00490    Loss -0.042036  0.001023\n",
            "Iteration 00491    Loss -0.042096  0.001005\n",
            "Iteration 00492    Loss -0.042120  0.000957\n",
            "Iteration 00493    Loss -0.042068  0.001017\n",
            "Iteration 00494    Loss -0.042081  0.000986\n",
            "Iteration 00495    Loss -0.042084  0.001002\n",
            "Iteration 00496    Loss -0.042192  0.001055\n",
            "Iteration 00497    Loss -0.041969  0.000972\n",
            "Iteration 00498    Loss -0.042205  0.000979\n",
            "Iteration 00499    Loss -0.042305  0.001047\n",
            " 93% 62/67 [34:06<02:44, 32.90s/it]/content/ZID/REAL/30.png\n",
            "Iteration 00000    Loss 0.162796  0.000011\n",
            "Iteration 00001    Loss 20.341763  0.000005\n",
            "Iteration 00002    Loss 0.213507  0.000002\n",
            "Iteration 00003    Loss 3.633387  0.000002\n",
            "Iteration 00004    Loss 1.678036  0.000001\n",
            "Iteration 00005    Loss 0.548434  0.000001\n",
            "Iteration 00006    Loss 0.807342  0.000001\n",
            "Iteration 00007    Loss 0.062970  0.000000\n",
            "Iteration 00008    Loss 0.060818  0.000000\n",
            "Iteration 00009    Loss 0.059256  0.000000\n",
            "Iteration 00010    Loss 0.057801  0.000000\n",
            "Iteration 00011    Loss 0.055822  0.000000\n",
            "Iteration 00012    Loss 0.054044  0.000000\n",
            "Iteration 00013    Loss 0.051082  0.000000\n",
            "Iteration 00014    Loss 0.062236  0.000000\n",
            "Iteration 00015    Loss 0.047229  0.000000\n",
            "Iteration 00016    Loss 0.045538  0.000000\n",
            "Iteration 00017    Loss 0.044034  0.000000\n",
            "Iteration 00018    Loss 0.042706  0.000000\n",
            "Iteration 00019    Loss 0.041340  0.000000\n",
            "Iteration 00020    Loss 0.039776  0.000000\n",
            "Iteration 00021    Loss 0.038588  0.000000\n",
            "Iteration 00022    Loss 0.037618  0.000000\n",
            "Iteration 00023    Loss 0.036424  0.000000\n",
            "Iteration 00024    Loss 0.035196  0.000000\n",
            "Iteration 00025    Loss 0.033825  0.000000\n",
            "Iteration 00026    Loss 0.032906  0.000000\n",
            "Iteration 00027    Loss 0.031695  0.000000\n",
            "Iteration 00028    Loss 0.030652  0.000000\n",
            "Iteration 00029    Loss 0.029730  0.000000\n",
            "Iteration 00030    Loss 0.028564  0.000000\n",
            "Iteration 00031    Loss 0.027742  0.000000\n",
            "Iteration 00032    Loss 0.026838  0.000000\n",
            "Iteration 00033    Loss 0.025933  0.000000\n",
            "Iteration 00034    Loss 0.025080  0.000000\n",
            "Iteration 00035    Loss 0.024176  0.000000\n",
            "Iteration 00036    Loss 0.023298  0.000000\n",
            "Iteration 00037    Loss 0.022511  0.000000\n",
            "Iteration 00038    Loss 0.021803  0.000000\n",
            "Iteration 00039    Loss 0.021064  0.000000\n",
            "Iteration 00040    Loss 0.020228  0.000000\n",
            "Iteration 00041    Loss 0.019609  0.000000\n",
            "Iteration 00042    Loss 0.018858  0.000000\n",
            "Iteration 00043    Loss 0.018118  0.000000\n",
            "Iteration 00044    Loss 0.017613  0.000000\n",
            "Iteration 00045    Loss 0.016822  0.000000\n",
            "Iteration 00046    Loss 0.016137  0.000000\n",
            "Iteration 00047    Loss 0.015488  0.000000\n",
            "Iteration 00048    Loss 0.014695  0.000000\n",
            "Iteration 00049    Loss 0.014006  0.000000\n",
            "Iteration 00050    Loss 0.013361  0.000000\n",
            "Iteration 00051    Loss 0.012668  0.000000\n",
            "Iteration 00052    Loss 0.012078  0.000000\n",
            "Iteration 00053    Loss 0.011423  0.000000\n",
            "Iteration 00054    Loss 0.010805  0.000000\n",
            "Iteration 00055    Loss 0.010177  0.000000\n",
            "Iteration 00056    Loss 0.009547  0.000000\n",
            "Iteration 00057    Loss 0.008934  0.000000\n",
            "Iteration 00058    Loss 0.008413  0.000000\n",
            "Iteration 00059    Loss 0.007771  0.000000\n",
            "Iteration 00060    Loss 0.007300  0.000000\n",
            "Iteration 00061    Loss 0.006625  0.000000\n",
            "Iteration 00062    Loss 0.006072  0.000000\n",
            "Iteration 00063    Loss 0.005531  0.000000\n",
            "Iteration 00064    Loss 0.005070  0.000000\n",
            "Iteration 00065    Loss 0.004453  0.000000\n",
            "Iteration 00066    Loss 0.003934  0.000000\n",
            "Iteration 00067    Loss 0.003405  0.000000\n",
            "Iteration 00068    Loss 0.002907  0.000000\n",
            "Iteration 00069    Loss 0.002358  0.000000\n",
            "Iteration 00070    Loss 0.001958  0.000000\n",
            "Iteration 00071    Loss 0.001443  0.000000\n",
            "Iteration 00072    Loss 0.000991  0.000000\n",
            "Iteration 00073    Loss 0.000485  0.000000\n",
            "Iteration 00074    Loss 0.000041  0.000000\n",
            "Iteration 00075    Loss -0.000485  0.000000\n",
            "Iteration 00076    Loss -0.001000  0.000000\n",
            "Iteration 00077    Loss -0.001539  0.000000\n",
            "Iteration 00078    Loss -0.001905  0.000000\n",
            "Iteration 00079    Loss -0.002392  0.000000\n",
            "Iteration 00080    Loss -0.002737  0.000000\n",
            "Iteration 00081    Loss -0.003295  0.000000\n",
            "Iteration 00082    Loss -0.003798  0.000000\n",
            "Iteration 00083    Loss -0.004221  0.000000\n",
            "Iteration 00084    Loss -0.004623  0.000000\n",
            "Iteration 00085    Loss -0.005107  0.000000\n",
            "Iteration 00086    Loss -0.005519  0.000000\n",
            "Iteration 00087    Loss -0.005968  0.000000\n",
            "Iteration 00088    Loss -0.006268  0.000000\n",
            "Iteration 00089    Loss -0.006770  0.000000\n",
            "Iteration 00090    Loss -0.007216  0.000000\n",
            "Iteration 00091    Loss -0.007506  0.000001\n",
            "Iteration 00092    Loss -0.007921  0.000001\n",
            "Iteration 00093    Loss -0.008473  0.000001\n",
            "Iteration 00094    Loss -0.008927  0.000001\n",
            "Iteration 00095    Loss -0.009080  0.000001\n",
            "Iteration 00096    Loss -0.009706  0.000001\n",
            "Iteration 00097    Loss -0.010040  0.000001\n",
            "Iteration 00098    Loss -0.010459  0.000001\n",
            "Iteration 00099    Loss -0.010883  0.000001\n",
            "Iteration 00100    Loss -0.011251  0.000001\n",
            "Iteration 00101    Loss -0.011602  0.000001\n",
            "Iteration 00102    Loss -0.011997  0.000001\n",
            "Iteration 00103    Loss -0.012348  0.000001\n",
            "Iteration 00104    Loss -0.012728  0.000001\n",
            "Iteration 00105    Loss -0.012882  0.000001\n",
            "Iteration 00106    Loss -0.013474  0.000001\n",
            "Iteration 00107    Loss -0.013867  0.000001\n",
            "Iteration 00108    Loss -0.014116  0.000001\n",
            "Iteration 00109    Loss -0.014560  0.000001\n",
            "Iteration 00110    Loss -0.014927  0.000001\n",
            "Iteration 00111    Loss -0.015228  0.000001\n",
            "Iteration 00112    Loss -0.015531  0.000001\n",
            "Iteration 00113    Loss -0.015921  0.000001\n",
            "Iteration 00114    Loss -0.016254  0.000001\n",
            "Iteration 00115    Loss -0.016634  0.000001\n",
            "Iteration 00116    Loss -0.016940  0.000001\n",
            "Iteration 00117    Loss -0.017235  0.000001\n",
            "Iteration 00118    Loss -0.017537  0.000001\n",
            "Iteration 00119    Loss -0.017772  0.000001\n",
            "Iteration 00120    Loss -0.018222  0.000001\n",
            "Iteration 00121    Loss -0.018446  0.000001\n",
            "Iteration 00122    Loss -0.018686  0.000001\n",
            "Iteration 00123    Loss -0.018945  0.000001\n",
            "Iteration 00124    Loss -0.019380  0.000001\n",
            "Iteration 00125    Loss -0.019619  0.000001\n",
            "Iteration 00126    Loss -0.019873  0.000001\n",
            "Iteration 00127    Loss -0.020079  0.000001\n",
            "Iteration 00128    Loss -0.020455  0.000001\n",
            "Iteration 00129    Loss -0.020765  0.000001\n",
            "Iteration 00130    Loss -0.020837  0.000001\n",
            "Iteration 00131    Loss -0.021221  0.000001\n",
            "Iteration 00132    Loss -0.021377  0.000001\n",
            "Iteration 00133    Loss -0.021657  0.000001\n",
            "Iteration 00134    Loss -0.021932  0.000001\n",
            "Iteration 00135    Loss -0.022214  0.000001\n",
            "Iteration 00136    Loss -0.022507  0.000001\n",
            "Iteration 00137    Loss -0.022626  0.000001\n",
            "Iteration 00138    Loss -0.022829  0.000001\n",
            "Iteration 00139    Loss -0.022982  0.000001\n",
            "Iteration 00140    Loss -0.023448  0.000001\n",
            "Iteration 00141    Loss -0.023525  0.000001\n",
            "Iteration 00142    Loss -0.023759  0.000001\n",
            "Iteration 00143    Loss -0.023964  0.000001\n",
            "Iteration 00144    Loss -0.024096  0.000001\n",
            "Iteration 00145    Loss -0.024345  0.000001\n",
            "Iteration 00146    Loss -0.024554  0.000001\n",
            "Iteration 00147    Loss -0.024649  0.000001\n",
            "Iteration 00148    Loss -0.024837  0.000001\n",
            "Iteration 00149    Loss -0.025121  0.000001\n",
            "Iteration 00150    Loss -0.025308  0.000001\n",
            "Iteration 00151    Loss -0.025601  0.000001\n",
            "Iteration 00152    Loss -0.025487  0.000001\n",
            "Iteration 00153    Loss -0.025765  0.000001\n",
            "Iteration 00154    Loss -0.026111  0.000001\n",
            "Iteration 00155    Loss -0.026223  0.000001\n",
            "Iteration 00156    Loss -0.026386  0.000001\n",
            "Iteration 00157    Loss -0.026621  0.000001\n",
            "Iteration 00158    Loss -0.026812  0.000001\n",
            "Iteration 00159    Loss -0.026917  0.000001\n",
            "Iteration 00160    Loss -0.027153  0.000001\n",
            "Iteration 00161    Loss -0.027284  0.000001\n",
            "Iteration 00162    Loss -0.027553  0.000001\n",
            "Iteration 00163    Loss -0.027697  0.000001\n",
            "Iteration 00164    Loss -0.027861  0.000001\n",
            "Iteration 00165    Loss -0.027948  0.000001\n",
            "Iteration 00166    Loss -0.028106  0.000001\n",
            "Iteration 00167    Loss -0.028292  0.000001\n",
            "Iteration 00168    Loss -0.028466  0.000001\n",
            "Iteration 00169    Loss -0.028540  0.000001\n",
            "Iteration 00170    Loss -0.028787  0.000001\n",
            "Iteration 00171    Loss -0.028729  0.000001\n",
            "Iteration 00172    Loss -0.028981  0.000001\n",
            "Iteration 00173    Loss -0.028954  0.000001\n",
            "Iteration 00174    Loss -0.029230  0.000001\n",
            "Iteration 00175    Loss -0.029519  0.000001\n",
            "Iteration 00176    Loss -0.029596  0.000001\n",
            "Iteration 00177    Loss -0.029781  0.000001\n",
            "Iteration 00178    Loss -0.029847  0.000001\n",
            "Iteration 00179    Loss -0.030017  0.000001\n",
            "Iteration 00180    Loss -0.030202  0.000001\n",
            "Iteration 00181    Loss -0.030268  0.000001\n",
            "Iteration 00182    Loss -0.030453  0.000001\n",
            "Iteration 00183    Loss -0.030580  0.000001\n",
            "Iteration 00184    Loss -0.030689  0.000001\n",
            "Iteration 00185    Loss -0.030717  0.000001\n",
            "Iteration 00186    Loss -0.030589  0.000001\n",
            "Iteration 00187    Loss -0.031211  0.000001\n",
            "Iteration 00188    Loss -0.031148  0.000001\n",
            "Iteration 00189    Loss -0.031330  0.000001\n",
            "Iteration 00190    Loss -0.031476  0.000001\n",
            "Iteration 00191    Loss -0.031542  0.000001\n",
            "Iteration 00192    Loss -0.031837  0.000001\n",
            "Iteration 00193    Loss -0.031694  0.000001\n",
            "Iteration 00194    Loss -0.031883  0.000001\n",
            "Iteration 00195    Loss -0.032034  0.000001\n",
            "Iteration 00196    Loss -0.032253  0.000001\n",
            "Iteration 00197    Loss -0.032234  0.000001\n",
            "Iteration 00198    Loss -0.032375  0.000001\n",
            "Iteration 00199    Loss -0.032548  0.000001\n",
            "Iteration 00200    Loss -0.032734  0.000001\n",
            "Iteration 00201    Loss -0.032757  0.000001\n",
            "Iteration 00202    Loss -0.032877  0.000001\n",
            "Iteration 00203    Loss -0.032894  0.000001\n",
            "Iteration 00204    Loss -0.033115  0.000001\n",
            "Iteration 00205    Loss -0.033143  0.000001\n",
            "Iteration 00206    Loss -0.033226  0.000001\n",
            "Iteration 00207    Loss -0.033268  0.000001\n",
            "Iteration 00208    Loss -0.033502  0.000001\n",
            "Iteration 00209    Loss -0.033668  0.000002\n",
            "Iteration 00210    Loss -0.033594  0.000001\n",
            "Iteration 00211    Loss -0.033722  0.000001\n",
            "Iteration 00212    Loss -0.033944  0.000002\n",
            "Iteration 00213    Loss -0.033987  0.000002\n",
            "Iteration 00214    Loss -0.034205  0.000001\n",
            "Iteration 00215    Loss -0.034277  0.000001\n",
            "Iteration 00216    Loss -0.034424  0.000001\n",
            "Iteration 00217    Loss -0.034402  0.000001\n",
            "Iteration 00218    Loss -0.034602  0.000001\n",
            "Iteration 00219    Loss -0.034664  0.000001\n",
            "Iteration 00220    Loss -0.034736  0.000001\n",
            "Iteration 00221    Loss -0.034572  0.000002\n",
            "Iteration 00222    Loss -0.034983  0.000002\n",
            "Iteration 00223    Loss -0.034771  0.000002\n",
            "Iteration 00224    Loss -0.035200  0.000002\n",
            "Iteration 00225    Loss -0.035121  0.000003\n",
            "Iteration 00226    Loss -0.035141  0.000003\n",
            "Iteration 00227    Loss -0.035533  0.000002\n",
            "Iteration 00228    Loss -0.035290  0.000001\n",
            "Iteration 00229    Loss -0.035615  0.000001\n",
            "Iteration 00230    Loss -0.035551  0.000002\n",
            "Iteration 00231    Loss -0.035858  0.000002\n",
            "Iteration 00232    Loss -0.035678  0.000002\n",
            "Iteration 00233    Loss -0.035882  0.000002\n",
            "Iteration 00234    Loss -0.036048  0.000002\n",
            "Iteration 00235    Loss -0.036146  0.000002\n",
            "Iteration 00236    Loss -0.036097  0.000003\n",
            "Iteration 00237    Loss -0.036138  0.000003\n",
            "Iteration 00238    Loss -0.036222  0.000003\n",
            "Iteration 00239    Loss -0.036276  0.000002\n",
            "Iteration 00240    Loss -0.036479  0.000002\n",
            "Iteration 00241    Loss -0.036554  0.000002\n",
            "Iteration 00242    Loss -0.036476  0.000002\n",
            "Iteration 00243    Loss -0.036451  0.000004\n",
            "Iteration 00244    Loss -0.036667  0.000004\n",
            "Iteration 00245    Loss -0.037012  0.000002\n",
            "Iteration 00246    Loss -0.036980  0.000002\n",
            "Iteration 00247    Loss -0.036962  0.000002\n",
            "Iteration 00248    Loss -0.036615  0.000002\n",
            "Iteration 00249    Loss -0.037286  0.000001\n",
            "Iteration 00250    Loss -0.037320  0.000002\n",
            "Iteration 00251    Loss -0.037379  0.000002\n",
            "Iteration 00252    Loss -0.037415  0.000002\n",
            "Iteration 00253    Loss -0.037415  0.000002\n",
            "Iteration 00254    Loss -0.037524  0.000003\n",
            "Iteration 00255    Loss -0.037418  0.000002\n",
            "Iteration 00256    Loss -0.037606  0.000002\n",
            "Iteration 00257    Loss -0.037375  0.000004\n",
            "Iteration 00258    Loss -0.037570  0.000003\n",
            "Iteration 00259    Loss -0.037968  0.000002\n",
            "Iteration 00260    Loss -0.037735  0.000001\n",
            "Iteration 00261    Loss -0.037938  0.000001\n",
            "Iteration 00262    Loss -0.038011  0.000001\n",
            "Iteration 00263    Loss -0.038018  0.000001\n",
            "Iteration 00264    Loss -0.037767  0.000001\n",
            "Iteration 00265    Loss -0.038181  0.000001\n",
            "Iteration 00266    Loss -0.038302  0.000002\n",
            "Iteration 00267    Loss -0.038214  0.000002\n",
            "Iteration 00268    Loss -0.038287  0.000002\n",
            "Iteration 00269    Loss -0.038482  0.000002\n",
            "Iteration 00270    Loss -0.038263  0.000002\n",
            "Iteration 00271    Loss -0.038273  0.000002\n",
            "Iteration 00272    Loss -0.038172  0.000001\n",
            "Iteration 00273    Loss -0.038730  0.000002\n",
            "Iteration 00274    Loss -0.038624  0.000002\n",
            "Iteration 00275    Loss -0.038885  0.000002\n",
            "Iteration 00276    Loss -0.038889  0.000002\n",
            "Iteration 00277    Loss -0.039017  0.000002\n",
            "Iteration 00278    Loss -0.038773  0.000002\n",
            "Iteration 00279    Loss -0.038671  0.000003\n",
            "Iteration 00280    Loss -0.039085  0.000004\n",
            "Iteration 00281    Loss -0.039256  0.000003\n",
            "Iteration 00282    Loss -0.039043  0.000002\n",
            "Iteration 00283    Loss -0.039189  0.000002\n",
            "Iteration 00284    Loss -0.039401  0.000001\n",
            "Iteration 00285    Loss -0.039460  0.000001\n",
            "Iteration 00286    Loss -0.039458  0.000002\n",
            "Iteration 00287    Loss -0.039112  0.000002\n",
            "Iteration 00288    Loss -0.039642  0.000002\n",
            "Iteration 00289    Loss -0.039663  0.000002\n",
            "Iteration 00290    Loss -0.039587  0.000002\n",
            "Iteration 00291    Loss -0.039686  0.000002\n",
            "Iteration 00292    Loss -0.039750  0.000003\n",
            "Iteration 00293    Loss -0.039914  0.000003\n",
            "Iteration 00294    Loss -0.039882  0.000002\n",
            "Iteration 00295    Loss -0.039710  0.000002\n",
            "Iteration 00296    Loss -0.039800  0.000002\n",
            "Iteration 00297    Loss -0.039745  0.000002\n",
            "Iteration 00298    Loss -0.039783  0.000004\n",
            "Iteration 00299    Loss -0.040182  0.000003\n",
            "Iteration 00300    Loss -0.040015  0.000002\n",
            "Iteration 00301    Loss -0.040101  0.000002\n",
            "Iteration 00302    Loss -0.040114  0.000002\n",
            "Iteration 00303    Loss -0.040272  0.000002\n",
            "Iteration 00304    Loss -0.040106  0.000002\n",
            "Iteration 00305    Loss -0.040478  0.000003\n",
            "Iteration 00306    Loss -0.040308  0.000003\n",
            "Iteration 00307    Loss -0.040406  0.000002\n",
            "Iteration 00308    Loss -0.040608  0.000002\n",
            "Iteration 00309    Loss -0.040583  0.000002\n",
            "Iteration 00310    Loss -0.040521  0.000003\n",
            "Iteration 00311    Loss -0.040726  0.000003\n",
            "Iteration 00312    Loss -0.040669  0.000003\n",
            "Iteration 00313    Loss -0.040651  0.000002\n",
            "Iteration 00314    Loss -0.040625  0.000003\n",
            "Iteration 00315    Loss -0.040431  0.000005\n",
            "Iteration 00316    Loss -0.040991  0.000004\n",
            "Iteration 00317    Loss -0.040889  0.000003\n",
            "Iteration 00318    Loss -0.040750  0.000003\n",
            "Iteration 00319    Loss -0.041165  0.000005\n",
            "Iteration 00320    Loss -0.040900  0.000003\n",
            "Iteration 00321    Loss -0.041180  0.000002\n",
            "Iteration 00322    Loss -0.041171  0.000002\n",
            "Iteration 00323    Loss -0.041102  0.000002\n",
            "Iteration 00324    Loss -0.041101  0.000002\n",
            "Iteration 00325    Loss -0.041103  0.000002\n",
            "Iteration 00326    Loss -0.041373  0.000003\n",
            "Iteration 00327    Loss -0.041237  0.000002\n",
            "Iteration 00328    Loss -0.041320  0.000001\n",
            "Iteration 00329    Loss -0.041415  0.000001\n",
            "Iteration 00330    Loss -0.041485  0.000002\n",
            "Iteration 00331    Loss -0.041362  0.000002\n",
            "Iteration 00332    Loss -0.041586  0.000003\n",
            "Iteration 00333    Loss -0.041620  0.000003\n",
            "Iteration 00334    Loss -0.041608  0.000002\n",
            "Iteration 00335    Loss -0.041608  0.000002\n",
            "Iteration 00336    Loss -0.041599  0.000002\n",
            "Iteration 00337    Loss -0.041510  0.000001\n",
            "Iteration 00338    Loss -0.041746  0.000002\n",
            "Iteration 00339    Loss -0.041812  0.000002\n",
            "Iteration 00340    Loss -0.041835  0.000002\n",
            "Iteration 00341    Loss -0.041612  0.000002\n",
            "Iteration 00342    Loss -0.041923  0.000002\n",
            "Iteration 00343    Loss -0.041709  0.000002\n",
            "Iteration 00344    Loss -0.041979  0.000002\n",
            "Iteration 00345    Loss -0.042013  0.000002\n",
            "Iteration 00346    Loss -0.041992  0.000002\n",
            "Iteration 00347    Loss -0.042111  0.000003\n",
            "Iteration 00348    Loss -0.042212  0.000003\n",
            "Iteration 00349    Loss -0.042151  0.000003\n",
            "Iteration 00350    Loss -0.042202  0.000003\n",
            "Iteration 00351    Loss -0.042262  0.000003\n",
            "Iteration 00352    Loss -0.042242  0.000003\n",
            "Iteration 00353    Loss -0.042333  0.000003\n",
            "Iteration 00354    Loss -0.042261  0.000003\n",
            "Iteration 00355    Loss -0.042414  0.000003\n",
            "Iteration 00356    Loss -0.042441  0.000003\n",
            "Iteration 00357    Loss -0.042485  0.000003\n",
            "Iteration 00358    Loss -0.042431  0.000002\n",
            "Iteration 00359    Loss -0.042524  0.000002\n",
            "Iteration 00360    Loss -0.042288  0.000002\n",
            "Iteration 00361    Loss -0.042600  0.000002\n",
            "Iteration 00362    Loss -0.042639  0.000003\n",
            "Iteration 00363    Loss -0.042598  0.000003\n",
            "Iteration 00364    Loss -0.042648  0.000002\n",
            "Iteration 00365    Loss -0.042693  0.000002\n",
            "Iteration 00366    Loss -0.042629  0.000003\n",
            "Iteration 00367    Loss -0.042667  0.000004\n",
            "Iteration 00368    Loss -0.042758  0.000005\n",
            "Iteration 00369    Loss -0.042834  0.000003\n",
            "Iteration 00370    Loss -0.042679  0.000002\n",
            "Iteration 00371    Loss -0.042874  0.000001\n",
            "Iteration 00372    Loss -0.042905  0.000001\n",
            "Iteration 00373    Loss -0.042911  0.000002\n",
            "Iteration 00374    Loss -0.042989  0.000002\n",
            "Iteration 00375    Loss -0.043023  0.000003\n",
            "Iteration 00376    Loss -0.042749  0.000002\n",
            "Iteration 00377    Loss -0.042958  0.000002\n",
            "Iteration 00378    Loss -0.043107  0.000002\n",
            "Iteration 00379    Loss -0.043044  0.000002\n",
            "Iteration 00380    Loss -0.043033  0.000003\n",
            "Iteration 00381    Loss -0.043114  0.000003\n",
            "Iteration 00382    Loss -0.043019  0.000003\n",
            "Iteration 00383    Loss -0.043057  0.000002\n",
            "Iteration 00384    Loss -0.043205  0.000002\n",
            "Iteration 00385    Loss -0.043282  0.000002\n",
            "Iteration 00386    Loss -0.043156  0.000003\n",
            "Iteration 00387    Loss -0.042533  0.000003\n",
            "Iteration 00388    Loss -0.043160  0.000003\n",
            "Iteration 00389    Loss -0.043358  0.000003\n",
            "Iteration 00390    Loss -0.043097  0.000002\n",
            "Iteration 00391    Loss -0.043426  0.000002\n",
            "Iteration 00392    Loss -0.043412  0.000002\n",
            "Iteration 00393    Loss -0.043369  0.000002\n",
            "Iteration 00394    Loss -0.043477  0.000003\n",
            "Iteration 00395    Loss -0.043500  0.000004\n",
            "Iteration 00396    Loss -0.043344  0.000003\n",
            "Iteration 00397    Loss -0.043534  0.000003\n",
            "Iteration 00398    Loss -0.043503  0.000003\n",
            "Iteration 00399    Loss -0.043524  0.000002\n",
            "Iteration 00400    Loss -0.043642  0.000002\n",
            "Iteration 00401    Loss -0.043628  0.000002\n",
            "Iteration 00402    Loss -0.043687  0.000003\n",
            "Iteration 00403    Loss -0.043718  0.000004\n",
            "Iteration 00404    Loss -0.043429  0.000004\n",
            "Iteration 00405    Loss -0.043539  0.000003\n",
            "Iteration 00406    Loss -0.043718  0.000002\n",
            "Iteration 00407    Loss -0.043474  0.000001\n",
            "Iteration 00408    Loss -0.043620  0.000002\n",
            "Iteration 00409    Loss -0.043818  0.000002\n",
            "Iteration 00410    Loss -0.043872  0.000002\n",
            "Iteration 00411    Loss -0.043745  0.000002\n",
            "Iteration 00412    Loss -0.043920  0.000002\n",
            "Iteration 00413    Loss -0.043924  0.000002\n",
            "Iteration 00414    Loss -0.043945  0.000002\n",
            "Iteration 00415    Loss -0.043788  0.000003\n",
            "Iteration 00416    Loss -0.043767  0.000003\n",
            "Iteration 00417    Loss -0.043939  0.000003\n",
            "Iteration 00418    Loss -0.044049  0.000003\n",
            "Iteration 00419    Loss -0.044056  0.000004\n",
            "Iteration 00420    Loss -0.044109  0.000005\n",
            "Iteration 00421    Loss -0.043987  0.000005\n",
            "Iteration 00422    Loss -0.043984  0.000003\n",
            "Iteration 00423    Loss -0.044149  0.000002\n",
            "Iteration 00424    Loss -0.044200  0.000002\n",
            "Iteration 00425    Loss -0.044088  0.000001\n",
            "Iteration 00426    Loss -0.044002  0.000001\n",
            "Iteration 00427    Loss -0.044024  0.000001\n",
            "Iteration 00428    Loss -0.044208  0.000001\n",
            "Iteration 00429    Loss -0.044181  0.000002\n",
            "Iteration 00430    Loss -0.044099  0.000004\n",
            "Iteration 00431    Loss -0.044273  0.000004\n",
            "Iteration 00432    Loss -0.044204  0.000003\n",
            "Iteration 00433    Loss -0.044262  0.000003\n",
            "Iteration 00434    Loss -0.044262  0.000004\n",
            "Iteration 00435    Loss -0.044343  0.000005\n",
            "Iteration 00436    Loss -0.044419  0.000004\n",
            "Iteration 00437    Loss -0.044386  0.000003\n",
            "Iteration 00438    Loss -0.044464  0.000003\n",
            "Iteration 00439    Loss -0.044423  0.000004\n",
            "Iteration 00440    Loss -0.044282  0.000004\n",
            "Iteration 00441    Loss -0.044455  0.000003\n",
            "Iteration 00442    Loss -0.044531  0.000002\n",
            "Iteration 00443    Loss -0.044504  0.000002\n",
            "Iteration 00444    Loss -0.044576  0.000002\n",
            "Iteration 00445    Loss -0.044403  0.000002\n",
            "Iteration 00446    Loss -0.044633  0.000002\n",
            "Iteration 00447    Loss -0.044647  0.000002\n",
            "Iteration 00448    Loss -0.044676  0.000002\n",
            "Iteration 00449    Loss -0.044656  0.000003\n",
            "Iteration 00450    Loss -0.044698  0.000003\n",
            "Iteration 00451    Loss -0.044748  0.000004\n",
            "Iteration 00452    Loss -0.044762  0.000003\n",
            "Iteration 00453    Loss -0.044791  0.000002\n",
            "Iteration 00454    Loss -0.044663  0.000001\n",
            "Iteration 00455    Loss -0.044743  0.000001\n",
            "Iteration 00456    Loss -0.044811  0.000001\n",
            "Iteration 00457    Loss -0.044844  0.000001\n",
            "Iteration 00458    Loss -0.044863  0.000002\n",
            "Iteration 00459    Loss -0.044804  0.000002\n",
            "Iteration 00460    Loss -0.044751  0.000003\n",
            "Iteration 00461    Loss -0.044911  0.000003\n",
            "Iteration 00462    Loss -0.044884  0.000003\n",
            "Iteration 00463    Loss -0.044737  0.000003\n",
            "Iteration 00464    Loss -0.044713  0.000003\n",
            "Iteration 00465    Loss -0.044942  0.000003\n",
            "Iteration 00466    Loss -0.044936  0.000002\n",
            "Iteration 00467    Loss -0.044875  0.000002\n",
            "Iteration 00468    Loss -0.044745  0.000002\n",
            "Iteration 00469    Loss -0.044996  0.000002\n",
            "Iteration 00470    Loss -0.044867  0.000002\n",
            "Iteration 00471    Loss -0.044975  0.000003\n",
            "Iteration 00472    Loss -0.044896  0.000003\n",
            "Iteration 00473    Loss -0.045081  0.000003\n",
            "Iteration 00474    Loss -0.045115  0.000002\n",
            "Iteration 00475    Loss -0.045125  0.000001\n",
            "Iteration 00476    Loss -0.044903  0.000001\n",
            "Iteration 00477    Loss -0.045010  0.000001\n",
            "Iteration 00478    Loss -0.045120  0.000002\n",
            "Iteration 00479    Loss -0.045171  0.000004\n",
            "Iteration 00480    Loss -0.045018  0.000006\n",
            "Iteration 00481    Loss -0.044901  0.000005\n",
            "Iteration 00482    Loss -0.045210  0.000002\n",
            "Iteration 00483    Loss -0.045265  0.000002\n",
            "Iteration 00484    Loss -0.045180  0.000002\n",
            "Iteration 00485    Loss -0.045259  0.000002\n",
            "Iteration 00486    Loss -0.045232  0.000002\n",
            "Iteration 00487    Loss -0.045324  0.000002\n",
            "Iteration 00488    Loss -0.045333  0.000003\n",
            "Iteration 00489    Loss -0.045242  0.000003\n",
            "Iteration 00490    Loss -0.045140  0.000003\n",
            "Iteration 00491    Loss -0.045407  0.000003\n",
            "Iteration 00492    Loss -0.045346  0.000002\n",
            "Iteration 00493    Loss -0.045415  0.000002\n",
            "Iteration 00494    Loss -0.045394  0.000002\n",
            "Iteration 00495    Loss -0.045446  0.000002\n",
            "Iteration 00496    Loss -0.045378  0.000002\n",
            "Iteration 00497    Loss -0.045493  0.000003\n",
            "Iteration 00498    Loss -0.045473  0.000003\n",
            "Iteration 00499    Loss -0.045469  0.000002\n",
            " 94% 63/67 [34:40<02:12, 33.17s/it]/content/ZID/REAL/florence_input.png\n",
            "Iteration 00000    Loss 0.330543  0.000031\n",
            "Iteration 00001    Loss 12.270445  0.000011\n",
            "Iteration 00002    Loss 0.230836  0.000005\n",
            "Iteration 00003    Loss 0.416092  0.000003\n",
            "Iteration 00004    Loss 0.234027  0.000003\n",
            "Iteration 00005    Loss 0.192682  0.000001\n",
            "Iteration 00006    Loss 0.144417  0.000001\n",
            "Iteration 00007    Loss 0.134948  0.000001\n",
            "Iteration 00008    Loss 0.130649  0.000001\n",
            "Iteration 00009    Loss 0.124951  0.000001\n",
            "Iteration 00010    Loss 0.119422  0.000001\n",
            "Iteration 00011    Loss 0.117601  0.000001\n",
            "Iteration 00012    Loss 0.115075  0.000001\n",
            "Iteration 00013    Loss 0.115046  0.000001\n",
            "Iteration 00014    Loss 0.111287  0.000001\n",
            "Iteration 00015    Loss 0.111089  0.000001\n",
            "Iteration 00016    Loss 0.109766  0.000001\n",
            "Iteration 00017    Loss 0.107413  0.000001\n",
            "Iteration 00018    Loss 0.105425  0.000001\n",
            "Iteration 00019    Loss 0.103715  0.000001\n",
            "Iteration 00020    Loss 0.102155  0.000001\n",
            "Iteration 00021    Loss 0.099676  0.000001\n",
            "Iteration 00022    Loss 0.097503  0.000002\n",
            "Iteration 00023    Loss 0.096275  0.000002\n",
            "Iteration 00024    Loss 0.094094  0.000002\n",
            "Iteration 00025    Loss 0.091008  0.000002\n",
            "Iteration 00026    Loss 0.089194  0.000002\n",
            "Iteration 00027    Loss 0.087112  0.000002\n",
            "Iteration 00028    Loss 0.085885  0.000002\n",
            "Iteration 00029    Loss 0.083050  0.000002\n",
            "Iteration 00030    Loss 0.082802  0.000002\n",
            "Iteration 00031    Loss 0.080386  0.000002\n",
            "Iteration 00032    Loss 0.080242  0.000002\n",
            "Iteration 00033    Loss 0.078699  0.000002\n",
            "Iteration 00034    Loss 0.077853  0.000002\n",
            "Iteration 00035    Loss 0.076520  0.000002\n",
            "Iteration 00036    Loss 0.075212  0.000002\n",
            "Iteration 00037    Loss 0.075031  0.000002\n",
            "Iteration 00038    Loss 0.073376  0.000002\n",
            "Iteration 00039    Loss 0.072080  0.000002\n",
            "Iteration 00040    Loss 0.072018  0.000002\n",
            "Iteration 00041    Loss 0.070659  0.000002\n",
            "Iteration 00042    Loss 0.069825  0.000002\n",
            "Iteration 00043    Loss 0.068085  0.000002\n",
            "Iteration 00044    Loss 0.067671  0.000002\n",
            "Iteration 00045    Loss 0.066302  0.000002\n",
            "Iteration 00046    Loss 0.065304  0.000002\n",
            "Iteration 00047    Loss 0.064305  0.000003\n",
            "Iteration 00048    Loss 0.063546  0.000003\n",
            "Iteration 00049    Loss 0.062187  0.000003\n",
            "Iteration 00050    Loss 0.061628  0.000003\n",
            "Iteration 00051    Loss 0.060943  0.000003\n",
            "Iteration 00052    Loss 0.059886  0.000003\n",
            "Iteration 00053    Loss 0.059029  0.000003\n",
            "Iteration 00054    Loss 0.057960  0.000004\n",
            "Iteration 00055    Loss 0.057301  0.000004\n",
            "Iteration 00056    Loss 0.056753  0.000004\n",
            "Iteration 00057    Loss 0.055966  0.000004\n",
            "Iteration 00058    Loss 0.055210  0.000005\n",
            "Iteration 00059    Loss 0.054082  0.000006\n",
            "Iteration 00060    Loss 0.053522  0.000005\n",
            "Iteration 00061    Loss 0.052485  0.000005\n",
            "Iteration 00062    Loss 0.052309  0.000005\n",
            "Iteration 00063    Loss 0.051534  0.000006\n",
            "Iteration 00064    Loss 0.050737  0.000006\n",
            "Iteration 00065    Loss 0.049686  0.000006\n",
            "Iteration 00066    Loss 0.048846  0.000007\n",
            "Iteration 00067    Loss 0.047878  0.000007\n",
            "Iteration 00068    Loss 0.047457  0.000007\n",
            "Iteration 00069    Loss 0.046751  0.000007\n",
            "Iteration 00070    Loss 0.046172  0.000007\n",
            "Iteration 00071    Loss 0.045114  0.000007\n",
            "Iteration 00072    Loss 0.044260  0.000009\n",
            "Iteration 00073    Loss 0.043732  0.000010\n",
            "Iteration 00074    Loss 0.043270  0.000010\n",
            "Iteration 00075    Loss 0.043151  0.000010\n",
            "Iteration 00076    Loss 0.041929  0.000009\n",
            "Iteration 00077    Loss 0.041128  0.000009\n",
            "Iteration 00078    Loss 0.040482  0.000010\n",
            "Iteration 00079    Loss 0.039376  0.000011\n",
            "Iteration 00080    Loss 0.039254  0.000012\n",
            "Iteration 00081    Loss 0.038251  0.000014\n",
            "Iteration 00082    Loss 0.038077  0.000014\n",
            "Iteration 00083    Loss 0.037129  0.000014\n",
            "Iteration 00084    Loss 0.036693  0.000013\n",
            "Iteration 00085    Loss 0.035631  0.000014\n",
            "Iteration 00086    Loss 0.035299  0.000016\n",
            "Iteration 00087    Loss 0.034399  0.000015\n",
            "Iteration 00088    Loss 0.033898  0.000014\n",
            "Iteration 00089    Loss 0.033070  0.000017\n",
            "Iteration 00090    Loss 0.032699  0.000020\n",
            "Iteration 00091    Loss 0.031903  0.000016\n",
            "Iteration 00092    Loss 0.031998  0.000016\n",
            "Iteration 00093    Loss 0.030871  0.000019\n",
            "Iteration 00094    Loss 0.030522  0.000021\n",
            "Iteration 00095    Loss 0.030196  0.000020\n",
            "Iteration 00096    Loss 0.029765  0.000019\n",
            "Iteration 00097    Loss 0.028770  0.000020\n",
            "Iteration 00098    Loss 0.028014  0.000021\n",
            "Iteration 00099    Loss 0.029032  0.000021\n",
            "Iteration 00100    Loss 0.027301  0.000024\n",
            "Iteration 00101    Loss 0.026604  0.000024\n",
            "Iteration 00102    Loss 0.026125  0.000023\n",
            "Iteration 00103    Loss 0.025674  0.000020\n",
            "Iteration 00104    Loss 0.025065  0.000020\n",
            "Iteration 00105    Loss 0.024159  0.000022\n",
            "Iteration 00106    Loss 0.023972  0.000027\n",
            "Iteration 00107    Loss 0.023601  0.000027\n",
            "Iteration 00108    Loss 0.022660  0.000025\n",
            "Iteration 00109    Loss 0.022081  0.000025\n",
            "Iteration 00110    Loss 0.021559  0.000027\n",
            "Iteration 00111    Loss 0.021218  0.000029\n",
            "Iteration 00112    Loss 0.020833  0.000029\n",
            "Iteration 00113    Loss 0.020049  0.000028\n",
            "Iteration 00114    Loss 0.019669  0.000028\n",
            "Iteration 00115    Loss 0.019679  0.000028\n",
            "Iteration 00116    Loss 0.018591  0.000029\n",
            "Iteration 00117    Loss 0.018354  0.000031\n",
            "Iteration 00118    Loss 0.018132  0.000034\n",
            "Iteration 00119    Loss 0.017017  0.000036\n",
            "Iteration 00120    Loss 0.016872  0.000034\n",
            "Iteration 00121    Loss 0.016540  0.000032\n",
            "Iteration 00122    Loss 0.015706  0.000032\n",
            "Iteration 00123    Loss 0.015474  0.000034\n",
            "Iteration 00124    Loss 0.014781  0.000032\n",
            "Iteration 00125    Loss 0.014547  0.000035\n",
            "Iteration 00126    Loss 0.013705  0.000037\n",
            "Iteration 00127    Loss 0.013246  0.000038\n",
            "Iteration 00128    Loss 0.013125  0.000037\n",
            "Iteration 00129    Loss 0.012578  0.000037\n",
            "Iteration 00130    Loss 0.011980  0.000042\n",
            "Iteration 00131    Loss 0.011739  0.000041\n",
            "Iteration 00132    Loss 0.011193  0.000041\n",
            "Iteration 00133    Loss 0.010698  0.000040\n",
            "Iteration 00134    Loss 0.010019  0.000035\n",
            "Iteration 00135    Loss 0.009949  0.000037\n",
            "Iteration 00136    Loss 0.009687  0.000044\n",
            "Iteration 00137    Loss 0.009049  0.000047\n",
            "Iteration 00138    Loss 0.008857  0.000041\n",
            "Iteration 00139    Loss 0.007871  0.000046\n",
            "Iteration 00140    Loss 0.007732  0.000047\n",
            "Iteration 00141    Loss 0.007175  0.000046\n",
            "Iteration 00142    Loss 0.007185  0.000048\n",
            "Iteration 00143    Loss 0.006336  0.000050\n",
            "Iteration 00144    Loss 0.005918  0.000049\n",
            "Iteration 00145    Loss 0.006434  0.000046\n",
            "Iteration 00146    Loss 0.005549  0.000051\n",
            "Iteration 00147    Loss 0.004746  0.000048\n",
            "Iteration 00148    Loss 0.004643  0.000050\n",
            "Iteration 00149    Loss 0.004046  0.000058\n",
            "Iteration 00150    Loss 0.003733  0.000055\n",
            "Iteration 00151    Loss 0.003270  0.000052\n",
            "Iteration 00152    Loss 0.003572  0.000058\n",
            "Iteration 00153    Loss 0.002422  0.000059\n",
            "Iteration 00154    Loss 0.002007  0.000052\n",
            "Iteration 00155    Loss 0.001651  0.000055\n",
            "Iteration 00156    Loss 0.001336  0.000057\n",
            "Iteration 00157    Loss 0.000898  0.000059\n",
            "Iteration 00158    Loss 0.000796  0.000059\n",
            "Iteration 00159    Loss 0.000716  0.000062\n",
            "Iteration 00160    Loss 0.000217  0.000056\n",
            "Iteration 00161    Loss -0.000035  0.000056\n",
            "Iteration 00162    Loss -0.000674  0.000065\n",
            "Iteration 00163    Loss -0.000986  0.000068\n",
            "Iteration 00164    Loss -0.001134  0.000066\n",
            "Iteration 00165    Loss -0.001764  0.000063\n",
            "Iteration 00166    Loss -0.002111  0.000058\n",
            "Iteration 00167    Loss -0.002461  0.000059\n",
            "Iteration 00168    Loss -0.002653  0.000068\n",
            "Iteration 00169    Loss -0.003084  0.000077\n",
            "Iteration 00170    Loss -0.003531  0.000071\n",
            "Iteration 00171    Loss -0.003309  0.000066\n",
            "Iteration 00172    Loss -0.003998  0.000069\n",
            "Iteration 00173    Loss -0.004227  0.000074\n",
            "Iteration 00174    Loss -0.004823  0.000072\n",
            "Iteration 00175    Loss -0.004717  0.000072\n",
            "Iteration 00176    Loss -0.005055  0.000072\n",
            "Iteration 00177    Loss -0.005545  0.000071\n",
            "Iteration 00178    Loss -0.005825  0.000078\n",
            "Iteration 00179    Loss -0.005964  0.000079\n",
            "Iteration 00180    Loss -0.006458  0.000078\n",
            "Iteration 00181    Loss -0.006869  0.000082\n",
            "Iteration 00182    Loss -0.007016  0.000069\n",
            "Iteration 00183    Loss -0.007007  0.000075\n",
            "Iteration 00184    Loss -0.007746  0.000086\n",
            "Iteration 00185    Loss -0.007867  0.000084\n",
            "Iteration 00186    Loss -0.008310  0.000083\n",
            "Iteration 00187    Loss -0.008384  0.000078\n",
            "Iteration 00188    Loss -0.008516  0.000073\n",
            "Iteration 00189    Loss -0.008938  0.000084\n",
            "Iteration 00190    Loss -0.008892  0.000094\n",
            "Iteration 00191    Loss -0.009711  0.000085\n",
            "Iteration 00192    Loss -0.008965  0.000083\n",
            "Iteration 00193    Loss -0.009827  0.000091\n",
            "Iteration 00194    Loss -0.010232  0.000080\n",
            "Iteration 00195    Loss -0.010757  0.000086\n",
            "Iteration 00196    Loss -0.010774  0.000095\n",
            "Iteration 00197    Loss -0.011132  0.000084\n",
            "Iteration 00198    Loss -0.011332  0.000082\n",
            "Iteration 00199    Loss -0.011385  0.000089\n",
            "Iteration 00200    Loss -0.011933  0.000085\n",
            "Iteration 00201    Loss -0.012186  0.000093\n",
            "Iteration 00202    Loss -0.012627  0.000105\n",
            "Iteration 00203    Loss -0.012372  0.000084\n",
            "Iteration 00204    Loss -0.013096  0.000083\n",
            "Iteration 00205    Loss -0.013170  0.000095\n",
            "Iteration 00206    Loss -0.013574  0.000093\n",
            "Iteration 00207    Loss -0.013796  0.000095\n",
            "Iteration 00208    Loss -0.013725  0.000097\n",
            "Iteration 00209    Loss -0.014266  0.000093\n",
            "Iteration 00210    Loss -0.014480  0.000092\n",
            "Iteration 00211    Loss -0.014809  0.000097\n",
            "Iteration 00212    Loss -0.014969  0.000098\n",
            "Iteration 00213    Loss -0.014749  0.000098\n",
            "Iteration 00214    Loss -0.015528  0.000109\n",
            "Iteration 00215    Loss -0.015740  0.000107\n",
            "Iteration 00216    Loss -0.015760  0.000102\n",
            "Iteration 00217    Loss -0.016155  0.000103\n",
            "Iteration 00218    Loss -0.016523  0.000106\n",
            "Iteration 00219    Loss -0.016778  0.000102\n",
            "Iteration 00220    Loss -0.016484  0.000111\n",
            "Iteration 00221    Loss -0.017024  0.000114\n",
            "Iteration 00222    Loss -0.016842  0.000106\n",
            "Iteration 00223    Loss -0.017418  0.000114\n",
            "Iteration 00224    Loss -0.017594  0.000115\n",
            "Iteration 00225    Loss -0.018023  0.000110\n",
            "Iteration 00226    Loss -0.018174  0.000107\n",
            "Iteration 00227    Loss -0.018409  0.000116\n",
            "Iteration 00228    Loss -0.018124  0.000112\n",
            "Iteration 00229    Loss -0.018632  0.000117\n",
            "Iteration 00230    Loss -0.019112  0.000124\n",
            "Iteration 00231    Loss -0.019279  0.000116\n",
            "Iteration 00232    Loss -0.019428  0.000123\n",
            "Iteration 00233    Loss -0.019537  0.000118\n",
            "Iteration 00234    Loss -0.020167  0.000115\n",
            "Iteration 00235    Loss -0.020473  0.000122\n",
            "Iteration 00236    Loss -0.020220  0.000129\n",
            "Iteration 00237    Loss -0.020848  0.000122\n",
            "Iteration 00238    Loss -0.021038  0.000110\n",
            "Iteration 00239    Loss -0.020990  0.000126\n",
            "Iteration 00240    Loss -0.021355  0.000139\n",
            "Iteration 00241    Loss -0.021515  0.000121\n",
            "Iteration 00242    Loss -0.021509  0.000115\n",
            "Iteration 00243    Loss -0.021993  0.000123\n",
            "Iteration 00244    Loss -0.022136  0.000136\n",
            "Iteration 00245    Loss -0.022455  0.000134\n",
            "Iteration 00246    Loss -0.022793  0.000122\n",
            "Iteration 00247    Loss -0.022996  0.000127\n",
            "Iteration 00248    Loss -0.023005  0.000129\n",
            "Iteration 00249    Loss -0.023256  0.000133\n",
            "Iteration 00250    Loss -0.023550  0.000133\n",
            "Iteration 00251    Loss -0.023397  0.000143\n",
            "Iteration 00252    Loss -0.023890  0.000140\n",
            "Iteration 00253    Loss -0.023980  0.000129\n",
            "Iteration 00254    Loss -0.024041  0.000138\n",
            "Iteration 00255    Loss -0.024416  0.000142\n",
            "Iteration 00256    Loss -0.024564  0.000141\n",
            "Iteration 00257    Loss -0.024631  0.000143\n",
            "Iteration 00258    Loss -0.024828  0.000129\n",
            "Iteration 00259    Loss -0.025249  0.000137\n",
            "Iteration 00260    Loss -0.025066  0.000151\n",
            "Iteration 00261    Loss -0.025235  0.000136\n",
            "Iteration 00262    Loss -0.025300  0.000141\n",
            "Iteration 00263    Loss -0.025921  0.000144\n",
            "Iteration 00264    Loss -0.025823  0.000145\n",
            "Iteration 00265    Loss -0.026013  0.000156\n",
            "Iteration 00266    Loss -0.026204  0.000132\n",
            "Iteration 00267    Loss -0.026325  0.000132\n",
            "Iteration 00268    Loss -0.026544  0.000157\n",
            "Iteration 00269    Loss -0.026841  0.000143\n",
            "Iteration 00270    Loss -0.026976  0.000148\n",
            "Iteration 00271    Loss -0.027125  0.000147\n",
            "Iteration 00272    Loss -0.026536  0.000140\n",
            "Iteration 00273    Loss -0.027382  0.000145\n",
            "Iteration 00274    Loss -0.027405  0.000153\n",
            "Iteration 00275    Loss -0.027717  0.000143\n",
            "Iteration 00276    Loss -0.027773  0.000152\n",
            "Iteration 00277    Loss -0.027531  0.000161\n",
            "Iteration 00278    Loss -0.027731  0.000140\n",
            "Iteration 00279    Loss -0.028208  0.000140\n",
            "Iteration 00280    Loss -0.028224  0.000159\n",
            "Iteration 00281    Loss -0.028407  0.000144\n",
            "Iteration 00282    Loss -0.028708  0.000149\n",
            "Iteration 00283    Loss -0.028790  0.000157\n",
            "Iteration 00284    Loss -0.028689  0.000147\n",
            "Iteration 00285    Loss -0.029147  0.000151\n",
            "Iteration 00286    Loss -0.029020  0.000158\n",
            "Iteration 00287    Loss -0.029293  0.000149\n",
            "Iteration 00288    Loss -0.029541  0.000149\n",
            "Iteration 00289    Loss -0.029684  0.000161\n",
            "Iteration 00290    Loss -0.029615  0.000166\n",
            "Iteration 00291    Loss -0.029925  0.000160\n",
            "Iteration 00292    Loss -0.029783  0.000161\n",
            "Iteration 00293    Loss -0.029994  0.000168\n",
            "Iteration 00294    Loss -0.030203  0.000157\n",
            "Iteration 00295    Loss -0.030412  0.000167\n",
            "Iteration 00296    Loss -0.030547  0.000174\n",
            "Iteration 00297    Loss -0.030760  0.000162\n",
            "Iteration 00298    Loss -0.030788  0.000162\n",
            "Iteration 00299    Loss -0.030913  0.000170\n",
            "Iteration 00300    Loss -0.030870  0.000170\n",
            "Iteration 00301    Loss -0.030890  0.000176\n",
            "Iteration 00302    Loss -0.031088  0.000186\n",
            "Iteration 00303    Loss -0.031047  0.000174\n",
            "Iteration 00304    Loss -0.031293  0.000153\n",
            "Iteration 00305    Loss -0.031281  0.000181\n",
            "Iteration 00306    Loss -0.031460  0.000182\n",
            "Iteration 00307    Loss -0.031673  0.000168\n",
            "Iteration 00308    Loss -0.031819  0.000185\n",
            "Iteration 00309    Loss -0.031907  0.000179\n",
            "Iteration 00310    Loss -0.032060  0.000169\n",
            "Iteration 00311    Loss -0.032064  0.000167\n",
            "Iteration 00312    Loss -0.032321  0.000177\n",
            "Iteration 00313    Loss -0.032138  0.000169\n",
            "Iteration 00314    Loss -0.032407  0.000176\n",
            "Iteration 00315    Loss -0.032500  0.000186\n",
            "Iteration 00316    Loss -0.032689  0.000184\n",
            "Iteration 00317    Loss -0.032807  0.000181\n",
            "Iteration 00318    Loss -0.032833  0.000172\n",
            "Iteration 00319    Loss -0.033019  0.000194\n",
            "Iteration 00320    Loss -0.033124  0.000187\n",
            "Iteration 00321    Loss -0.033100  0.000180\n",
            "Iteration 00322    Loss -0.032780  0.000184\n",
            "Iteration 00323    Loss -0.033253  0.000190\n",
            "Iteration 00324    Loss -0.033271  0.000158\n",
            "Iteration 00325    Loss -0.033683  0.000176\n",
            "Iteration 00326    Loss -0.033439  0.000199\n",
            "Iteration 00327    Loss -0.033723  0.000157\n",
            "Iteration 00328    Loss -0.033878  0.000159\n",
            "Iteration 00329    Loss -0.033562  0.000193\n",
            "Iteration 00330    Loss -0.033975  0.000196\n",
            "Iteration 00331    Loss -0.034208  0.000169\n",
            "Iteration 00332    Loss -0.034342  0.000179\n",
            "Iteration 00333    Loss -0.034395  0.000196\n",
            "Iteration 00334    Loss -0.034480  0.000186\n",
            "Iteration 00335    Loss -0.034321  0.000168\n",
            "Iteration 00336    Loss -0.034670  0.000191\n",
            "Iteration 00337    Loss -0.034602  0.000195\n",
            "Iteration 00338    Loss -0.034611  0.000187\n",
            "Iteration 00339    Loss -0.034872  0.000188\n",
            "Iteration 00340    Loss -0.034858  0.000197\n",
            "Iteration 00341    Loss -0.034964  0.000200\n",
            "Iteration 00342    Loss -0.035118  0.000191\n",
            "Iteration 00343    Loss -0.035122  0.000186\n",
            "Iteration 00344    Loss -0.035044  0.000201\n",
            "Iteration 00345    Loss -0.035393  0.000187\n",
            "Iteration 00346    Loss -0.035466  0.000186\n",
            "Iteration 00347    Loss -0.035537  0.000200\n",
            "Iteration 00348    Loss -0.035566  0.000202\n",
            "Iteration 00349    Loss -0.035701  0.000200\n",
            "Iteration 00350    Loss -0.035876  0.000198\n",
            "Iteration 00351    Loss -0.035866  0.000201\n",
            "Iteration 00352    Loss -0.035813  0.000206\n",
            "Iteration 00353    Loss -0.036004  0.000194\n",
            "Iteration 00354    Loss -0.036135  0.000203\n",
            "Iteration 00355    Loss -0.036247  0.000216\n",
            "Iteration 00356    Loss -0.036337  0.000213\n",
            "Iteration 00357    Loss -0.036375  0.000202\n",
            "Iteration 00358    Loss -0.036448  0.000212\n",
            "Iteration 00359    Loss -0.036467  0.000226\n",
            "Iteration 00360    Loss -0.036517  0.000223\n",
            "Iteration 00361    Loss -0.036655  0.000216\n",
            "Iteration 00362    Loss -0.036735  0.000217\n",
            "Iteration 00363    Loss -0.036865  0.000213\n",
            "Iteration 00364    Loss -0.036564  0.000222\n",
            "Iteration 00365    Loss -0.036788  0.000231\n",
            "Iteration 00366    Loss -0.036917  0.000217\n",
            "Iteration 00367    Loss -0.036912  0.000219\n",
            "Iteration 00368    Loss -0.037065  0.000233\n",
            "Iteration 00369    Loss -0.037187  0.000216\n",
            "Iteration 00370    Loss -0.037320  0.000228\n",
            "Iteration 00371    Loss -0.037216  0.000237\n",
            "Iteration 00372    Loss -0.037177  0.000215\n",
            "Iteration 00373    Loss -0.037342  0.000227\n",
            "Iteration 00374    Loss -0.037169  0.000237\n",
            "Iteration 00375    Loss -0.037504  0.000225\n",
            "Iteration 00376    Loss -0.037478  0.000239\n",
            "Iteration 00377    Loss -0.037562  0.000246\n",
            "Iteration 00378    Loss -0.037550  0.000212\n",
            "Iteration 00379    Loss -0.037505  0.000209\n",
            "Iteration 00380    Loss -0.037739  0.000238\n",
            "Iteration 00381    Loss -0.037868  0.000238\n",
            "Iteration 00382    Loss -0.037892  0.000225\n",
            "Iteration 00383    Loss -0.037895  0.000229\n",
            "Iteration 00384    Loss -0.037960  0.000247\n",
            "Iteration 00385    Loss -0.038013  0.000248\n",
            "Iteration 00386    Loss -0.038158  0.000213\n",
            "Iteration 00387    Loss -0.038177  0.000244\n",
            "Iteration 00388    Loss -0.038128  0.000241\n",
            "Iteration 00389    Loss -0.038314  0.000224\n",
            "Iteration 00390    Loss -0.038416  0.000252\n",
            "Iteration 00391    Loss -0.038233  0.000237\n",
            "Iteration 00392    Loss -0.038480  0.000233\n",
            "Iteration 00393    Loss -0.038516  0.000267\n",
            "Iteration 00394    Loss -0.038460  0.000228\n",
            "Iteration 00395    Loss -0.038651  0.000228\n",
            "Iteration 00396    Loss -0.038637  0.000288\n",
            "Iteration 00397    Loss -0.038745  0.000230\n",
            "Iteration 00398    Loss -0.038771  0.000240\n",
            "Iteration 00399    Loss -0.038879  0.000272\n",
            "Iteration 00400    Loss -0.038888  0.000257\n",
            "Iteration 00401    Loss -0.038889  0.000245\n",
            "Iteration 00402    Loss -0.038973  0.000236\n",
            "Iteration 00403    Loss -0.039059  0.000263\n",
            "Iteration 00404    Loss -0.039144  0.000282\n",
            "Iteration 00405    Loss -0.039113  0.000256\n",
            "Iteration 00406    Loss -0.039040  0.000261\n",
            "Iteration 00407    Loss -0.039128  0.000247\n",
            "Iteration 00408    Loss -0.039126  0.000278\n",
            "Iteration 00409    Loss -0.039271  0.000258\n",
            "Iteration 00410    Loss -0.039197  0.000239\n",
            "Iteration 00411    Loss -0.039397  0.000293\n",
            "Iteration 00412    Loss -0.039333  0.000260\n",
            "Iteration 00413    Loss -0.039392  0.000221\n",
            "Iteration 00414    Loss -0.039478  0.000267\n",
            "Iteration 00415    Loss -0.039360  0.000298\n",
            "Iteration 00416    Loss -0.039494  0.000224\n",
            "Iteration 00417    Loss -0.039627  0.000236\n",
            "Iteration 00418    Loss -0.039559  0.000261\n",
            "Iteration 00419    Loss -0.039785  0.000283\n",
            "Iteration 00420    Loss -0.039760  0.000254\n",
            "Iteration 00421    Loss -0.039813  0.000258\n",
            "Iteration 00422    Loss -0.039810  0.000270\n",
            "Iteration 00423    Loss -0.039778  0.000271\n",
            "Iteration 00424    Loss -0.040036  0.000266\n",
            "Iteration 00425    Loss -0.040000  0.000280\n",
            "Iteration 00426    Loss -0.040097  0.000277\n",
            "Iteration 00427    Loss -0.040080  0.000284\n",
            "Iteration 00428    Loss -0.040111  0.000295\n",
            "Iteration 00429    Loss -0.040092  0.000270\n",
            "Iteration 00430    Loss -0.040223  0.000280\n",
            "Iteration 00431    Loss -0.040318  0.000309\n",
            "Iteration 00432    Loss -0.040344  0.000289\n",
            "Iteration 00433    Loss -0.040368  0.000294\n",
            "Iteration 00434    Loss -0.040365  0.000292\n",
            "Iteration 00435    Loss -0.040444  0.000293\n",
            "Iteration 00436    Loss -0.040467  0.000297\n",
            "Iteration 00437    Loss -0.040574  0.000299\n",
            "Iteration 00438    Loss -0.040599  0.000316\n",
            "Iteration 00439    Loss -0.040623  0.000315\n",
            "Iteration 00440    Loss -0.040579  0.000299\n",
            "Iteration 00441    Loss -0.040611  0.000311\n",
            "Iteration 00442    Loss -0.040512  0.000322\n",
            "Iteration 00443    Loss -0.040719  0.000325\n",
            "Iteration 00444    Loss -0.040757  0.000276\n",
            "Iteration 00445    Loss -0.040604  0.000325\n",
            "Iteration 00446    Loss -0.040588  0.000316\n",
            "Iteration 00447    Loss -0.040769  0.000306\n",
            "Iteration 00448    Loss -0.040929  0.000321\n",
            "Iteration 00449    Loss -0.040907  0.000305\n",
            "Iteration 00450    Loss -0.040750  0.000330\n",
            "Iteration 00451    Loss -0.041053  0.000323\n",
            "Iteration 00452    Loss -0.040983  0.000320\n",
            "Iteration 00453    Loss -0.041077  0.000324\n",
            "Iteration 00454    Loss -0.041143  0.000337\n",
            "Iteration 00455    Loss -0.041118  0.000305\n",
            "Iteration 00456    Loss -0.041098  0.000326\n",
            "Iteration 00457    Loss -0.041209  0.000322\n",
            "Iteration 00458    Loss -0.041235  0.000318\n",
            "Iteration 00459    Loss -0.041079  0.000325\n",
            "Iteration 00460    Loss -0.041175  0.000320\n",
            "Iteration 00461    Loss -0.041194  0.000306\n",
            "Iteration 00462    Loss -0.041101  0.000289\n",
            "Iteration 00463    Loss -0.041151  0.000313\n",
            "Iteration 00464    Loss -0.041323  0.000346\n",
            "Iteration 00465    Loss -0.041270  0.000272\n",
            "Iteration 00466    Loss -0.041341  0.000292\n",
            "Iteration 00467    Loss -0.041305  0.000304\n",
            "Iteration 00468    Loss -0.041506  0.000323\n",
            "Iteration 00469    Loss -0.041451  0.000322\n",
            "Iteration 00470    Loss -0.041544  0.000291\n",
            "Iteration 00471    Loss -0.041535  0.000300\n",
            "Iteration 00472    Loss -0.041644  0.000347\n",
            "Iteration 00473    Loss -0.041657  0.000320\n",
            "Iteration 00474    Loss -0.041700  0.000306\n",
            "Iteration 00475    Loss -0.041726  0.000313\n",
            "Iteration 00476    Loss -0.041771  0.000336\n",
            "Iteration 00477    Loss -0.041732  0.000337\n",
            "Iteration 00478    Loss -0.041822  0.000324\n",
            "Iteration 00479    Loss -0.041884  0.000326\n",
            "Iteration 00480    Loss -0.041894  0.000358\n",
            "Iteration 00481    Loss -0.041811  0.000333\n",
            "Iteration 00482    Loss -0.041988  0.000359\n",
            "Iteration 00483    Loss -0.042004  0.000360\n",
            "Iteration 00484    Loss -0.041922  0.000319\n",
            "Iteration 00485    Loss -0.041974  0.000368\n",
            "Iteration 00486    Loss -0.041975  0.000336\n",
            "Iteration 00487    Loss -0.042032  0.000359\n",
            "Iteration 00488    Loss -0.042145  0.000381\n",
            "Iteration 00489    Loss -0.042162  0.000327\n",
            "Iteration 00490    Loss -0.042189  0.000395\n",
            "Iteration 00491    Loss -0.042120  0.000356\n",
            "Iteration 00492    Loss -0.042136  0.000339\n",
            "Iteration 00493    Loss -0.042169  0.000348\n",
            "Iteration 00494    Loss -0.042203  0.000338\n",
            "Iteration 00495    Loss -0.042203  0.000376\n",
            "Iteration 00496    Loss -0.042306  0.000381\n",
            "Iteration 00497    Loss -0.042339  0.000337\n",
            "Iteration 00498    Loss -0.042341  0.000353\n",
            "Iteration 00499    Loss -0.042416  0.000369\n",
            " 96% 64/67 [35:10<01:37, 32.37s/it]/content/ZID/REAL/11.png\n",
            "Iteration 00000    Loss 0.439409  0.000013\n",
            "Iteration 00001    Loss 3.898444  0.000004\n",
            "Iteration 00002    Loss 0.360318  0.000003\n",
            "Iteration 00003    Loss 0.289368  0.000002\n",
            "Iteration 00004    Loss 0.351155  0.000002\n",
            "Iteration 00005    Loss 0.218795  0.000001\n",
            "Iteration 00006    Loss 0.199036  0.000001\n",
            "Iteration 00007    Loss 0.172518  0.000001\n",
            "Iteration 00008    Loss 0.155413  0.000001\n",
            "Iteration 00009    Loss 0.140758  0.000001\n",
            "Iteration 00010    Loss 0.163413  0.000001\n",
            "Iteration 00011    Loss 0.119453  0.000001\n",
            "Iteration 00012    Loss 0.111309  0.000002\n",
            "Iteration 00013    Loss 0.104900  0.000002\n",
            "Iteration 00014    Loss 0.100804  0.000002\n",
            "Iteration 00015    Loss 0.097155  0.000003\n",
            "Iteration 00016    Loss 0.092383  0.000003\n",
            "Iteration 00017    Loss 0.089334  0.000002\n",
            "Iteration 00018    Loss 0.086271  0.000002\n",
            "Iteration 00019    Loss 0.083360  0.000002\n",
            "Iteration 00020    Loss 0.081010  0.000003\n",
            "Iteration 00021    Loss 0.079234  0.000002\n",
            "Iteration 00022    Loss 0.077231  0.000002\n",
            "Iteration 00023    Loss 0.075178  0.000002\n",
            "Iteration 00024    Loss 0.073265  0.000002\n",
            "Iteration 00025    Loss 0.071612  0.000002\n",
            "Iteration 00026    Loss 0.070058  0.000002\n",
            "Iteration 00027    Loss 0.068949  0.000002\n",
            "Iteration 00028    Loss 0.067160  0.000002\n",
            "Iteration 00029    Loss 0.066125  0.000002\n",
            "Iteration 00030    Loss 0.064526  0.000002\n",
            "Iteration 00031    Loss 0.063250  0.000002\n",
            "Iteration 00032    Loss 0.061974  0.000002\n",
            "Iteration 00033    Loss 0.060792  0.000003\n",
            "Iteration 00034    Loss 0.059366  0.000003\n",
            "Iteration 00035    Loss 0.058390  0.000003\n",
            "Iteration 00036    Loss 0.057146  0.000003\n",
            "Iteration 00037    Loss 0.055987  0.000003\n",
            "Iteration 00038    Loss 0.054912  0.000004\n",
            "Iteration 00039    Loss 0.053899  0.000005\n",
            "Iteration 00040    Loss 0.052790  0.000005\n",
            "Iteration 00041    Loss 0.051718  0.000005\n",
            "Iteration 00042    Loss 0.050731  0.000005\n",
            "Iteration 00043    Loss 0.050156  0.000006\n",
            "Iteration 00044    Loss 0.049211  0.000008\n",
            "Iteration 00045    Loss 0.048443  0.000010\n",
            "Iteration 00046    Loss 0.047572  0.000011\n",
            "Iteration 00047    Loss 0.046498  0.000010\n",
            "Iteration 00048    Loss 0.045661  0.000007\n",
            "Iteration 00049    Loss 0.044740  0.000005\n",
            "Iteration 00050    Loss 0.044019  0.000004\n",
            "Iteration 00051    Loss 0.043044  0.000004\n",
            "Iteration 00052    Loss 0.042245  0.000004\n",
            "Iteration 00053    Loss 0.041368  0.000005\n",
            "Iteration 00054    Loss 0.040683  0.000005\n",
            "Iteration 00055    Loss 0.039836  0.000006\n",
            "Iteration 00056    Loss 0.039335  0.000007\n",
            "Iteration 00057    Loss 0.039830  0.000007\n",
            "Iteration 00058    Loss 0.038556  0.000006\n",
            "Iteration 00059    Loss 0.038417  0.000006\n",
            "Iteration 00060    Loss 0.037003  0.000005\n",
            "Iteration 00061    Loss 0.036474  0.000005\n",
            "Iteration 00062    Loss 0.035602  0.000005\n",
            "Iteration 00063    Loss 0.034508  0.000006\n",
            "Iteration 00064    Loss 0.034025  0.000006\n",
            "Iteration 00065    Loss 0.033264  0.000006\n",
            "Iteration 00066    Loss 0.032412  0.000006\n",
            "Iteration 00067    Loss 0.031778  0.000006\n",
            "Iteration 00068    Loss 0.031051  0.000007\n",
            "Iteration 00069    Loss 0.030397  0.000007\n",
            "Iteration 00070    Loss 0.029693  0.000008\n",
            "Iteration 00071    Loss 0.029025  0.000008\n",
            "Iteration 00072    Loss 0.028438  0.000008\n",
            "Iteration 00073    Loss 0.027695  0.000006\n",
            "Iteration 00074    Loss 0.027061  0.000006\n",
            "Iteration 00075    Loss 0.026471  0.000006\n",
            "Iteration 00076    Loss 0.025810  0.000008\n",
            "Iteration 00077    Loss 0.025308  0.000010\n",
            "Iteration 00078    Loss 0.024649  0.000010\n",
            "Iteration 00079    Loss 0.024107  0.000009\n",
            "Iteration 00080    Loss 0.023526  0.000008\n",
            "Iteration 00081    Loss 0.022958  0.000007\n",
            "Iteration 00082    Loss 0.022407  0.000007\n",
            "Iteration 00083    Loss 0.021755  0.000008\n",
            "Iteration 00084    Loss 0.021288  0.000008\n",
            "Iteration 00085    Loss 0.020734  0.000009\n",
            "Iteration 00086    Loss 0.020150  0.000009\n",
            "Iteration 00087    Loss 0.019608  0.000009\n",
            "Iteration 00088    Loss 0.019167  0.000009\n",
            "Iteration 00089    Loss 0.018724  0.000010\n",
            "Iteration 00090    Loss 0.018347  0.000010\n",
            "Iteration 00091    Loss 0.017926  0.000012\n",
            "Iteration 00092    Loss 0.017509  0.000011\n",
            "Iteration 00093    Loss 0.017019  0.000010\n",
            "Iteration 00094    Loss 0.016538  0.000009\n",
            "Iteration 00095    Loss 0.015982  0.000009\n",
            "Iteration 00096    Loss 0.015531  0.000010\n",
            "Iteration 00097    Loss 0.015109  0.000011\n",
            "Iteration 00098    Loss 0.014500  0.000012\n",
            "Iteration 00099    Loss 0.013963  0.000009\n",
            "Iteration 00100    Loss 0.013535  0.000009\n",
            "Iteration 00101    Loss 0.013038  0.000008\n",
            "Iteration 00102    Loss 0.012588  0.000009\n",
            "Iteration 00103    Loss 0.012136  0.000011\n",
            "Iteration 00104    Loss 0.011663  0.000013\n",
            "Iteration 00105    Loss 0.011230  0.000008\n",
            "Iteration 00106    Loss 0.010809  0.000007\n",
            "Iteration 00107    Loss 0.010419  0.000011\n",
            "Iteration 00108    Loss 0.009986  0.000013\n",
            "Iteration 00109    Loss 0.009581  0.000009\n",
            "Iteration 00110    Loss 0.009108  0.000012\n",
            "Iteration 00111    Loss 0.008719  0.000015\n",
            "Iteration 00112    Loss 0.008321  0.000008\n",
            "Iteration 00113    Loss 0.007989  0.000013\n",
            "Iteration 00114    Loss 0.007503  0.000013\n",
            "Iteration 00115    Loss 0.007157  0.000015\n",
            "Iteration 00116    Loss 0.006761  0.000015\n",
            "Iteration 00117    Loss 0.006373  0.000011\n",
            "Iteration 00118    Loss 0.006001  0.000012\n",
            "Iteration 00119    Loss 0.005655  0.000013\n",
            "Iteration 00120    Loss 0.005296  0.000019\n",
            "Iteration 00121    Loss 0.004891  0.000011\n",
            "Iteration 00122    Loss 0.004547  0.000009\n",
            "Iteration 00123    Loss 0.004212  0.000013\n",
            "Iteration 00124    Loss 0.003854  0.000012\n",
            "Iteration 00125    Loss 0.003562  0.000015\n",
            "Iteration 00126    Loss 0.003268  0.000018\n",
            "Iteration 00127    Loss 0.002873  0.000016\n",
            "Iteration 00128    Loss 0.002524  0.000008\n",
            "Iteration 00129    Loss 0.002224  0.000010\n",
            "Iteration 00130    Loss 0.001891  0.000017\n",
            "Iteration 00131    Loss 0.001599  0.000013\n",
            "Iteration 00132    Loss 0.001259  0.000010\n",
            "Iteration 00133    Loss 0.000928  0.000011\n",
            "Iteration 00134    Loss 0.000598  0.000014\n",
            "Iteration 00135    Loss 0.000345  0.000014\n",
            "Iteration 00136    Loss 0.000001  0.000011\n",
            "Iteration 00137    Loss -0.000262  0.000010\n",
            "Iteration 00138    Loss -0.000599  0.000013\n",
            "Iteration 00139    Loss -0.000896  0.000017\n",
            "Iteration 00140    Loss -0.001195  0.000016\n",
            "Iteration 00141    Loss -0.001522  0.000013\n",
            "Iteration 00142    Loss -0.001793  0.000011\n",
            "Iteration 00143    Loss -0.002094  0.000014\n",
            "Iteration 00144    Loss -0.002317  0.000012\n",
            "Iteration 00145    Loss -0.002616  0.000008\n",
            "Iteration 00146    Loss -0.002950  0.000008\n",
            "Iteration 00147    Loss -0.003131  0.000010\n",
            "Iteration 00148    Loss -0.003416  0.000007\n",
            "Iteration 00149    Loss -0.003650  0.000007\n",
            "Iteration 00150    Loss -0.003898  0.000011\n",
            "Iteration 00151    Loss -0.004153  0.000018\n",
            "Iteration 00152    Loss -0.004470  0.000022\n",
            "Iteration 00153    Loss -0.004720  0.000014\n",
            "Iteration 00154    Loss -0.004966  0.000010\n",
            "Iteration 00155    Loss -0.005247  0.000010\n",
            "Iteration 00156    Loss -0.005490  0.000012\n",
            "Iteration 00157    Loss -0.005775  0.000014\n",
            "Iteration 00158    Loss -0.005928  0.000014\n",
            "Iteration 00159    Loss -0.006272  0.000013\n",
            "Iteration 00160    Loss -0.006538  0.000012\n",
            "Iteration 00161    Loss -0.006732  0.000011\n",
            "Iteration 00162    Loss -0.006911  0.000012\n",
            "Iteration 00163    Loss -0.007170  0.000014\n",
            "Iteration 00164    Loss -0.007420  0.000014\n",
            "Iteration 00165    Loss -0.007625  0.000012\n",
            "Iteration 00166    Loss -0.007949  0.000009\n",
            "Iteration 00167    Loss -0.008119  0.000008\n",
            "Iteration 00168    Loss -0.008416  0.000007\n",
            "Iteration 00169    Loss -0.008630  0.000007\n",
            "Iteration 00170    Loss -0.008870  0.000008\n",
            "Iteration 00171    Loss -0.009083  0.000008\n",
            "Iteration 00172    Loss -0.009335  0.000008\n",
            "Iteration 00173    Loss -0.009528  0.000008\n",
            "Iteration 00174    Loss -0.009806  0.000008\n",
            "Iteration 00175    Loss -0.009970  0.000008\n",
            "Iteration 00176    Loss -0.010232  0.000007\n",
            "Iteration 00177    Loss -0.010457  0.000007\n",
            "Iteration 00178    Loss -0.010689  0.000008\n",
            "Iteration 00179    Loss -0.010904  0.000008\n",
            "Iteration 00180    Loss -0.011141  0.000007\n",
            "Iteration 00181    Loss -0.011333  0.000007\n",
            "Iteration 00182    Loss -0.011525  0.000006\n",
            "Iteration 00183    Loss -0.011745  0.000007\n",
            "Iteration 00184    Loss -0.011996  0.000007\n",
            "Iteration 00185    Loss -0.012169  0.000007\n",
            "Iteration 00186    Loss -0.012383  0.000006\n",
            "Iteration 00187    Loss -0.012616  0.000006\n",
            "Iteration 00188    Loss -0.012784  0.000006\n",
            "Iteration 00189    Loss -0.012958  0.000007\n",
            "Iteration 00190    Loss -0.013212  0.000007\n",
            "Iteration 00191    Loss -0.013389  0.000007\n",
            "Iteration 00192    Loss -0.013580  0.000007\n",
            "Iteration 00193    Loss -0.013793  0.000007\n",
            "Iteration 00194    Loss -0.013959  0.000007\n",
            "Iteration 00195    Loss -0.014178  0.000007\n",
            "Iteration 00196    Loss -0.014398  0.000008\n",
            "Iteration 00197    Loss -0.014581  0.000007\n",
            "Iteration 00198    Loss -0.014805  0.000007\n",
            "Iteration 00199    Loss -0.014968  0.000006\n",
            "Iteration 00200    Loss -0.015155  0.000006\n",
            "Iteration 00201    Loss -0.015334  0.000007\n",
            "Iteration 00202    Loss -0.015499  0.000007\n",
            "Iteration 00203    Loss -0.015682  0.000006\n",
            "Iteration 00204    Loss -0.015882  0.000006\n",
            "Iteration 00205    Loss -0.016102  0.000006\n",
            "Iteration 00206    Loss -0.016294  0.000006\n",
            "Iteration 00207    Loss -0.016434  0.000006\n",
            "Iteration 00208    Loss -0.016647  0.000006\n",
            "Iteration 00209    Loss -0.016852  0.000006\n",
            "Iteration 00210    Loss -0.017003  0.000005\n",
            "Iteration 00211    Loss -0.017216  0.000005\n",
            "Iteration 00212    Loss -0.017368  0.000005\n",
            "Iteration 00213    Loss -0.017570  0.000005\n",
            "Iteration 00214    Loss -0.017740  0.000005\n",
            "Iteration 00215    Loss -0.017893  0.000005\n",
            "Iteration 00216    Loss -0.018055  0.000005\n",
            "Iteration 00217    Loss -0.018246  0.000005\n",
            "Iteration 00218    Loss -0.018434  0.000005\n",
            "Iteration 00219    Loss -0.018590  0.000005\n",
            "Iteration 00220    Loss -0.018770  0.000005\n",
            "Iteration 00221    Loss -0.018927  0.000005\n",
            "Iteration 00222    Loss -0.019096  0.000005\n",
            "Iteration 00223    Loss -0.019264  0.000005\n",
            "Iteration 00224    Loss -0.019419  0.000004\n",
            "Iteration 00225    Loss -0.019608  0.000004\n",
            "Iteration 00226    Loss -0.019767  0.000004\n",
            "Iteration 00227    Loss -0.019881  0.000004\n",
            "Iteration 00228    Loss -0.020049  0.000004\n",
            "Iteration 00229    Loss -0.020219  0.000004\n",
            "Iteration 00230    Loss -0.020411  0.000004\n",
            "Iteration 00231    Loss -0.020573  0.000004\n",
            "Iteration 00232    Loss -0.020728  0.000004\n",
            "Iteration 00233    Loss -0.020883  0.000004\n",
            "Iteration 00234    Loss -0.021055  0.000004\n",
            "Iteration 00235    Loss -0.021209  0.000005\n",
            "Iteration 00236    Loss -0.021349  0.000004\n",
            "Iteration 00237    Loss -0.021505  0.000004\n",
            "Iteration 00238    Loss -0.021625  0.000004\n",
            "Iteration 00239    Loss -0.021800  0.000004\n",
            "Iteration 00240    Loss -0.021919  0.000005\n",
            "Iteration 00241    Loss -0.022108  0.000004\n",
            "Iteration 00242    Loss -0.022263  0.000004\n",
            "Iteration 00243    Loss -0.022394  0.000004\n",
            "Iteration 00244    Loss -0.022553  0.000004\n",
            "Iteration 00245    Loss -0.022720  0.000004\n",
            "Iteration 00246    Loss -0.022891  0.000004\n",
            "Iteration 00247    Loss -0.022980  0.000004\n",
            "Iteration 00248    Loss -0.023111  0.000004\n",
            "Iteration 00249    Loss -0.023234  0.000004\n",
            "Iteration 00250    Loss -0.023412  0.000004\n",
            "Iteration 00251    Loss -0.023511  0.000004\n",
            "Iteration 00252    Loss -0.023704  0.000004\n",
            "Iteration 00253    Loss -0.023852  0.000004\n",
            "Iteration 00254    Loss -0.023985  0.000004\n",
            "Iteration 00255    Loss -0.024125  0.000004\n",
            "Iteration 00256    Loss -0.024287  0.000004\n",
            "Iteration 00257    Loss -0.024399  0.000004\n",
            "Iteration 00258    Loss -0.024586  0.000004\n",
            "Iteration 00259    Loss -0.024695  0.000004\n",
            "Iteration 00260    Loss -0.024855  0.000004\n",
            "Iteration 00261    Loss -0.024960  0.000004\n",
            "Iteration 00262    Loss -0.025105  0.000004\n",
            "Iteration 00263    Loss -0.025263  0.000004\n",
            "Iteration 00264    Loss -0.025398  0.000004\n",
            "Iteration 00265    Loss -0.025514  0.000004\n",
            "Iteration 00266    Loss -0.025616  0.000004\n",
            "Iteration 00267    Loss -0.025769  0.000004\n",
            "Iteration 00268    Loss -0.025905  0.000004\n",
            "Iteration 00269    Loss -0.026018  0.000004\n",
            "Iteration 00270    Loss -0.026177  0.000004\n",
            "Iteration 00271    Loss -0.026320  0.000004\n",
            "Iteration 00272    Loss -0.026445  0.000004\n",
            "Iteration 00273    Loss -0.026571  0.000004\n",
            "Iteration 00274    Loss -0.026705  0.000004\n",
            "Iteration 00275    Loss -0.026798  0.000004\n",
            "Iteration 00276    Loss -0.026956  0.000004\n",
            "Iteration 00277    Loss -0.027068  0.000004\n",
            "Iteration 00278    Loss -0.027180  0.000004\n",
            "Iteration 00279    Loss -0.027326  0.000004\n",
            "Iteration 00280    Loss -0.027448  0.000004\n",
            "Iteration 00281    Loss -0.027554  0.000004\n",
            "Iteration 00282    Loss -0.027683  0.000004\n",
            "Iteration 00283    Loss -0.027801  0.000004\n",
            "Iteration 00284    Loss -0.027932  0.000004\n",
            "Iteration 00285    Loss -0.028053  0.000004\n",
            "Iteration 00286    Loss -0.028158  0.000004\n",
            "Iteration 00287    Loss -0.028287  0.000004\n",
            "Iteration 00288    Loss -0.028368  0.000004\n",
            "Iteration 00289    Loss -0.028515  0.000004\n",
            "Iteration 00290    Loss -0.028613  0.000004\n",
            "Iteration 00291    Loss -0.028756  0.000004\n",
            "Iteration 00292    Loss -0.028864  0.000004\n",
            "Iteration 00293    Loss -0.028963  0.000004\n",
            "Iteration 00294    Loss -0.029079  0.000004\n",
            "Iteration 00295    Loss -0.029199  0.000004\n",
            "Iteration 00296    Loss -0.029310  0.000004\n",
            "Iteration 00297    Loss -0.029388  0.000004\n",
            "Iteration 00298    Loss -0.029472  0.000005\n",
            "Iteration 00299    Loss -0.029610  0.000004\n",
            "Iteration 00300    Loss -0.029731  0.000004\n",
            "Iteration 00301    Loss -0.029842  0.000005\n",
            "Iteration 00302    Loss -0.029974  0.000005\n",
            "Iteration 00303    Loss -0.030070  0.000005\n",
            "Iteration 00304    Loss -0.030170  0.000005\n",
            "Iteration 00305    Loss -0.030279  0.000005\n",
            "Iteration 00306    Loss -0.030382  0.000004\n",
            "Iteration 00307    Loss -0.030461  0.000004\n",
            "Iteration 00308    Loss -0.030599  0.000005\n",
            "Iteration 00309    Loss -0.030688  0.000005\n",
            "Iteration 00310    Loss -0.030777  0.000005\n",
            "Iteration 00311    Loss -0.030864  0.000004\n",
            "Iteration 00312    Loss -0.030973  0.000004\n",
            "Iteration 00313    Loss -0.031089  0.000005\n",
            "Iteration 00314    Loss -0.031186  0.000004\n",
            "Iteration 00315    Loss -0.031270  0.000005\n",
            "Iteration 00316    Loss -0.031382  0.000005\n",
            "Iteration 00317    Loss -0.031459  0.000005\n",
            "Iteration 00318    Loss -0.031590  0.000004\n",
            "Iteration 00319    Loss -0.031693  0.000004\n",
            "Iteration 00320    Loss -0.031783  0.000005\n",
            "Iteration 00321    Loss -0.031883  0.000005\n",
            "Iteration 00322    Loss -0.031994  0.000005\n",
            "Iteration 00323    Loss -0.032083  0.000004\n",
            "Iteration 00324    Loss -0.032174  0.000004\n",
            "Iteration 00325    Loss -0.032274  0.000005\n",
            "Iteration 00326    Loss -0.032350  0.000005\n",
            "Iteration 00327    Loss -0.032459  0.000005\n",
            "Iteration 00328    Loss -0.032546  0.000005\n",
            "Iteration 00329    Loss -0.032628  0.000004\n",
            "Iteration 00330    Loss -0.032730  0.000004\n",
            "Iteration 00331    Loss -0.032803  0.000004\n",
            "Iteration 00332    Loss -0.032903  0.000005\n",
            "Iteration 00333    Loss -0.032995  0.000005\n",
            "Iteration 00334    Loss -0.033087  0.000004\n",
            "Iteration 00335    Loss -0.033169  0.000004\n",
            "Iteration 00336    Loss -0.033259  0.000005\n",
            "Iteration 00337    Loss -0.033319  0.000004\n",
            "Iteration 00338    Loss -0.033421  0.000005\n",
            "Iteration 00339    Loss -0.033498  0.000004\n",
            "Iteration 00340    Loss -0.033612  0.000005\n",
            "Iteration 00341    Loss -0.033695  0.000005\n",
            "Iteration 00342    Loss -0.033770  0.000004\n",
            "Iteration 00343    Loss -0.033859  0.000005\n",
            "Iteration 00344    Loss -0.033966  0.000004\n",
            "Iteration 00345    Loss -0.034030  0.000004\n",
            "Iteration 00346    Loss -0.034120  0.000004\n",
            "Iteration 00347    Loss -0.034211  0.000004\n",
            "Iteration 00348    Loss -0.034265  0.000005\n",
            "Iteration 00349    Loss -0.034364  0.000005\n",
            "Iteration 00350    Loss -0.034450  0.000004\n",
            "Iteration 00351    Loss -0.034537  0.000004\n",
            "Iteration 00352    Loss -0.034615  0.000004\n",
            "Iteration 00353    Loss -0.034694  0.000004\n",
            "Iteration 00354    Loss -0.034779  0.000004\n",
            "Iteration 00355    Loss -0.034859  0.000005\n",
            "Iteration 00356    Loss -0.034940  0.000004\n",
            "Iteration 00357    Loss -0.035019  0.000004\n",
            "Iteration 00358    Loss -0.035090  0.000004\n",
            "Iteration 00359    Loss -0.035182  0.000004\n",
            "Iteration 00360    Loss -0.035247  0.000004\n",
            "Iteration 00361    Loss -0.035325  0.000005\n",
            "Iteration 00362    Loss -0.035382  0.000004\n",
            "Iteration 00363    Loss -0.035485  0.000004\n",
            "Iteration 00364    Loss -0.035556  0.000005\n",
            "Iteration 00365    Loss -0.035626  0.000005\n",
            "Iteration 00366    Loss -0.035713  0.000005\n",
            "Iteration 00367    Loss -0.035786  0.000004\n",
            "Iteration 00368    Loss -0.035856  0.000004\n",
            "Iteration 00369    Loss -0.035917  0.000004\n",
            "Iteration 00370    Loss -0.036008  0.000005\n",
            "Iteration 00371    Loss -0.036063  0.000005\n",
            "Iteration 00372    Loss -0.036135  0.000005\n",
            "Iteration 00373    Loss -0.036210  0.000004\n",
            "Iteration 00374    Loss -0.036271  0.000005\n",
            "Iteration 00375    Loss -0.036356  0.000005\n",
            "Iteration 00376    Loss -0.036410  0.000005\n",
            "Iteration 00377    Loss -0.036489  0.000004\n",
            "Iteration 00378    Loss -0.036541  0.000004\n",
            "Iteration 00379    Loss -0.036620  0.000005\n",
            "Iteration 00380    Loss -0.036698  0.000005\n",
            "Iteration 00381    Loss -0.036773  0.000004\n",
            "Iteration 00382    Loss -0.036829  0.000005\n",
            "Iteration 00383    Loss -0.036898  0.000005\n",
            "Iteration 00384    Loss -0.036980  0.000005\n",
            "Iteration 00385    Loss -0.037035  0.000005\n",
            "Iteration 00386    Loss -0.037096  0.000005\n",
            "Iteration 00387    Loss -0.037155  0.000005\n",
            "Iteration 00388    Loss -0.037223  0.000005\n",
            "Iteration 00389    Loss -0.037307  0.000005\n",
            "Iteration 00390    Loss -0.037358  0.000005\n",
            "Iteration 00391    Loss -0.037413  0.000005\n",
            "Iteration 00392    Loss -0.037473  0.000005\n",
            "Iteration 00393    Loss -0.037547  0.000005\n",
            "Iteration 00394    Loss -0.037593  0.000005\n",
            "Iteration 00395    Loss -0.037668  0.000005\n",
            "Iteration 00396    Loss -0.037728  0.000005\n",
            "Iteration 00397    Loss -0.037792  0.000005\n",
            "Iteration 00398    Loss -0.037857  0.000005\n",
            "Iteration 00399    Loss -0.037921  0.000004\n",
            "Iteration 00400    Loss -0.037978  0.000005\n",
            "Iteration 00401    Loss -0.038027  0.000005\n",
            "Iteration 00402    Loss -0.038093  0.000005\n",
            "Iteration 00403    Loss -0.038151  0.000005\n",
            "Iteration 00404    Loss -0.038208  0.000005\n",
            "Iteration 00405    Loss -0.038282  0.000005\n",
            "Iteration 00406    Loss -0.038333  0.000005\n",
            "Iteration 00407    Loss -0.038395  0.000005\n",
            "Iteration 00408    Loss -0.038468  0.000005\n",
            "Iteration 00409    Loss -0.038525  0.000005\n",
            "Iteration 00410    Loss -0.038573  0.000005\n",
            "Iteration 00411    Loss -0.038629  0.000005\n",
            "Iteration 00412    Loss -0.038688  0.000005\n",
            "Iteration 00413    Loss -0.038734  0.000005\n",
            "Iteration 00414    Loss -0.038777  0.000005\n",
            "Iteration 00415    Loss -0.038836  0.000005\n",
            "Iteration 00416    Loss -0.038878  0.000005\n",
            "Iteration 00417    Loss -0.038939  0.000005\n",
            "Iteration 00418    Loss -0.039006  0.000005\n",
            "Iteration 00419    Loss -0.039077  0.000005\n",
            "Iteration 00420    Loss -0.039114  0.000005\n",
            "Iteration 00421    Loss -0.039172  0.000005\n",
            "Iteration 00422    Loss -0.039227  0.000005\n",
            "Iteration 00423    Loss -0.039273  0.000005\n",
            "Iteration 00424    Loss -0.039330  0.000004\n",
            "Iteration 00425    Loss -0.039388  0.000004\n",
            "Iteration 00426    Loss -0.039443  0.000005\n",
            "Iteration 00427    Loss -0.039491  0.000005\n",
            "Iteration 00428    Loss -0.039535  0.000005\n",
            "Iteration 00429    Loss -0.039594  0.000004\n",
            "Iteration 00430    Loss -0.039644  0.000005\n",
            "Iteration 00431    Loss -0.039690  0.000005\n",
            "Iteration 00432    Loss -0.039739  0.000005\n",
            "Iteration 00433    Loss -0.039806  0.000005\n",
            "Iteration 00434    Loss -0.039833  0.000005\n",
            "Iteration 00435    Loss -0.039902  0.000005\n",
            "Iteration 00436    Loss -0.039946  0.000005\n",
            "Iteration 00437    Loss -0.039991  0.000005\n",
            "Iteration 00438    Loss -0.040031  0.000005\n",
            "Iteration 00439    Loss -0.040087  0.000005\n",
            "Iteration 00440    Loss -0.040130  0.000005\n",
            "Iteration 00441    Loss -0.040192  0.000005\n",
            "Iteration 00442    Loss -0.040237  0.000005\n",
            "Iteration 00443    Loss -0.040278  0.000005\n",
            "Iteration 00444    Loss -0.040309  0.000005\n",
            "Iteration 00445    Loss -0.040369  0.000005\n",
            "Iteration 00446    Loss -0.040421  0.000005\n",
            "Iteration 00447    Loss -0.040456  0.000005\n",
            "Iteration 00448    Loss -0.040508  0.000005\n",
            "Iteration 00449    Loss -0.040553  0.000005\n",
            "Iteration 00450    Loss -0.040576  0.000005\n",
            "Iteration 00451    Loss -0.040609  0.000005\n",
            "Iteration 00452    Loss -0.040635  0.000005\n",
            "Iteration 00453    Loss -0.040715  0.000005\n",
            "Iteration 00454    Loss -0.040765  0.000005\n",
            "Iteration 00455    Loss -0.040797  0.000005\n",
            "Iteration 00456    Loss -0.040844  0.000004\n",
            "Iteration 00457    Loss -0.040893  0.000004\n",
            "Iteration 00458    Loss -0.040927  0.000005\n",
            "Iteration 00459    Loss -0.040984  0.000005\n",
            "Iteration 00460    Loss -0.041011  0.000004\n",
            "Iteration 00461    Loss -0.041067  0.000004\n",
            "Iteration 00462    Loss -0.041114  0.000005\n",
            "Iteration 00463    Loss -0.041152  0.000005\n",
            "Iteration 00464    Loss -0.041185  0.000004\n",
            "Iteration 00465    Loss -0.041230  0.000005\n",
            "Iteration 00466    Loss -0.041275  0.000005\n",
            "Iteration 00467    Loss -0.041307  0.000005\n",
            "Iteration 00468    Loss -0.041361  0.000004\n",
            "Iteration 00469    Loss -0.041402  0.000005\n",
            "Iteration 00470    Loss -0.041435  0.000005\n",
            "Iteration 00471    Loss -0.041486  0.000005\n",
            "Iteration 00472    Loss -0.041514  0.000005\n",
            "Iteration 00473    Loss -0.041555  0.000005\n",
            "Iteration 00474    Loss -0.041601  0.000005\n",
            "Iteration 00475    Loss -0.041632  0.000005\n",
            "Iteration 00476    Loss -0.041678  0.000005\n",
            "Iteration 00477    Loss -0.041714  0.000005\n",
            "Iteration 00478    Loss -0.041757  0.000005\n",
            "Iteration 00479    Loss -0.041789  0.000005\n",
            "Iteration 00480    Loss -0.041830  0.000005\n",
            "Iteration 00481    Loss -0.041870  0.000005\n",
            "Iteration 00482    Loss -0.041915  0.000005\n",
            "Iteration 00483    Loss -0.041952  0.000005\n",
            "Iteration 00484    Loss -0.041982  0.000005\n",
            "Iteration 00485    Loss -0.042020  0.000005\n",
            "Iteration 00486    Loss -0.042062  0.000005\n",
            "Iteration 00487    Loss -0.042085  0.000005\n",
            "Iteration 00488    Loss -0.042127  0.000005\n",
            "Iteration 00489    Loss -0.042161  0.000005\n",
            "Iteration 00490    Loss -0.042192  0.000005\n",
            "Iteration 00491    Loss -0.042236  0.000005\n",
            "Iteration 00492    Loss -0.042280  0.000005\n",
            "Iteration 00493    Loss -0.042308  0.000005\n",
            "Iteration 00494    Loss -0.042342  0.000005\n",
            "Iteration 00495    Loss -0.042365  0.000005\n",
            "Iteration 00496    Loss -0.042405  0.000005\n",
            "Iteration 00497    Loss -0.042433  0.000005\n",
            "Iteration 00498    Loss -0.042470  0.000005\n",
            "Iteration 00499    Loss -0.042512  0.000005\n",
            " 97% 65/67 [35:40<01:03, 31.61s/it]/content/ZID/REAL/test_image1.png\n",
            "Iteration 00000    Loss 0.370236  0.000010\n",
            "Iteration 00001    Loss 15.482818  0.000005\n",
            "Iteration 00002    Loss 0.332856  0.000001\n",
            "Iteration 00003    Loss 0.214270  0.000001\n",
            "Iteration 00004    Loss 0.196635  0.000000\n",
            "Iteration 00005    Loss 0.158822  0.000000\n",
            "Iteration 00006    Loss 0.139332  0.000000\n",
            "Iteration 00007    Loss 0.124658  0.000000\n",
            "Iteration 00008    Loss 0.112181  0.000000\n",
            "Iteration 00009    Loss 0.103921  0.000000\n",
            "Iteration 00010    Loss 0.097341  0.000000\n",
            "Iteration 00011    Loss 0.091483  0.000000\n",
            "Iteration 00012    Loss 0.087136  0.000000\n",
            "Iteration 00013    Loss 0.084107  0.000000\n",
            "Iteration 00014    Loss 0.080852  0.000000\n",
            "Iteration 00015    Loss 0.077594  0.000000\n",
            "Iteration 00016    Loss 0.077534  0.000000\n",
            "Iteration 00017    Loss 0.072678  0.000000\n",
            "Iteration 00018    Loss 0.070554  0.000000\n",
            "Iteration 00019    Loss 0.068757  0.000000\n",
            "Iteration 00020    Loss 0.067541  0.000000\n",
            "Iteration 00021    Loss 0.065611  0.000000\n",
            "Iteration 00022    Loss 0.063604  0.000000\n",
            "Iteration 00023    Loss 0.062575  0.000000\n",
            "Iteration 00024    Loss 0.060608  0.000000\n",
            "Iteration 00025    Loss 0.059203  0.000000\n",
            "Iteration 00026    Loss 0.057275  0.000000\n",
            "Iteration 00027    Loss 0.094796  0.000000\n",
            "Iteration 00028    Loss 0.054407  0.000000\n",
            "Iteration 00029    Loss 0.053534  0.000000\n",
            "Iteration 00030    Loss 0.052323  0.000000\n",
            "Iteration 00031    Loss 0.050924  0.000000\n",
            "Iteration 00032    Loss 0.049913  0.000000\n",
            "Iteration 00033    Loss 0.048763  0.000001\n",
            "Iteration 00034    Loss 0.047572  0.000001\n",
            "Iteration 00035    Loss 0.046330  0.000001\n",
            "Iteration 00036    Loss 0.045263  0.000001\n",
            "Iteration 00037    Loss 0.044006  0.000001\n",
            "Iteration 00038    Loss 0.043026  0.000001\n",
            "Iteration 00039    Loss 0.042061  0.000001\n",
            "Iteration 00040    Loss 0.040856  0.000001\n",
            "Iteration 00041    Loss 0.039841  0.000001\n",
            "Iteration 00042    Loss 0.038817  0.000001\n",
            "Iteration 00043    Loss 0.037606  0.000001\n",
            "Iteration 00044    Loss 0.036640  0.000001\n",
            "Iteration 00045    Loss 0.035505  0.000001\n",
            "Iteration 00046    Loss 0.034833  0.000001\n",
            "Iteration 00047    Loss 0.033637  0.000001\n",
            "Iteration 00048    Loss 0.032675  0.000001\n",
            "Iteration 00049    Loss 0.031596  0.000001\n",
            "Iteration 00050    Loss 0.030785  0.000001\n",
            "Iteration 00051    Loss 0.029817  0.000001\n",
            "Iteration 00052    Loss 0.029004  0.000001\n",
            "Iteration 00053    Loss 0.027936  0.000001\n",
            "Iteration 00054    Loss 0.027025  0.000001\n",
            "Iteration 00055    Loss 0.026268  0.000001\n",
            "Iteration 00056    Loss 0.025383  0.000002\n",
            "Iteration 00057    Loss 0.024455  0.000002\n",
            "Iteration 00058    Loss 0.023612  0.000002\n",
            "Iteration 00059    Loss 0.022878  0.000002\n",
            "Iteration 00060    Loss 0.022031  0.000002\n",
            "Iteration 00061    Loss 0.021221  0.000002\n",
            "Iteration 00062    Loss 0.020424  0.000002\n",
            "Iteration 00063    Loss 0.019615  0.000002\n",
            "Iteration 00064    Loss 0.018789  0.000002\n",
            "Iteration 00065    Loss 0.018140  0.000002\n",
            "Iteration 00066    Loss 0.017440  0.000002\n",
            "Iteration 00067    Loss 0.016579  0.000002\n",
            "Iteration 00068    Loss 0.015868  0.000002\n",
            "Iteration 00069    Loss 0.015288  0.000002\n",
            "Iteration 00070    Loss 0.014513  0.000002\n",
            "Iteration 00071    Loss 0.013890  0.000002\n",
            "Iteration 00072    Loss 0.013179  0.000003\n",
            "Iteration 00073    Loss 0.012421  0.000003\n",
            "Iteration 00074    Loss 0.011812  0.000003\n",
            "Iteration 00075    Loss 0.011093  0.000004\n",
            "Iteration 00076    Loss 0.010472  0.000003\n",
            "Iteration 00077    Loss 0.009706  0.000003\n",
            "Iteration 00078    Loss 0.009071  0.000004\n",
            "Iteration 00079    Loss 0.008467  0.000004\n",
            "Iteration 00080    Loss 0.007915  0.000004\n",
            "Iteration 00081    Loss 0.007275  0.000005\n",
            "Iteration 00082    Loss 0.006612  0.000005\n",
            "Iteration 00083    Loss 0.006086  0.000005\n",
            "Iteration 00084    Loss 0.005599  0.000005\n",
            "Iteration 00085    Loss 0.004890  0.000005\n",
            "Iteration 00086    Loss 0.004449  0.000006\n",
            "Iteration 00087    Loss 0.003814  0.000005\n",
            "Iteration 00088    Loss 0.003234  0.000006\n",
            "Iteration 00089    Loss 0.002725  0.000006\n",
            "Iteration 00090    Loss 0.002242  0.000006\n",
            "Iteration 00091    Loss 0.001718  0.000006\n",
            "Iteration 00092    Loss 0.001236  0.000006\n",
            "Iteration 00093    Loss 0.000684  0.000006\n",
            "Iteration 00094    Loss 0.000162  0.000006\n",
            "Iteration 00095    Loss -0.000312  0.000007\n",
            "Iteration 00096    Loss -0.000851  0.000006\n",
            "Iteration 00097    Loss -0.001294  0.000007\n",
            "Iteration 00098    Loss -0.001811  0.000007\n",
            "Iteration 00099    Loss -0.002230  0.000007\n",
            "Iteration 00100    Loss -0.002714  0.000008\n",
            "Iteration 00101    Loss -0.003121  0.000007\n",
            "Iteration 00102    Loss -0.003636  0.000008\n",
            "Iteration 00103    Loss -0.004084  0.000008\n",
            "Iteration 00104    Loss -0.004466  0.000009\n",
            "Iteration 00105    Loss -0.004910  0.000008\n",
            "Iteration 00106    Loss -0.005304  0.000007\n",
            "Iteration 00107    Loss -0.005561  0.000008\n",
            "Iteration 00108    Loss -0.006095  0.000007\n",
            "Iteration 00109    Loss -0.006547  0.000009\n",
            "Iteration 00110    Loss -0.006997  0.000008\n",
            "Iteration 00111    Loss -0.007318  0.000009\n",
            "Iteration 00112    Loss -0.007711  0.000009\n",
            "Iteration 00113    Loss -0.008099  0.000007\n",
            "Iteration 00114    Loss -0.008525  0.000009\n",
            "Iteration 00115    Loss -0.008874  0.000010\n",
            "Iteration 00116    Loss -0.009232  0.000008\n",
            "Iteration 00117    Loss -0.009600  0.000009\n",
            "Iteration 00118    Loss -0.009988  0.000010\n",
            "Iteration 00119    Loss -0.010335  0.000010\n",
            "Iteration 00120    Loss -0.010667  0.000010\n",
            "Iteration 00121    Loss -0.011025  0.000010\n",
            "Iteration 00122    Loss -0.011359  0.000011\n",
            "Iteration 00123    Loss -0.011698  0.000009\n",
            "Iteration 00124    Loss -0.012038  0.000010\n",
            "Iteration 00125    Loss -0.012309  0.000012\n",
            "Iteration 00126    Loss -0.012676  0.000011\n",
            "Iteration 00127    Loss -0.012968  0.000012\n",
            "Iteration 00128    Loss -0.013299  0.000012\n",
            "Iteration 00129    Loss -0.013630  0.000012\n",
            "Iteration 00130    Loss -0.013930  0.000012\n",
            "Iteration 00131    Loss -0.014218  0.000012\n",
            "Iteration 00132    Loss -0.014511  0.000014\n",
            "Iteration 00133    Loss -0.014782  0.000012\n",
            "Iteration 00134    Loss -0.015079  0.000013\n",
            "Iteration 00135    Loss -0.015319  0.000013\n",
            "Iteration 00136    Loss -0.015614  0.000014\n",
            "Iteration 00137    Loss -0.015950  0.000013\n",
            "Iteration 00138    Loss -0.016210  0.000015\n",
            "Iteration 00139    Loss -0.016450  0.000013\n",
            "Iteration 00140    Loss -0.016614  0.000016\n",
            "Iteration 00141    Loss -0.016889  0.000014\n",
            "Iteration 00142    Loss -0.017161  0.000016\n",
            "Iteration 00143    Loss -0.017359  0.000011\n",
            "Iteration 00144    Loss -0.017698  0.000016\n",
            "Iteration 00145    Loss -0.017913  0.000017\n",
            "Iteration 00146    Loss -0.018125  0.000015\n",
            "Iteration 00147    Loss -0.018355  0.000018\n",
            "Iteration 00148    Loss -0.018659  0.000016\n",
            "Iteration 00149    Loss -0.018906  0.000016\n",
            "Iteration 00150    Loss -0.019103  0.000017\n",
            "Iteration 00151    Loss -0.019422  0.000017\n",
            "Iteration 00152    Loss -0.019630  0.000018\n",
            "Iteration 00153    Loss -0.019823  0.000020\n",
            "Iteration 00154    Loss -0.020094  0.000016\n",
            "Iteration 00155    Loss -0.020223  0.000019\n",
            "Iteration 00156    Loss -0.020438  0.000018\n",
            "Iteration 00157    Loss -0.020648  0.000019\n",
            "Iteration 00158    Loss -0.020821  0.000016\n",
            "Iteration 00159    Loss -0.021081  0.000018\n",
            "Iteration 00160    Loss -0.021291  0.000019\n",
            "Iteration 00161    Loss -0.021478  0.000020\n",
            "Iteration 00162    Loss -0.021635  0.000021\n",
            "Iteration 00163    Loss -0.021978  0.000020\n",
            "Iteration 00164    Loss -0.022113  0.000022\n",
            "Iteration 00165    Loss -0.022279  0.000021\n",
            "Iteration 00166    Loss -0.022441  0.000023\n",
            "Iteration 00167    Loss -0.022608  0.000020\n",
            "Iteration 00168    Loss -0.022869  0.000023\n",
            "Iteration 00169    Loss -0.023059  0.000023\n",
            "Iteration 00170    Loss -0.023188  0.000020\n",
            "Iteration 00171    Loss -0.023362  0.000025\n",
            "Iteration 00172    Loss -0.023602  0.000021\n",
            "Iteration 00173    Loss -0.023813  0.000026\n",
            "Iteration 00174    Loss -0.023982  0.000025\n",
            "Iteration 00175    Loss -0.024014  0.000024\n",
            "Iteration 00176    Loss -0.024199  0.000027\n",
            "Iteration 00177    Loss -0.024401  0.000023\n",
            "Iteration 00178    Loss -0.024484  0.000026\n",
            "Iteration 00179    Loss -0.024670  0.000026\n",
            "Iteration 00180    Loss -0.024764  0.000020\n",
            "Iteration 00181    Loss -0.025039  0.000027\n",
            "Iteration 00182    Loss -0.025125  0.000022\n",
            "Iteration 00183    Loss -0.025309  0.000025\n",
            "Iteration 00184    Loss -0.025394  0.000028\n",
            "Iteration 00185    Loss -0.025594  0.000020\n",
            "Iteration 00186    Loss -0.025746  0.000023\n",
            "Iteration 00187    Loss -0.025870  0.000031\n",
            "Iteration 00188    Loss -0.025909  0.000023\n",
            "Iteration 00189    Loss -0.026158  0.000024\n",
            "Iteration 00190    Loss -0.026276  0.000027\n",
            "Iteration 00191    Loss -0.026474  0.000026\n",
            "Iteration 00192    Loss -0.026646  0.000026\n",
            "Iteration 00193    Loss -0.026855  0.000027\n",
            "Iteration 00194    Loss -0.026994  0.000026\n",
            "Iteration 00195    Loss -0.027110  0.000028\n",
            "Iteration 00196    Loss -0.027129  0.000030\n",
            "Iteration 00197    Loss -0.027263  0.000029\n",
            "Iteration 00198    Loss -0.027472  0.000029\n",
            "Iteration 00199    Loss -0.027461  0.000029\n",
            "Iteration 00200    Loss -0.027690  0.000030\n",
            "Iteration 00201    Loss -0.027869  0.000033\n",
            "Iteration 00202    Loss -0.027951  0.000029\n",
            "Iteration 00203    Loss -0.028154  0.000030\n",
            "Iteration 00204    Loss -0.028238  0.000033\n",
            "Iteration 00205    Loss -0.028347  0.000031\n",
            "Iteration 00206    Loss -0.028467  0.000032\n",
            "Iteration 00207    Loss -0.028607  0.000033\n",
            "Iteration 00208    Loss -0.028710  0.000033\n",
            "Iteration 00209    Loss -0.028801  0.000035\n",
            "Iteration 00210    Loss -0.028930  0.000032\n",
            "Iteration 00211    Loss -0.029104  0.000035\n",
            "Iteration 00212    Loss -0.029115  0.000033\n",
            "Iteration 00213    Loss -0.029343  0.000035\n",
            "Iteration 00214    Loss -0.029484  0.000036\n",
            "Iteration 00215    Loss -0.029416  0.000033\n",
            "Iteration 00216    Loss -0.029641  0.000037\n",
            "Iteration 00217    Loss -0.029699  0.000035\n",
            "Iteration 00218    Loss -0.029871  0.000035\n",
            "Iteration 00219    Loss -0.029955  0.000039\n",
            "Iteration 00220    Loss -0.030030  0.000035\n",
            "Iteration 00221    Loss -0.030071  0.000039\n",
            "Iteration 00222    Loss -0.030165  0.000039\n",
            "Iteration 00223    Loss -0.030278  0.000033\n",
            "Iteration 00224    Loss -0.030518  0.000038\n",
            "Iteration 00225    Loss -0.030520  0.000041\n",
            "Iteration 00226    Loss -0.030588  0.000036\n",
            "Iteration 00227    Loss -0.030746  0.000036\n",
            "Iteration 00228    Loss -0.030810  0.000039\n",
            "Iteration 00229    Loss -0.030956  0.000039\n",
            "Iteration 00230    Loss -0.030956  0.000038\n",
            "Iteration 00231    Loss -0.031010  0.000038\n",
            "Iteration 00232    Loss -0.031194  0.000040\n",
            "Iteration 00233    Loss -0.031388  0.000038\n",
            "Iteration 00234    Loss -0.031386  0.000040\n",
            "Iteration 00235    Loss -0.031425  0.000040\n",
            "Iteration 00236    Loss -0.031515  0.000036\n",
            "Iteration 00237    Loss -0.031615  0.000041\n",
            "Iteration 00238    Loss -0.031772  0.000043\n",
            "Iteration 00239    Loss -0.031836  0.000038\n",
            "Iteration 00240    Loss -0.031941  0.000041\n",
            "Iteration 00241    Loss -0.031963  0.000040\n",
            "Iteration 00242    Loss -0.032128  0.000040\n",
            "Iteration 00243    Loss -0.032161  0.000044\n",
            "Iteration 00244    Loss -0.032345  0.000039\n",
            "Iteration 00245    Loss -0.032453  0.000042\n",
            "Iteration 00246    Loss -0.032401  0.000045\n",
            "Iteration 00247    Loss -0.032629  0.000044\n",
            "Iteration 00248    Loss -0.032628  0.000045\n",
            "Iteration 00249    Loss -0.032675  0.000044\n",
            "Iteration 00250    Loss -0.032849  0.000045\n",
            "Iteration 00251    Loss -0.032864  0.000042\n",
            "Iteration 00252    Loss -0.032956  0.000045\n",
            "Iteration 00253    Loss -0.032959  0.000039\n",
            "Iteration 00254    Loss -0.033180  0.000047\n",
            "Iteration 00255    Loss -0.033211  0.000042\n",
            "Iteration 00256    Loss -0.033245  0.000042\n",
            "Iteration 00257    Loss -0.033359  0.000046\n",
            "Iteration 00258    Loss -0.033349  0.000041\n",
            "Iteration 00259    Loss -0.033417  0.000045\n",
            "Iteration 00260    Loss -0.033571  0.000046\n",
            "Iteration 00261    Loss -0.033668  0.000042\n",
            "Iteration 00262    Loss -0.033649  0.000051\n",
            "Iteration 00263    Loss -0.033856  0.000046\n",
            "Iteration 00264    Loss -0.033817  0.000045\n",
            "Iteration 00265    Loss -0.033781  0.000049\n",
            "Iteration 00266    Loss -0.034016  0.000046\n",
            "Iteration 00267    Loss -0.034068  0.000048\n",
            "Iteration 00268    Loss -0.034203  0.000050\n",
            "Iteration 00269    Loss -0.034158  0.000046\n",
            "Iteration 00270    Loss -0.034317  0.000048\n",
            "Iteration 00271    Loss -0.034335  0.000050\n",
            "Iteration 00272    Loss -0.034449  0.000049\n",
            "Iteration 00273    Loss -0.034543  0.000051\n",
            "Iteration 00274    Loss -0.034628  0.000052\n",
            "Iteration 00275    Loss -0.034710  0.000051\n",
            "Iteration 00276    Loss -0.034653  0.000051\n",
            "Iteration 00277    Loss -0.034702  0.000050\n",
            "Iteration 00278    Loss -0.034555  0.000049\n",
            "Iteration 00279    Loss -0.034729  0.000044\n",
            "Iteration 00280    Loss -0.034855  0.000055\n",
            "Iteration 00281    Loss -0.034842  0.000040\n",
            "Iteration 00282    Loss -0.034912  0.000037\n",
            "Iteration 00283    Loss -0.035060  0.000057\n",
            "Iteration 00284    Loss -0.035104  0.000039\n",
            "Iteration 00285    Loss -0.035166  0.000038\n",
            "Iteration 00286    Loss -0.035177  0.000044\n",
            "Iteration 00287    Loss -0.035202  0.000045\n",
            "Iteration 00288    Loss -0.035308  0.000040\n",
            "Iteration 00289    Loss -0.035354  0.000044\n",
            "Iteration 00290    Loss -0.035519  0.000046\n",
            "Iteration 00291    Loss -0.035537  0.000048\n",
            "Iteration 00292    Loss -0.035664  0.000048\n",
            "Iteration 00293    Loss -0.035712  0.000046\n",
            "Iteration 00294    Loss -0.035830  0.000046\n",
            "Iteration 00295    Loss -0.035783  0.000048\n",
            "Iteration 00296    Loss -0.035911  0.000050\n",
            "Iteration 00297    Loss -0.035918  0.000052\n",
            "Iteration 00298    Loss -0.036044  0.000052\n",
            "Iteration 00299    Loss -0.036143  0.000051\n",
            "Iteration 00300    Loss -0.036145  0.000050\n",
            "Iteration 00301    Loss -0.036181  0.000050\n",
            "Iteration 00302    Loss -0.036283  0.000052\n",
            "Iteration 00303    Loss -0.036265  0.000053\n",
            "Iteration 00304    Loss -0.036429  0.000052\n",
            "Iteration 00305    Loss -0.036422  0.000055\n",
            "Iteration 00306    Loss -0.036452  0.000058\n",
            "Iteration 00307    Loss -0.036572  0.000058\n",
            "Iteration 00308    Loss -0.036511  0.000056\n",
            "Iteration 00309    Loss -0.036585  0.000056\n",
            "Iteration 00310    Loss -0.036695  0.000058\n",
            "Iteration 00311    Loss -0.036735  0.000057\n",
            "Iteration 00312    Loss -0.036872  0.000059\n",
            "Iteration 00313    Loss -0.036837  0.000063\n",
            "Iteration 00314    Loss -0.036837  0.000059\n",
            "Iteration 00315    Loss -0.036952  0.000061\n",
            "Iteration 00316    Loss -0.036894  0.000058\n",
            "Iteration 00317    Loss -0.037060  0.000062\n",
            "Iteration 00318    Loss -0.037087  0.000061\n",
            "Iteration 00319    Loss -0.037093  0.000057\n",
            "Iteration 00320    Loss -0.037191  0.000062\n",
            "Iteration 00321    Loss -0.037133  0.000058\n",
            "Iteration 00322    Loss -0.037196  0.000059\n",
            "Iteration 00323    Loss -0.037254  0.000062\n",
            "Iteration 00324    Loss -0.037312  0.000060\n",
            "Iteration 00325    Loss -0.037385  0.000063\n",
            "Iteration 00326    Loss -0.037465  0.000066\n",
            "Iteration 00327    Loss -0.037465  0.000063\n",
            "Iteration 00328    Loss -0.037629  0.000064\n",
            "Iteration 00329    Loss -0.037596  0.000064\n",
            "Iteration 00330    Loss -0.037653  0.000064\n",
            "Iteration 00331    Loss -0.037670  0.000068\n",
            "Iteration 00332    Loss -0.037697  0.000067\n",
            "Iteration 00333    Loss -0.037823  0.000065\n",
            "Iteration 00334    Loss -0.037885  0.000069\n",
            "Iteration 00335    Loss -0.037932  0.000066\n",
            "Iteration 00336    Loss -0.037934  0.000067\n",
            "Iteration 00337    Loss -0.037882  0.000065\n",
            "Iteration 00338    Loss -0.037970  0.000069\n",
            "Iteration 00339    Loss -0.038039  0.000064\n",
            "Iteration 00340    Loss -0.038140  0.000067\n",
            "Iteration 00341    Loss -0.038156  0.000072\n",
            "Iteration 00342    Loss -0.038021  0.000064\n",
            "Iteration 00343    Loss -0.038233  0.000070\n",
            "Iteration 00344    Loss -0.038187  0.000067\n",
            "Iteration 00345    Loss -0.038275  0.000066\n",
            "Iteration 00346    Loss -0.038379  0.000070\n",
            "Iteration 00347    Loss -0.038131  0.000068\n",
            "Iteration 00348    Loss -0.038308  0.000066\n",
            "Iteration 00349    Loss -0.038381  0.000065\n",
            "Iteration 00350    Loss -0.038409  0.000071\n",
            "Iteration 00351    Loss -0.038573  0.000067\n",
            "Iteration 00352    Loss -0.038501  0.000070\n",
            "Iteration 00353    Loss -0.038482  0.000064\n",
            "Iteration 00354    Loss -0.038706  0.000073\n",
            "Iteration 00355    Loss -0.038588  0.000072\n",
            "Iteration 00356    Loss -0.038588  0.000066\n",
            "Iteration 00357    Loss -0.038753  0.000067\n",
            "Iteration 00358    Loss -0.038691  0.000068\n",
            "Iteration 00359    Loss -0.038818  0.000077\n",
            "Iteration 00360    Loss -0.038737  0.000068\n",
            "Iteration 00361    Loss -0.038791  0.000065\n",
            "Iteration 00362    Loss -0.038799  0.000072\n",
            "Iteration 00363    Loss -0.038832  0.000069\n",
            "Iteration 00364    Loss -0.038829  0.000068\n",
            "Iteration 00365    Loss -0.039034  0.000074\n",
            "Iteration 00366    Loss -0.039096  0.000065\n",
            "Iteration 00367    Loss -0.039086  0.000077\n",
            "Iteration 00368    Loss -0.039092  0.000074\n",
            "Iteration 00369    Loss -0.039201  0.000067\n",
            "Iteration 00370    Loss -0.039159  0.000075\n",
            "Iteration 00371    Loss -0.039293  0.000075\n",
            "Iteration 00372    Loss -0.039345  0.000074\n",
            "Iteration 00373    Loss -0.039316  0.000072\n",
            "Iteration 00374    Loss -0.039274  0.000075\n",
            "Iteration 00375    Loss -0.039358  0.000079\n",
            "Iteration 00376    Loss -0.039450  0.000071\n",
            "Iteration 00377    Loss -0.039417  0.000078\n",
            "Iteration 00378    Loss -0.039457  0.000076\n",
            "Iteration 00379    Loss -0.039551  0.000069\n",
            "Iteration 00380    Loss -0.039597  0.000080\n",
            "Iteration 00381    Loss -0.039582  0.000077\n",
            "Iteration 00382    Loss -0.039687  0.000080\n",
            "Iteration 00383    Loss -0.039629  0.000077\n",
            "Iteration 00384    Loss -0.039680  0.000082\n",
            "Iteration 00385    Loss -0.039510  0.000078\n",
            "Iteration 00386    Loss -0.039509  0.000079\n",
            "Iteration 00387    Loss -0.039660  0.000068\n",
            "Iteration 00388    Loss -0.039787  0.000066\n",
            "Iteration 00389    Loss -0.039781  0.000083\n",
            "Iteration 00390    Loss -0.039807  0.000064\n",
            "Iteration 00391    Loss -0.039748  0.000062\n",
            "Iteration 00392    Loss -0.039886  0.000074\n",
            "Iteration 00393    Loss -0.039932  0.000077\n",
            "Iteration 00394    Loss -0.039961  0.000069\n",
            "Iteration 00395    Loss -0.039984  0.000073\n",
            "Iteration 00396    Loss -0.039925  0.000080\n",
            "Iteration 00397    Loss -0.040082  0.000074\n",
            "Iteration 00398    Loss -0.040150  0.000077\n",
            "Iteration 00399    Loss -0.040117  0.000079\n",
            "Iteration 00400    Loss -0.040127  0.000077\n",
            "Iteration 00401    Loss -0.040205  0.000075\n",
            "Iteration 00402    Loss -0.040252  0.000076\n",
            "Iteration 00403    Loss -0.040255  0.000084\n",
            "Iteration 00404    Loss -0.040329  0.000078\n",
            "Iteration 00405    Loss -0.040343  0.000082\n",
            "Iteration 00406    Loss -0.040288  0.000084\n",
            "Iteration 00407    Loss -0.040354  0.000084\n",
            "Iteration 00408    Loss -0.040315  0.000083\n",
            "Iteration 00409    Loss -0.040460  0.000088\n",
            "Iteration 00410    Loss -0.040496  0.000084\n",
            "Iteration 00411    Loss -0.040458  0.000090\n",
            "Iteration 00412    Loss -0.040486  0.000081\n",
            "Iteration 00413    Loss -0.040526  0.000092\n",
            "Iteration 00414    Loss -0.040478  0.000082\n",
            "Iteration 00415    Loss -0.040412  0.000089\n",
            "Iteration 00416    Loss -0.040564  0.000081\n",
            "Iteration 00417    Loss -0.040502  0.000085\n",
            "Iteration 00418    Loss -0.040680  0.000087\n",
            "Iteration 00419    Loss -0.040597  0.000088\n",
            "Iteration 00420    Loss -0.040685  0.000090\n",
            "Iteration 00421    Loss -0.040764  0.000085\n",
            "Iteration 00422    Loss -0.040773  0.000088\n",
            "Iteration 00423    Loss -0.040809  0.000089\n",
            "Iteration 00424    Loss -0.040712  0.000087\n",
            "Iteration 00425    Loss -0.040684  0.000091\n",
            "Iteration 00426    Loss -0.040909  0.000091\n",
            "Iteration 00427    Loss -0.040763  0.000085\n",
            "Iteration 00428    Loss -0.040913  0.000092\n",
            "Iteration 00429    Loss -0.040911  0.000098\n",
            "Iteration 00430    Loss -0.040882  0.000077\n",
            "Iteration 00431    Loss -0.041017  0.000092\n",
            "Iteration 00432    Loss -0.041008  0.000098\n",
            "Iteration 00433    Loss -0.041040  0.000086\n",
            "Iteration 00434    Loss -0.041055  0.000098\n",
            "Iteration 00435    Loss -0.041046  0.000083\n",
            "Iteration 00436    Loss -0.041133  0.000088\n",
            "Iteration 00437    Loss -0.041113  0.000094\n",
            "Iteration 00438    Loss -0.041162  0.000095\n",
            "Iteration 00439    Loss -0.041133  0.000096\n",
            "Iteration 00440    Loss -0.041155  0.000086\n",
            "Iteration 00441    Loss -0.041243  0.000102\n",
            "Iteration 00442    Loss -0.041262  0.000091\n",
            "Iteration 00443    Loss -0.041282  0.000092\n",
            "Iteration 00444    Loss -0.041262  0.000106\n",
            "Iteration 00445    Loss -0.041328  0.000089\n",
            "Iteration 00446    Loss -0.041349  0.000091\n",
            "Iteration 00447    Loss -0.041293  0.000100\n",
            "Iteration 00448    Loss -0.041341  0.000097\n",
            "Iteration 00449    Loss -0.041453  0.000096\n",
            "Iteration 00450    Loss -0.041419  0.000099\n",
            "Iteration 00451    Loss -0.041478  0.000103\n",
            "Iteration 00452    Loss -0.041464  0.000096\n",
            "Iteration 00453    Loss -0.041506  0.000098\n",
            "Iteration 00454    Loss -0.041458  0.000112\n",
            "Iteration 00455    Loss -0.041507  0.000095\n",
            "Iteration 00456    Loss -0.041535  0.000103\n",
            "Iteration 00457    Loss -0.041561  0.000093\n",
            "Iteration 00458    Loss -0.041566  0.000102\n",
            "Iteration 00459    Loss -0.041673  0.000104\n",
            "Iteration 00460    Loss -0.041607  0.000096\n",
            "Iteration 00461    Loss -0.041700  0.000109\n",
            "Iteration 00462    Loss -0.041615  0.000102\n",
            "Iteration 00463    Loss -0.041726  0.000107\n",
            "Iteration 00464    Loss -0.041743  0.000106\n",
            "Iteration 00465    Loss -0.041798  0.000109\n",
            "Iteration 00466    Loss -0.041768  0.000108\n",
            "Iteration 00467    Loss -0.041836  0.000110\n",
            "Iteration 00468    Loss -0.041834  0.000106\n",
            "Iteration 00469    Loss -0.041861  0.000113\n",
            "Iteration 00470    Loss -0.041892  0.000106\n",
            "Iteration 00471    Loss -0.041839  0.000116\n",
            "Iteration 00472    Loss -0.041886  0.000113\n",
            "Iteration 00473    Loss -0.041847  0.000100\n",
            "Iteration 00474    Loss -0.041938  0.000115\n",
            "Iteration 00475    Loss -0.041935  0.000097\n",
            "Iteration 00476    Loss -0.041935  0.000116\n",
            "Iteration 00477    Loss -0.041893  0.000092\n",
            "Iteration 00478    Loss -0.041815  0.000110\n",
            "Iteration 00479    Loss -0.041901  0.000095\n",
            "Iteration 00480    Loss -0.042005  0.000099\n",
            "Iteration 00481    Loss -0.041856  0.000105\n",
            "Iteration 00482    Loss -0.041916  0.000100\n",
            "Iteration 00483    Loss -0.042000  0.000096\n",
            "Iteration 00484    Loss -0.041981  0.000097\n",
            "Iteration 00485    Loss -0.042063  0.000115\n",
            "Iteration 00486    Loss -0.042078  0.000097\n",
            "Iteration 00487    Loss -0.042104  0.000098\n",
            "Iteration 00488    Loss -0.042131  0.000109\n",
            "Iteration 00489    Loss -0.042163  0.000100\n",
            "Iteration 00490    Loss -0.042197  0.000101\n",
            "Iteration 00491    Loss -0.042106  0.000104\n",
            "Iteration 00492    Loss -0.042179  0.000104\n",
            "Iteration 00493    Loss -0.042187  0.000109\n",
            "Iteration 00494    Loss -0.042299  0.000113\n",
            "Iteration 00495    Loss -0.042264  0.000110\n",
            "Iteration 00496    Loss -0.042283  0.000111\n",
            "Iteration 00497    Loss -0.042352  0.000112\n",
            "Iteration 00498    Loss -0.042358  0.000110\n",
            "Iteration 00499    Loss -0.042384  0.000121\n",
            " 99% 66/67 [36:14<00:32, 32.41s/it]/content/ZID/REAL/cityscape_input.png\n",
            "Iteration 00000    Loss 0.254360  0.000075\n",
            "Iteration 00001    Loss 27.546570  0.000020\n",
            "Iteration 00002    Loss 0.644313  0.000005\n",
            "Iteration 00003    Loss 0.592338  0.000002\n",
            "Iteration 00004    Loss 0.963858  0.000001\n",
            "Iteration 00005    Loss 0.060623  0.000001\n",
            "Iteration 00006    Loss 0.390876  0.000001\n",
            "Iteration 00007    Loss 0.037308  0.000001\n",
            "Iteration 00008    Loss 0.188752  0.000001\n",
            "Iteration 00009    Loss 0.094703  0.000001\n",
            "Iteration 00010    Loss 0.053923  0.000000\n",
            "Iteration 00011    Loss 0.096421  0.000000\n",
            "Iteration 00012    Loss 0.041850  0.000000\n",
            "Iteration 00013    Loss 0.039429  0.000000\n",
            "Iteration 00014    Loss 0.061260  0.000000\n",
            "Iteration 00015    Loss 0.027118  0.000000\n",
            "Iteration 00016    Loss 0.026931  0.000000\n",
            "Iteration 00017    Loss 0.040553  0.000000\n",
            "Iteration 00018    Loss 0.028782  0.000000\n",
            "Iteration 00019    Loss 0.021306  0.000000\n",
            "Iteration 00020    Loss 0.031899  0.000000\n",
            "Iteration 00021    Loss 0.027383  0.000001\n",
            "Iteration 00022    Loss 0.017494  0.000001\n",
            "Iteration 00023    Loss 0.023631  0.000001\n",
            "Iteration 00024    Loss 0.023152  0.000001\n",
            "Iteration 00025    Loss 0.014149  0.000001\n",
            "Iteration 00026    Loss 0.018404  0.000001\n",
            "Iteration 00027    Loss 0.017630  0.000001\n",
            "Iteration 00028    Loss 0.011160  0.000001\n",
            "Iteration 00029    Loss 0.012306  0.000001\n",
            "Iteration 00030    Loss 0.012057  0.000001\n",
            "Iteration 00031    Loss 0.008049  0.000001\n",
            "Iteration 00032    Loss 0.007851  0.000001\n",
            "Iteration 00033    Loss 0.008518  0.000001\n",
            "Iteration 00034    Loss 0.005826  0.000001\n",
            "Iteration 00035    Loss 0.005145  0.000001\n",
            "Iteration 00036    Loss 0.005459  0.000001\n",
            "Iteration 00037    Loss 0.003697  0.000001\n",
            "Iteration 00038    Loss 0.002520  0.000002\n",
            "Iteration 00039    Loss 0.003059  0.000002\n",
            "Iteration 00040    Loss 0.001967  0.000002\n",
            "Iteration 00041    Loss 0.000686  0.000002\n",
            "Iteration 00042    Loss 0.000277  0.000002\n",
            "Iteration 00043    Loss 0.000007  0.000002\n",
            "Iteration 00044    Loss -0.000860  0.000002\n",
            "Iteration 00045    Loss -0.001341  0.000002\n",
            "Iteration 00046    Loss -0.001974  0.000002\n",
            "Iteration 00047    Loss -0.002483  0.000003\n",
            "Iteration 00048    Loss -0.002967  0.000003\n",
            "Iteration 00049    Loss -0.003644  0.000003\n",
            "Iteration 00050    Loss -0.003870  0.000003\n",
            "Iteration 00051    Loss -0.004498  0.000003\n",
            "Iteration 00052    Loss -0.005059  0.000003\n",
            "Iteration 00053    Loss -0.005642  0.000003\n",
            "Iteration 00054    Loss -0.006098  0.000003\n",
            "Iteration 00055    Loss -0.006465  0.000003\n",
            "Iteration 00056    Loss -0.006873  0.000003\n",
            "Iteration 00057    Loss -0.007332  0.000003\n",
            "Iteration 00058    Loss -0.007888  0.000003\n",
            "Iteration 00059    Loss -0.008172  0.000003\n",
            "Iteration 00060    Loss -0.008611  0.000003\n",
            "Iteration 00061    Loss -0.008545  0.000003\n",
            "Iteration 00062    Loss -0.009403  0.000003\n",
            "Iteration 00063    Loss -0.009604  0.000003\n",
            "Iteration 00064    Loss -0.010051  0.000003\n",
            "Iteration 00065    Loss -0.010706  0.000003\n",
            "Iteration 00066    Loss -0.011071  0.000003\n",
            "Iteration 00067    Loss -0.011316  0.000004\n",
            "Iteration 00068    Loss -0.011529  0.000004\n",
            "Iteration 00069    Loss -0.012225  0.000004\n",
            "Iteration 00070    Loss -0.012461  0.000004\n",
            "Iteration 00071    Loss -0.012774  0.000004\n",
            "Iteration 00072    Loss -0.013049  0.000004\n",
            "Iteration 00073    Loss -0.013401  0.000004\n",
            "Iteration 00074    Loss -0.013879  0.000005\n",
            "Iteration 00075    Loss -0.014150  0.000004\n",
            "Iteration 00076    Loss -0.014512  0.000004\n",
            "Iteration 00077    Loss -0.014833  0.000004\n",
            "Iteration 00078    Loss -0.014871  0.000005\n",
            "Iteration 00079    Loss -0.015515  0.000005\n",
            "Iteration 00080    Loss -0.015864  0.000005\n",
            "Iteration 00081    Loss -0.016146  0.000005\n",
            "Iteration 00082    Loss -0.016428  0.000005\n",
            "Iteration 00083    Loss -0.016795  0.000005\n",
            "Iteration 00084    Loss -0.017117  0.000005\n",
            "Iteration 00085    Loss -0.017365  0.000005\n",
            "Iteration 00086    Loss -0.017772  0.000005\n",
            "Iteration 00087    Loss -0.017912  0.000005\n",
            "Iteration 00088    Loss -0.018169  0.000006\n",
            "Iteration 00089    Loss -0.018065  0.000006\n",
            "Iteration 00090    Loss -0.018451  0.000006\n",
            "Iteration 00091    Loss -0.019064  0.000006\n",
            "Iteration 00092    Loss -0.019299  0.000007\n",
            "Iteration 00093    Loss -0.019614  0.000006\n",
            "Iteration 00094    Loss -0.019956  0.000006\n",
            "Iteration 00095    Loss -0.020149  0.000006\n",
            "Iteration 00096    Loss -0.020380  0.000007\n",
            "Iteration 00097    Loss -0.020745  0.000007\n",
            "Iteration 00098    Loss -0.020869  0.000007\n",
            "Iteration 00099    Loss -0.021178  0.000008\n",
            "Iteration 00100    Loss -0.021335  0.000008\n",
            "Iteration 00101    Loss -0.021650  0.000008\n",
            "Iteration 00102    Loss -0.021875  0.000008\n",
            "Iteration 00103    Loss -0.022237  0.000009\n",
            "Iteration 00104    Loss -0.022448  0.000008\n",
            "Iteration 00105    Loss -0.022519  0.000008\n",
            "Iteration 00106    Loss -0.022828  0.000009\n",
            "Iteration 00107    Loss -0.023091  0.000009\n",
            "Iteration 00108    Loss -0.023415  0.000009\n",
            "Iteration 00109    Loss -0.023520  0.000010\n",
            "Iteration 00110    Loss -0.023740  0.000010\n",
            "Iteration 00111    Loss -0.023881  0.000010\n",
            "Iteration 00112    Loss -0.023986  0.000011\n",
            "Iteration 00113    Loss -0.024372  0.000011\n",
            "Iteration 00114    Loss -0.024401  0.000011\n",
            "Iteration 00115    Loss -0.024783  0.000010\n",
            "Iteration 00116    Loss -0.024807  0.000012\n",
            "Iteration 00117    Loss -0.025165  0.000011\n",
            "Iteration 00118    Loss -0.025486  0.000012\n",
            "Iteration 00119    Loss -0.025525  0.000013\n",
            "Iteration 00120    Loss -0.025800  0.000010\n",
            "Iteration 00121    Loss -0.025903  0.000011\n",
            "Iteration 00122    Loss -0.026102  0.000013\n",
            "Iteration 00123    Loss -0.026427  0.000013\n",
            "Iteration 00124    Loss -0.026435  0.000012\n",
            "Iteration 00125    Loss -0.026608  0.000012\n",
            "Iteration 00126    Loss -0.026869  0.000012\n",
            "Iteration 00127    Loss -0.027109  0.000012\n",
            "Iteration 00128    Loss -0.027288  0.000014\n",
            "Iteration 00129    Loss -0.027549  0.000013\n",
            "Iteration 00130    Loss -0.027639  0.000013\n",
            "Iteration 00131    Loss -0.027900  0.000014\n",
            "Iteration 00132    Loss -0.027799  0.000015\n",
            "Iteration 00133    Loss -0.028287  0.000015\n",
            "Iteration 00134    Loss -0.028399  0.000014\n",
            "Iteration 00135    Loss -0.028563  0.000014\n",
            "Iteration 00136    Loss -0.028773  0.000016\n",
            "Iteration 00137    Loss -0.028741  0.000016\n",
            "Iteration 00138    Loss -0.028908  0.000016\n",
            "Iteration 00139    Loss -0.029067  0.000017\n",
            "Iteration 00140    Loss -0.029175  0.000016\n",
            "Iteration 00141    Loss -0.029501  0.000016\n",
            "Iteration 00142    Loss -0.029534  0.000017\n",
            "Iteration 00143    Loss -0.029801  0.000018\n",
            "Iteration 00144    Loss -0.029810  0.000017\n",
            "Iteration 00145    Loss -0.030166  0.000016\n",
            "Iteration 00146    Loss -0.030289  0.000016\n",
            "Iteration 00147    Loss -0.029579  0.000017\n",
            "Iteration 00148    Loss -0.030591  0.000017\n",
            "Iteration 00149    Loss -0.030523  0.000018\n",
            "Iteration 00150    Loss -0.030883  0.000017\n",
            "Iteration 00151    Loss -0.030921  0.000017\n",
            "Iteration 00152    Loss -0.031240  0.000018\n",
            "Iteration 00153    Loss -0.031388  0.000018\n",
            "Iteration 00154    Loss -0.031372  0.000017\n",
            "Iteration 00155    Loss -0.031507  0.000017\n",
            "Iteration 00156    Loss -0.031579  0.000019\n",
            "Iteration 00157    Loss -0.031764  0.000019\n",
            "Iteration 00158    Loss -0.031796  0.000018\n",
            "Iteration 00159    Loss -0.032171  0.000020\n",
            "Iteration 00160    Loss -0.032131  0.000020\n",
            "Iteration 00161    Loss -0.032446  0.000019\n",
            "Iteration 00162    Loss -0.032565  0.000019\n",
            "Iteration 00163    Loss -0.032791  0.000021\n",
            "Iteration 00164    Loss -0.032665  0.000021\n",
            "Iteration 00165    Loss -0.032857  0.000019\n",
            "Iteration 00166    Loss -0.032918  0.000018\n",
            "Iteration 00167    Loss -0.032796  0.000020\n",
            "Iteration 00168    Loss -0.033091  0.000020\n",
            "Iteration 00169    Loss -0.033517  0.000020\n",
            "Iteration 00170    Loss -0.033485  0.000020\n",
            "Iteration 00171    Loss -0.033714  0.000020\n",
            "Iteration 00172    Loss -0.033611  0.000021\n",
            "Iteration 00173    Loss -0.033899  0.000021\n",
            "Iteration 00174    Loss -0.034032  0.000020\n",
            "Iteration 00175    Loss -0.033824  0.000020\n",
            "Iteration 00176    Loss -0.034280  0.000019\n",
            "Iteration 00177    Loss -0.034417  0.000021\n",
            "Iteration 00178    Loss -0.034158  0.000022\n",
            "Iteration 00179    Loss -0.034503  0.000020\n",
            "Iteration 00180    Loss -0.034755  0.000019\n",
            "Iteration 00181    Loss -0.034474  0.000019\n",
            "Iteration 00182    Loss -0.034481  0.000019\n",
            "Iteration 00183    Loss -0.034734  0.000022\n",
            "Iteration 00184    Loss -0.035027  0.000024\n",
            "Iteration 00185    Loss -0.035121  0.000022\n",
            "Iteration 00186    Loss -0.035046  0.000020\n",
            "Iteration 00187    Loss -0.035269  0.000022\n",
            "Iteration 00188    Loss -0.034848  0.000019\n",
            "Iteration 00189    Loss -0.035373  0.000019\n",
            "Iteration 00190    Loss -0.035599  0.000020\n",
            "Iteration 00191    Loss -0.035674  0.000024\n",
            "Iteration 00192    Loss -0.035345  0.000024\n",
            "Iteration 00193    Loss -0.035686  0.000021\n",
            "Iteration 00194    Loss -0.035931  0.000021\n",
            "Iteration 00195    Loss -0.035728  0.000021\n",
            "Iteration 00196    Loss -0.035688  0.000021\n",
            "Iteration 00197    Loss -0.036329  0.000022\n",
            "Iteration 00198    Loss -0.036364  0.000021\n",
            "Iteration 00199    Loss -0.036036  0.000020\n",
            "Iteration 00200    Loss -0.036175  0.000020\n",
            "Iteration 00201    Loss -0.036274  0.000021\n",
            "Iteration 00202    Loss -0.036593  0.000023\n",
            "Iteration 00203    Loss -0.036860  0.000026\n",
            "Iteration 00204    Loss -0.036728  0.000025\n",
            "Iteration 00205    Loss -0.036611  0.000022\n",
            "Iteration 00206    Loss -0.036980  0.000021\n",
            "Iteration 00207    Loss -0.036955  0.000022\n",
            "Iteration 00208    Loss -0.036764  0.000025\n",
            "Iteration 00209    Loss -0.036999  0.000023\n",
            "Iteration 00210    Loss -0.037131  0.000024\n",
            "Iteration 00211    Loss -0.036771  0.000024\n",
            "Iteration 00212    Loss -0.037388  0.000024\n",
            "Iteration 00213    Loss -0.037229  0.000023\n",
            "Iteration 00214    Loss -0.036946  0.000023\n",
            "Iteration 00215    Loss -0.037881  0.000022\n",
            "Iteration 00216    Loss -0.037856  0.000023\n",
            "Iteration 00217    Loss -0.037989  0.000026\n",
            "Iteration 00218    Loss -0.037875  0.000025\n",
            "Iteration 00219    Loss -0.037553  0.000024\n",
            "Iteration 00220    Loss -0.038076  0.000023\n",
            "Iteration 00221    Loss -0.038178  0.000023\n",
            "Iteration 00222    Loss -0.038122  0.000024\n",
            "Iteration 00223    Loss -0.038311  0.000024\n",
            "Iteration 00224    Loss -0.038435  0.000023\n",
            "Iteration 00225    Loss -0.038277  0.000022\n",
            "Iteration 00226    Loss -0.038275  0.000023\n",
            "Iteration 00227    Loss -0.038497  0.000025\n",
            "Iteration 00228    Loss -0.038719  0.000028\n",
            "Iteration 00229    Loss -0.038775  0.000028\n",
            "Iteration 00230    Loss -0.038855  0.000028\n",
            "Iteration 00231    Loss -0.039011  0.000027\n",
            "Iteration 00232    Loss -0.039043  0.000025\n",
            "Iteration 00233    Loss -0.038947  0.000025\n",
            "Iteration 00234    Loss -0.039017  0.000026\n",
            "Iteration 00235    Loss -0.039224  0.000026\n",
            "Iteration 00236    Loss -0.039248  0.000027\n",
            "Iteration 00237    Loss -0.039261  0.000027\n",
            "Iteration 00238    Loss -0.039405  0.000026\n",
            "Iteration 00239    Loss -0.038775  0.000026\n",
            "Iteration 00240    Loss -0.039152  0.000026\n",
            "Iteration 00241    Loss -0.039460  0.000028\n",
            "Iteration 00242    Loss -0.039531  0.000029\n",
            "Iteration 00243    Loss -0.039240  0.000030\n",
            "Iteration 00244    Loss -0.039679  0.000028\n",
            "Iteration 00245    Loss -0.039543  0.000028\n",
            "Iteration 00246    Loss -0.039587  0.000027\n",
            "Iteration 00247    Loss -0.039636  0.000026\n",
            "Iteration 00248    Loss -0.040035  0.000030\n",
            "Iteration 00249    Loss -0.040074  0.000028\n",
            "Iteration 00250    Loss -0.039787  0.000025\n",
            "Iteration 00251    Loss -0.040085  0.000027\n",
            "Iteration 00252    Loss -0.040040  0.000028\n",
            "Iteration 00253    Loss -0.039972  0.000028\n",
            "Iteration 00254    Loss -0.040260  0.000030\n",
            "Iteration 00255    Loss -0.040213  0.000027\n",
            "Iteration 00256    Loss -0.039928  0.000027\n",
            "Iteration 00257    Loss -0.039899  0.000027\n",
            "Iteration 00258    Loss -0.040460  0.000029\n",
            "Iteration 00259    Loss -0.040349  0.000029\n",
            "Iteration 00260    Loss -0.040496  0.000028\n",
            "Iteration 00261    Loss -0.040049  0.000029\n",
            "Iteration 00262    Loss -0.040700  0.000030\n",
            "Iteration 00263    Loss -0.040721  0.000029\n",
            "Iteration 00264    Loss -0.040787  0.000029\n",
            "Iteration 00265    Loss -0.040698  0.000029\n",
            "Iteration 00266    Loss -0.040781  0.000032\n",
            "Iteration 00267    Loss -0.040667  0.000035\n",
            "Iteration 00268    Loss -0.040941  0.000032\n",
            "Iteration 00269    Loss -0.040856  0.000030\n",
            "Iteration 00270    Loss -0.041085  0.000030\n",
            "Iteration 00271    Loss -0.040986  0.000031\n",
            "Iteration 00272    Loss -0.040665  0.000032\n",
            "Iteration 00273    Loss -0.041074  0.000034\n",
            "Iteration 00274    Loss -0.041172  0.000031\n",
            "Iteration 00275    Loss -0.041260  0.000028\n",
            "Iteration 00276    Loss -0.041325  0.000029\n",
            "Iteration 00277    Loss -0.040966  0.000032\n",
            "Iteration 00278    Loss -0.041282  0.000032\n",
            "Iteration 00279    Loss -0.041299  0.000034\n",
            "Iteration 00280    Loss -0.041518  0.000036\n",
            "Iteration 00281    Loss -0.041383  0.000035\n",
            "Iteration 00282    Loss -0.041490  0.000032\n",
            "Iteration 00283    Loss -0.041565  0.000032\n",
            "Iteration 00284    Loss -0.041228  0.000034\n",
            "Iteration 00285    Loss -0.041657  0.000033\n",
            "Iteration 00286    Loss -0.041545  0.000034\n",
            "Iteration 00287    Loss -0.041779  0.000037\n",
            "Iteration 00288    Loss -0.041787  0.000037\n",
            "Iteration 00289    Loss -0.041764  0.000034\n",
            "Iteration 00290    Loss -0.041800  0.000033\n",
            "Iteration 00291    Loss -0.041788  0.000038\n",
            "Iteration 00292    Loss -0.041920  0.000038\n",
            "Iteration 00293    Loss -0.041773  0.000038\n",
            "Iteration 00294    Loss -0.041994  0.000032\n",
            "Iteration 00295    Loss -0.042052  0.000034\n",
            "Iteration 00296    Loss -0.042154  0.000040\n",
            "Iteration 00297    Loss -0.042082  0.000040\n",
            "Iteration 00298    Loss -0.042074  0.000036\n",
            "Iteration 00299    Loss -0.042213  0.000035\n",
            "Iteration 00300    Loss -0.042175  0.000036\n",
            "Iteration 00301    Loss -0.042356  0.000037\n",
            "Iteration 00302    Loss -0.042221  0.000038\n",
            "Iteration 00303    Loss -0.042387  0.000040\n",
            "Iteration 00304    Loss -0.042344  0.000041\n",
            "Iteration 00305    Loss -0.042424  0.000042\n",
            "Iteration 00306    Loss -0.042281  0.000038\n",
            "Iteration 00307    Loss -0.042459  0.000038\n",
            "Iteration 00308    Loss -0.042599  0.000041\n",
            "Iteration 00309    Loss -0.042521  0.000039\n",
            "Iteration 00310    Loss -0.042599  0.000040\n",
            "Iteration 00311    Loss -0.042595  0.000041\n",
            "Iteration 00312    Loss -0.042636  0.000040\n",
            "Iteration 00313    Loss -0.042583  0.000039\n",
            "Iteration 00314    Loss -0.042699  0.000041\n",
            "Iteration 00315    Loss -0.042819  0.000040\n",
            "Iteration 00316    Loss -0.042831  0.000040\n",
            "Iteration 00317    Loss -0.042548  0.000043\n",
            "Iteration 00318    Loss -0.042823  0.000039\n",
            "Iteration 00319    Loss -0.042888  0.000041\n",
            "Iteration 00320    Loss -0.042923  0.000045\n",
            "Iteration 00321    Loss -0.042904  0.000043\n",
            "Iteration 00322    Loss -0.042976  0.000042\n",
            "Iteration 00323    Loss -0.042885  0.000041\n",
            "Iteration 00324    Loss -0.042938  0.000042\n",
            "Iteration 00325    Loss -0.043094  0.000043\n",
            "Iteration 00326    Loss -0.043111  0.000039\n",
            "Iteration 00327    Loss -0.043077  0.000039\n",
            "Iteration 00328    Loss -0.043178  0.000043\n",
            "Iteration 00329    Loss -0.042929  0.000045\n",
            "Iteration 00330    Loss -0.043082  0.000044\n",
            "Iteration 00331    Loss -0.043245  0.000041\n",
            "Iteration 00332    Loss -0.043273  0.000041\n",
            "Iteration 00333    Loss -0.042965  0.000044\n",
            "Iteration 00334    Loss -0.043267  0.000045\n",
            "Iteration 00335    Loss -0.043295  0.000038\n",
            "Iteration 00336    Loss -0.043389  0.000041\n",
            "Iteration 00337    Loss -0.043348  0.000048\n",
            "Iteration 00338    Loss -0.043444  0.000043\n",
            "Iteration 00339    Loss -0.043410  0.000037\n",
            "Iteration 00340    Loss -0.043222  0.000035\n",
            "Iteration 00341    Loss -0.043389  0.000041\n",
            "Iteration 00342    Loss -0.043458  0.000052\n",
            "Iteration 00343    Loss -0.043511  0.000045\n",
            "Iteration 00344    Loss -0.043511  0.000038\n",
            "Iteration 00345    Loss -0.043148  0.000039\n",
            "Iteration 00346    Loss -0.043510  0.000039\n",
            "Iteration 00347    Loss -0.043554  0.000043\n",
            "Iteration 00348    Loss -0.043350  0.000042\n",
            "Iteration 00349    Loss -0.043706  0.000041\n",
            "Iteration 00350    Loss -0.043691  0.000041\n",
            "Iteration 00351    Loss -0.043717  0.000041\n",
            "Iteration 00352    Loss -0.043554  0.000043\n",
            "Iteration 00353    Loss -0.043607  0.000043\n",
            "Iteration 00354    Loss -0.043820  0.000044\n",
            "Iteration 00355    Loss -0.043815  0.000042\n",
            "Iteration 00356    Loss -0.043725  0.000042\n",
            "Iteration 00357    Loss -0.043734  0.000046\n",
            "Iteration 00358    Loss -0.043918  0.000043\n",
            "Iteration 00359    Loss -0.043706  0.000045\n",
            "Iteration 00360    Loss -0.043806  0.000046\n",
            "Iteration 00361    Loss -0.043813  0.000043\n",
            "Iteration 00362    Loss -0.044026  0.000041\n",
            "Iteration 00363    Loss -0.044063  0.000043\n",
            "Iteration 00364    Loss -0.044064  0.000045\n",
            "Iteration 00365    Loss -0.044076  0.000044\n",
            "Iteration 00366    Loss -0.044100  0.000045\n",
            "Iteration 00367    Loss -0.043974  0.000048\n",
            "Iteration 00368    Loss -0.043935  0.000047\n",
            "Iteration 00369    Loss -0.044141  0.000046\n",
            "Iteration 00370    Loss -0.043961  0.000045\n",
            "Iteration 00371    Loss -0.044003  0.000047\n",
            "Iteration 00372    Loss -0.044155  0.000050\n",
            "Iteration 00373    Loss -0.044049  0.000046\n",
            "Iteration 00374    Loss -0.044122  0.000045\n",
            "Iteration 00375    Loss -0.044277  0.000048\n",
            "Iteration 00376    Loss -0.044053  0.000048\n",
            "Iteration 00377    Loss -0.044221  0.000045\n",
            "Iteration 00378    Loss -0.044283  0.000044\n",
            "Iteration 00379    Loss -0.044278  0.000047\n",
            "Iteration 00380    Loss -0.043982  0.000050\n",
            "Iteration 00381    Loss -0.044312  0.000047\n",
            "Iteration 00382    Loss -0.044394  0.000045\n",
            "Iteration 00383    Loss -0.044430  0.000044\n",
            "Iteration 00384    Loss -0.044135  0.000044\n",
            "Iteration 00385    Loss -0.043839  0.000046\n",
            "Iteration 00386    Loss -0.044434  0.000054\n",
            "Iteration 00387    Loss -0.044460  0.000045\n",
            "Iteration 00388    Loss -0.044417  0.000042\n",
            "Iteration 00389    Loss -0.044323  0.000044\n",
            "Iteration 00390    Loss -0.044349  0.000046\n",
            "Iteration 00391    Loss -0.044410  0.000043\n",
            "Iteration 00392    Loss -0.044598  0.000038\n",
            "Iteration 00393    Loss -0.044574  0.000038\n",
            "Iteration 00394    Loss -0.044455  0.000046\n",
            "Iteration 00395    Loss -0.044533  0.000048\n",
            "Iteration 00396    Loss -0.044668  0.000045\n",
            "Iteration 00397    Loss -0.044677  0.000043\n",
            "Iteration 00398    Loss -0.044729  0.000041\n",
            "Iteration 00399    Loss -0.044486  0.000041\n",
            "Iteration 00400    Loss -0.044747  0.000043\n",
            "Iteration 00401    Loss -0.044786  0.000045\n",
            "Iteration 00402    Loss -0.044821  0.000046\n",
            "Iteration 00403    Loss -0.044692  0.000049\n",
            "Iteration 00404    Loss -0.044627  0.000052\n",
            "Iteration 00405    Loss -0.044769  0.000050\n",
            "Iteration 00406    Loss -0.044843  0.000048\n",
            "Iteration 00407    Loss -0.044739  0.000046\n",
            "Iteration 00408    Loss -0.044670  0.000046\n",
            "Iteration 00409    Loss -0.044870  0.000047\n",
            "Iteration 00410    Loss -0.044951  0.000049\n",
            "Iteration 00411    Loss -0.044987  0.000048\n",
            "Iteration 00412    Loss -0.044975  0.000047\n",
            "Iteration 00413    Loss -0.044994  0.000049\n",
            "Iteration 00414    Loss -0.045016  0.000054\n",
            "Iteration 00415    Loss -0.044888  0.000052\n",
            "Iteration 00416    Loss -0.045044  0.000048\n",
            "Iteration 00417    Loss -0.045078  0.000049\n",
            "Iteration 00418    Loss -0.045074  0.000050\n",
            "Iteration 00419    Loss -0.044939  0.000053\n",
            "Iteration 00420    Loss -0.044976  0.000055\n",
            "Iteration 00421    Loss -0.044926  0.000055\n",
            "Iteration 00422    Loss -0.045165  0.000055\n",
            "Iteration 00423    Loss -0.045108  0.000051\n",
            "Iteration 00424    Loss -0.044926  0.000049\n",
            "Iteration 00425    Loss -0.045204  0.000054\n",
            "Iteration 00426    Loss -0.045090  0.000053\n",
            "Iteration 00427    Loss -0.045224  0.000052\n",
            "Iteration 00428    Loss -0.045196  0.000051\n",
            "Iteration 00429    Loss -0.045247  0.000053\n",
            "Iteration 00430    Loss -0.045287  0.000054\n",
            "Iteration 00431    Loss -0.045321  0.000052\n",
            "Iteration 00432    Loss -0.045242  0.000051\n",
            "Iteration 00433    Loss -0.045094  0.000053\n",
            "Iteration 00434    Loss -0.045326  0.000054\n",
            "Iteration 00435    Loss -0.045266  0.000054\n",
            "Iteration 00436    Loss -0.045381  0.000052\n",
            "Iteration 00437    Loss -0.045371  0.000056\n",
            "Iteration 00438    Loss -0.045415  0.000058\n",
            "Iteration 00439    Loss -0.045261  0.000055\n",
            "Iteration 00440    Loss -0.045146  0.000056\n",
            "Iteration 00441    Loss -0.045450  0.000055\n",
            "Iteration 00442    Loss -0.045418  0.000056\n",
            "Iteration 00443    Loss -0.045468  0.000057\n",
            "Iteration 00444    Loss -0.045416  0.000056\n",
            "Iteration 00445    Loss -0.045375  0.000053\n",
            "Iteration 00446    Loss -0.045502  0.000051\n",
            "Iteration 00447    Loss -0.045494  0.000055\n",
            "Iteration 00448    Loss -0.045385  0.000058\n",
            "Iteration 00449    Loss -0.045394  0.000058\n",
            "Iteration 00450    Loss -0.045559  0.000048\n",
            "Iteration 00451    Loss -0.045557  0.000053\n",
            "Iteration 00452    Loss -0.045494  0.000058\n",
            "Iteration 00453    Loss -0.045475  0.000053\n",
            "Iteration 00454    Loss -0.045178  0.000050\n",
            "Iteration 00455    Loss -0.045516  0.000054\n",
            "Iteration 00456    Loss -0.045486  0.000053\n",
            "Iteration 00457    Loss -0.045551  0.000053\n",
            "Iteration 00458    Loss -0.045434  0.000058\n",
            "Iteration 00459    Loss -0.045613  0.000064\n",
            "Iteration 00460    Loss -0.045615  0.000049\n",
            "Iteration 00461    Loss -0.045574  0.000047\n",
            "Iteration 00462    Loss -0.045629  0.000053\n",
            "Iteration 00463    Loss -0.045578  0.000055\n",
            "Iteration 00464    Loss -0.045645  0.000049\n",
            "Iteration 00465    Loss -0.045721  0.000045\n",
            "Iteration 00466    Loss -0.045642  0.000048\n",
            "Iteration 00467    Loss -0.045694  0.000057\n",
            "Iteration 00468    Loss -0.045757  0.000053\n",
            "Iteration 00469    Loss -0.045806  0.000053\n",
            "Iteration 00470    Loss -0.045828  0.000056\n",
            "Iteration 00471    Loss -0.045733  0.000058\n",
            "Iteration 00472    Loss -0.045864  0.000056\n",
            "Iteration 00473    Loss -0.045729  0.000055\n",
            "Iteration 00474    Loss -0.045827  0.000056\n",
            "Iteration 00475    Loss -0.045854  0.000053\n",
            "Iteration 00476    Loss -0.045853  0.000054\n",
            "Iteration 00477    Loss -0.045858  0.000058\n",
            "Iteration 00478    Loss -0.045897  0.000056\n",
            "Iteration 00479    Loss -0.045918  0.000052\n",
            "Iteration 00480    Loss -0.045972  0.000054\n",
            "Iteration 00481    Loss -0.045952  0.000057\n",
            "Iteration 00482    Loss -0.045882  0.000056\n",
            "Iteration 00483    Loss -0.045872  0.000056\n",
            "Iteration 00484    Loss -0.045930  0.000059\n",
            "Iteration 00485    Loss -0.045888  0.000063\n",
            "Iteration 00486    Loss -0.045972  0.000057\n",
            "Iteration 00487    Loss -0.045950  0.000056\n",
            "Iteration 00488    Loss -0.045912  0.000058\n",
            "Iteration 00489    Loss -0.045912  0.000058\n",
            "Iteration 00490    Loss -0.045933  0.000057\n",
            "Iteration 00491    Loss -0.046001  0.000058\n",
            "Iteration 00492    Loss -0.046031  0.000059\n",
            "Iteration 00493    Loss -0.045826  0.000058\n",
            "Iteration 00494    Loss -0.046039  0.000059\n",
            "Iteration 00495    Loss -0.045990  0.000061\n",
            "Iteration 00496    Loss -0.046077  0.000064\n",
            "Iteration 00497    Loss -0.045681  0.000063\n",
            "Iteration 00498    Loss -0.045871  0.000061\n",
            "Iteration 00499    Loss -0.046118  0.000056\n",
            "100% 67/67 [36:48<00:00, 32.97s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r real_processed.zip out/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-ljZJv90X8Z",
        "outputId": "1e194cdd-df24-4f82-f701-8851036f8c88"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: out/ (stored 0%)\n",
            "  adding: out/buildings_input.jpg (deflated 0%)\n",
            "  adding: out/23.jpg (deflated 1%)\n",
            "  adding: out/5.jpg (deflated 1%)\n",
            "  adding: out/29.jpg (deflated 2%)\n",
            "  adding: out/hongkong_input.jpg (deflated 0%)\n",
            "  adding: out/stadium_input.jpg (deflated 0%)\n",
            "  adding: out/12.jpg (deflated 0%)\n",
            "  adding: out/hill.jpg (deflated 1%)\n",
            "  adding: out/night_input.jpg (deflated 0%)\n",
            "  adding: out/6.jpg (deflated 1%)\n",
            "  adding: out/forest_input.jpg (deflated 0%)\n",
            "  adding: out/26.jpg (deflated 1%)\n",
            "  adding: out/ny12_input.jpg (deflated 0%)\n",
            "  adding: out/urbino_input.jpg (deflated 1%)\n",
            "  adding: out/17.jpg (deflated 0%)\n",
            "  adding: out/28.jpg (deflated 1%)\n",
            "  adding: out/pumpkins_input.jpg (deflated 0%)\n",
            "  adding: out/1.jpg (deflated 1%)\n",
            "  adding: out/19.jpg (deflated 2%)\n",
            "  adding: out/swan_input.jpg (deflated 0%)\n",
            "  adding: out/aerial_input.jpg (deflated 0%)\n",
            "  adding: out/y1_input.jpg (deflated 0%)\n",
            "  adding: out/3.jpg (deflated 0%)\n",
            "  adding: out/florence_input.jpg (deflated 0%)\n",
            "  adding: out/train_input.jpg (deflated 1%)\n",
            "  adding: out/test_image1.jpg (deflated 0%)\n",
            "  adding: out/house_input.jpg (deflated 0%)\n",
            "  adding: out/31.jpg (deflated 1%)\n",
            "  adding: out/16.jpg (deflated 1%)\n",
            "  adding: out/snow_input.jpg (deflated 0%)\n",
            "  adding: out/tree_input.jpg (deflated 1%)\n",
            "  adding: out/11.jpg (deflated 3%)\n",
            "  adding: out/15.jpg (deflated 2%)\n",
            "  adding: out/lviv_input.jpg (deflated 0%)\n",
            "  adding: out/27.jpg (deflated 1%)\n",
            "  adding: out/seoul.jpg (deflated 0%)\n",
            "  adding: out/mountain_input.jpg (deflated 0%)\n",
            "  adding: out/30.jpg (deflated 1%)\n",
            "  adding: out/24.jpg (deflated 0%)\n",
            "  adding: out/tiananmen_input.jpg (deflated 0%)\n",
            "  adding: out/flags_input.jpg (deflated 0%)\n",
            "  adding: out/22.jpg (deflated 0%)\n",
            "  adding: out/14.jpg (deflated 1%)\n",
            "  adding: out/18.jpg (deflated 2%)\n",
            "  adding: out/cones_input.jpg (deflated 0%)\n",
            "  adding: out/schechner_input.jpg (deflated 1%)\n",
            "  adding: out/32.jpg (deflated 1%)\n",
            "  adding: out/13.jpg (deflated 1%)\n",
            "  adding: out/cityscape_input.jpg (deflated 0%)\n",
            "  adding: out/road_input.jpg (deflated 0%)\n",
            "  adding: out/canon_input.jpg (deflated 0%)\n",
            "  adding: out/dubai_input.jpg (deflated 0%)\n",
            "  adding: out/towns.jpg (deflated 0%)\n",
            "  adding: out/cliff_input.jpg (deflated 1%)\n",
            "  adding: out/10.jpg (deflated 2%)\n",
            "  adding: out/9.jpg (deflated 0%)\n",
            "  adding: out/8.jpg (deflated 0%)\n",
            "  adding: out/33.jpg (deflated 1%)\n",
            "  adding: out/21.jpg (deflated 2%)\n",
            "  adding: out/herzeliya_input.jpg (deflated 0%)\n",
            "  adding: out/25.jpg (deflated 0%)\n",
            "  adding: out/20.jpg (deflated 2%)\n",
            "  adding: out/ny17_input.jpg (deflated 0%)\n",
            "  adding: out/castle_input.jpg (deflated 0%)\n",
            "  adding: out/7.jpg (deflated 1%)\n",
            "  adding: out/y16_input.jpg (deflated 0%)\n",
            "  adding: out/4.jpg (deflated 2%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "i4ifMhZc3ddf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nc3SK6kZ-PJH"
      },
      "source": [
        "# Process indoor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jv-2bEW6Sd3h",
        "outputId": "71642d81-2a03-4a13-eba2-d9f0d0ce736c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "YywTv03F_Glg",
        "outputId": "bc55e6a9-ced8-4e77-8f52-32ffd99dfc67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Iteration 00010    Loss 0.113448  0.000000\n",
            "Iteration 00011    Loss 0.103762  0.000000\n",
            "Iteration 00012    Loss 0.098580  0.000000\n",
            "Iteration 00013    Loss 0.092847  0.000000\n",
            "Iteration 00014    Loss 0.091389  0.000000\n",
            "Iteration 00015    Loss 0.086058  0.000000\n",
            "Iteration 00016    Loss 0.084756  0.000000\n",
            "Iteration 00017    Loss 0.082606  0.000000\n",
            "Iteration 00018    Loss 0.081565  0.000000\n",
            "Iteration 00019    Loss 0.079752  0.000000\n",
            "Iteration 00020    Loss 0.078433  0.000000\n",
            "Iteration 00021    Loss 0.076870  0.000000\n",
            "Iteration 00022    Loss 0.076332  0.000000\n",
            "Iteration 00023    Loss 0.074677  0.000000\n",
            "Iteration 00024    Loss 0.074283  0.000000\n",
            "Iteration 00025    Loss 0.073911  0.000000\n",
            "Iteration 00026    Loss 0.072200  0.000000\n",
            "Iteration 00027    Loss 0.070692  0.000000\n",
            "Iteration 00028    Loss 0.069172  0.000000\n",
            "Iteration 00029    Loss 0.068172  0.000000\n",
            "Iteration 00030    Loss 0.067348  0.000000\n",
            "Iteration 00031    Loss 0.066425  0.000000\n",
            "Iteration 00032    Loss 0.065574  0.000001\n",
            "Iteration 00033    Loss 0.064396  0.000001\n",
            "Iteration 00034    Loss 0.063463  0.000001\n",
            "Iteration 00035    Loss 0.062486  0.000001\n",
            "Iteration 00036    Loss 0.061721  0.000001\n",
            "Iteration 00037    Loss 0.060575  0.000001\n",
            "Iteration 00038    Loss 0.059869  0.000001\n",
            "Iteration 00039    Loss 0.058642  0.000001\n",
            "Iteration 00040    Loss 0.057866  0.000001\n",
            "Iteration 00041    Loss 0.056776  0.000001\n",
            "Iteration 00042    Loss 0.056425  0.000001\n",
            "Iteration 00043    Loss 0.055333  0.000001\n",
            "Iteration 00044    Loss 0.054512  0.000001\n",
            "Iteration 00045    Loss 0.053374  0.000001\n",
            "Iteration 00046    Loss 0.052057  0.000001\n",
            "Iteration 00047    Loss 0.050864  0.000001\n",
            "Iteration 00048    Loss 0.050800  0.000001\n",
            "Iteration 00049    Loss 0.049469  0.000001\n",
            "Iteration 00050    Loss 0.049185  0.000001\n",
            "Iteration 00051    Loss 0.048415  0.000001\n",
            "Iteration 00052    Loss 0.047476  0.000001\n",
            "Iteration 00053    Loss 0.046385  0.000001\n",
            "Iteration 00054    Loss 0.045482  0.000001\n",
            "Iteration 00055    Loss 0.044638  0.000001\n",
            "Iteration 00056    Loss 0.043773  0.000001\n",
            "Iteration 00057    Loss 0.042689  0.000002\n",
            "Iteration 00058    Loss 0.042406  0.000002\n",
            "Iteration 00059    Loss 0.041421  0.000002\n",
            "Iteration 00060    Loss 0.041056  0.000002\n",
            "Iteration 00061    Loss 0.040340  0.000002\n",
            "Iteration 00062    Loss 0.039180  0.000002\n",
            "Iteration 00063    Loss 0.038837  0.000002\n",
            "Iteration 00064    Loss 0.038420  0.000002\n",
            "Iteration 00065    Loss 0.037056  0.000002\n",
            "Iteration 00066    Loss 0.036679  0.000002\n",
            "Iteration 00067    Loss 0.035573  0.000002\n",
            "Iteration 00068    Loss 0.034855  0.000002\n",
            "Iteration 00069    Loss 0.034360  0.000002\n",
            "Iteration 00070    Loss 0.033760  0.000002\n",
            "Iteration 00071    Loss 0.033410  0.000002\n",
            "Iteration 00072    Loss 0.032389  0.000002\n",
            "Iteration 00073    Loss 0.032012  0.000002\n",
            "Iteration 00074    Loss 0.031634  0.000003\n",
            "Iteration 00075    Loss 0.030346  0.000003\n",
            "Iteration 00076    Loss 0.030458  0.000003\n",
            "Iteration 00077    Loss 0.029331  0.000003\n",
            "Iteration 00078    Loss 0.028752  0.000003\n",
            "Iteration 00079    Loss 0.028629  0.000003\n",
            "Iteration 00080    Loss 0.027490  0.000003\n",
            "Iteration 00081    Loss 0.027180  0.000003\n",
            "Iteration 00082    Loss 0.026155  0.000003\n",
            "Iteration 00083    Loss 0.026023  0.000003\n",
            "Iteration 00084    Loss 0.025112  0.000003\n",
            "Iteration 00085    Loss 0.024692  0.000003\n",
            "Iteration 00086    Loss 0.024149  0.000003\n",
            "Iteration 00087    Loss 0.023435  0.000004\n",
            "Iteration 00088    Loss 0.023139  0.000004\n",
            "Iteration 00089    Loss 0.022472  0.000004\n",
            "Iteration 00090    Loss 0.022087  0.000004\n",
            "Iteration 00091    Loss 0.021909  0.000004\n",
            "Iteration 00092    Loss 0.021315  0.000004\n",
            "Iteration 00093    Loss 0.020779  0.000004\n",
            "Iteration 00094    Loss 0.019965  0.000005\n",
            "Iteration 00095    Loss 0.019676  0.000005\n",
            "Iteration 00096    Loss 0.019651  0.000005\n",
            "Iteration 00097    Loss 0.018423  0.000005\n",
            "Iteration 00098    Loss 0.017757  0.000005\n",
            "Iteration 00099    Loss 0.017655  0.000005\n",
            "Iteration 00100    Loss 0.017267  0.000005\n",
            "Iteration 00101    Loss 0.017338  0.000005\n",
            "Iteration 00102    Loss 0.016390  0.000005\n",
            "Iteration 00103    Loss 0.016133  0.000006\n",
            "Iteration 00104    Loss 0.015107  0.000006\n",
            "Iteration 00105    Loss 0.014672  0.000007\n",
            "Iteration 00106    Loss 0.014183  0.000007\n",
            "Iteration 00107    Loss 0.014006  0.000006\n",
            "Iteration 00108    Loss 0.013446  0.000006\n",
            "Iteration 00109    Loss 0.012784  0.000006\n",
            "Iteration 00110    Loss 0.012277  0.000006\n",
            "Iteration 00111    Loss 0.012016  0.000007\n",
            "Iteration 00112    Loss 0.011681  0.000007\n",
            "Iteration 00113    Loss 0.011347  0.000006\n",
            "Iteration 00114    Loss 0.010845  0.000006\n",
            "Iteration 00115    Loss 0.010546  0.000007\n",
            "Iteration 00116    Loss 0.009960  0.000007\n",
            "Iteration 00117    Loss 0.009357  0.000007\n",
            "Iteration 00118    Loss 0.008774  0.000008\n",
            "Iteration 00119    Loss 0.008669  0.000008\n",
            "Iteration 00120    Loss 0.008226  0.000008\n",
            "Iteration 00121    Loss 0.007913  0.000008\n",
            "Iteration 00122    Loss 0.007639  0.000008\n",
            "Iteration 00123    Loss 0.007210  0.000008\n",
            "Iteration 00124    Loss 0.006540  0.000008\n",
            "Iteration 00125    Loss 0.006309  0.000009\n",
            "Iteration 00126    Loss 0.006238  0.000009\n",
            "Iteration 00127    Loss 0.005625  0.000009\n",
            "Iteration 00128    Loss 0.005236  0.000009\n",
            "Iteration 00129    Loss 0.004607  0.000010\n",
            "Iteration 00130    Loss 0.004346  0.000010\n",
            "Iteration 00131    Loss 0.004068  0.000010\n",
            "Iteration 00132    Loss 0.003451  0.000010\n",
            "Iteration 00133    Loss 0.003140  0.000010\n",
            "Iteration 00134    Loss 0.003410  0.000010\n",
            "Iteration 00135    Loss 0.002539  0.000010\n",
            "Iteration 00136    Loss 0.002032  0.000011\n",
            "Iteration 00137    Loss 0.002052  0.000011\n",
            "Iteration 00138    Loss 0.001282  0.000010\n",
            "Iteration 00139    Loss 0.001147  0.000010\n",
            "Iteration 00140    Loss 0.000767  0.000011\n",
            "Iteration 00141    Loss 0.000877  0.000012\n",
            "Iteration 00142    Loss 0.000185  0.000012\n",
            "Iteration 00143    Loss -0.000004  0.000012\n",
            "Iteration 00144    Loss -0.000763  0.000013\n",
            "Iteration 00145    Loss -0.001053  0.000014\n",
            "Iteration 00146    Loss -0.001431  0.000013\n",
            "Iteration 00147    Loss -0.001713  0.000013\n",
            "Iteration 00148    Loss -0.001634  0.000013\n",
            "Iteration 00149    Loss -0.002354  0.000014\n",
            "Iteration 00150    Loss -0.002574  0.000014\n",
            "Iteration 00151    Loss -0.002438  0.000014\n",
            "Iteration 00152    Loss -0.003203  0.000014\n",
            "Iteration 00153    Loss -0.003327  0.000014\n",
            "Iteration 00154    Loss -0.003641  0.000015\n",
            "Iteration 00155    Loss -0.003832  0.000016\n",
            "Iteration 00156    Loss -0.004437  0.000015\n",
            "Iteration 00157    Loss -0.004949  0.000015\n",
            "Iteration 00158    Loss -0.005167  0.000016\n",
            "Iteration 00159    Loss -0.004854  0.000016\n",
            "Iteration 00160    Loss -0.005633  0.000015\n",
            "Iteration 00161    Loss -0.005883  0.000016\n",
            "Iteration 00162    Loss -0.006033  0.000017\n",
            "Iteration 00163    Loss -0.006611  0.000016\n",
            "Iteration 00164    Loss -0.006710  0.000016\n",
            "Iteration 00165    Loss -0.006712  0.000018\n",
            "Iteration 00166    Loss -0.007325  0.000019\n",
            "Iteration 00167    Loss -0.007621  0.000018\n",
            "Iteration 00168    Loss -0.007781  0.000017\n",
            "Iteration 00169    Loss -0.008147  0.000017\n",
            "Iteration 00170    Loss -0.008315  0.000018\n",
            "Iteration 00171    Loss -0.008667  0.000019\n",
            "Iteration 00172    Loss -0.008347  0.000018\n",
            "Iteration 00173    Loss -0.009272  0.000020\n",
            "Iteration 00174    Loss -0.009468  0.000021\n",
            "Iteration 00175    Loss -0.009682  0.000019\n",
            "Iteration 00176    Loss -0.009144  0.000018\n",
            "Iteration 00177    Loss -0.010261  0.000020\n",
            "Iteration 00178    Loss -0.010486  0.000022\n",
            "Iteration 00179    Loss -0.010746  0.000021\n",
            "Iteration 00180    Loss -0.010699  0.000020\n",
            "Iteration 00181    Loss -0.011058  0.000019\n",
            "Iteration 00182    Loss -0.011141  0.000022\n",
            "Iteration 00183    Loss -0.011703  0.000022\n",
            "Iteration 00184    Loss -0.011691  0.000021\n",
            "Iteration 00185    Loss -0.012158  0.000021\n",
            "Iteration 00186    Loss -0.012234  0.000023\n",
            "Iteration 00187    Loss -0.012766  0.000024\n",
            "Iteration 00188    Loss -0.012742  0.000023\n",
            "Iteration 00189    Loss -0.013170  0.000023\n",
            "Iteration 00190    Loss -0.013403  0.000022\n",
            "Iteration 00191    Loss -0.013608  0.000023\n",
            "Iteration 00192    Loss -0.013674  0.000025\n",
            "Iteration 00193    Loss -0.014082  0.000026\n",
            "Iteration 00194    Loss -0.014352  0.000024\n",
            "Iteration 00195    Loss -0.014558  0.000025\n",
            "Iteration 00196    Loss -0.014482  0.000026\n",
            "Iteration 00197    Loss -0.014749  0.000025\n",
            "Iteration 00198    Loss -0.014974  0.000025\n",
            "Iteration 00199    Loss -0.015451  0.000026\n",
            "Iteration 00200    Loss -0.015569  0.000027\n",
            "Iteration 00201    Loss -0.015722  0.000028\n",
            "Iteration 00202    Loss -0.016017  0.000029\n",
            "Iteration 00203    Loss -0.016107  0.000029\n",
            "Iteration 00204    Loss -0.016366  0.000030\n",
            "Iteration 00205    Loss -0.016584  0.000029\n",
            "Iteration 00206    Loss -0.016458  0.000029\n",
            "Iteration 00207    Loss -0.017096  0.000024\n",
            "Iteration 00208    Loss -0.017226  0.000024\n",
            "Iteration 00209    Loss -0.017252  0.000030\n",
            "Iteration 00210    Loss -0.017524  0.000031\n",
            "Iteration 00211    Loss -0.017831  0.000029\n",
            "Iteration 00212    Loss -0.017785  0.000031\n",
            "Iteration 00213    Loss -0.018004  0.000035\n",
            "Iteration 00214    Loss -0.018420  0.000034\n",
            "Iteration 00215    Loss -0.018679  0.000030\n",
            "Iteration 00216    Loss -0.018715  0.000031\n",
            "Iteration 00217    Loss -0.018548  0.000033\n",
            "Iteration 00218    Loss -0.018417  0.000031\n",
            "Iteration 00219    Loss -0.019313  0.000033\n",
            "Iteration 00220    Loss -0.019517  0.000036\n",
            "Iteration 00221    Loss -0.019418  0.000034\n",
            "Iteration 00222    Loss -0.019921  0.000033\n",
            "Iteration 00223    Loss -0.019793  0.000037\n",
            "Iteration 00224    Loss -0.019983  0.000040\n",
            "Iteration 00225    Loss -0.020444  0.000033\n",
            "Iteration 00226    Loss -0.020455  0.000031\n",
            "Iteration 00227    Loss -0.020561  0.000038\n",
            "Iteration 00228    Loss -0.020747  0.000038\n",
            "Iteration 00229    Loss -0.021064  0.000036\n",
            "Iteration 00230    Loss -0.021268  0.000037\n",
            "Iteration 00231    Loss -0.021319  0.000041\n",
            "Iteration 00232    Loss -0.021476  0.000041\n",
            "Iteration 00233    Loss -0.021543  0.000038\n",
            "Iteration 00234    Loss -0.021925  0.000039\n",
            "Iteration 00235    Loss -0.021921  0.000045\n",
            "Iteration 00236    Loss -0.022177  0.000042\n",
            "Iteration 00237    Loss -0.022352  0.000038\n",
            "Iteration 00238    Loss -0.022549  0.000040\n",
            "Iteration 00239    Loss -0.022723  0.000045\n",
            "Iteration 00240    Loss -0.021867  0.000043\n",
            "Iteration 00241    Loss -0.022974  0.000043\n",
            "Iteration 00242    Loss -0.022625  0.000047\n",
            "Iteration 00243    Loss -0.023221  0.000042\n",
            "Iteration 00244    Loss -0.023280  0.000042\n",
            "Iteration 00245    Loss -0.023582  0.000046\n",
            "Iteration 00246    Loss -0.023332  0.000051\n",
            "Iteration 00247    Loss -0.023871  0.000045\n",
            "Iteration 00248    Loss -0.023780  0.000045\n",
            "Iteration 00249    Loss -0.024117  0.000049\n",
            "Iteration 00250    Loss -0.024260  0.000049\n",
            "Iteration 00251    Loss -0.024443  0.000049\n",
            "Iteration 00252    Loss -0.024317  0.000052\n",
            "Iteration 00253    Loss -0.024429  0.000051\n",
            "Iteration 00254    Loss -0.024976  0.000050\n",
            "Iteration 00255    Loss -0.024909  0.000053\n",
            "Iteration 00256    Loss -0.025254  0.000055\n",
            "Iteration 00257    Loss -0.025346  0.000058\n",
            "Iteration 00258    Loss -0.025489  0.000058\n",
            "Iteration 00259    Loss -0.025397  0.000058\n",
            "Iteration 00260    Loss -0.025667  0.000061\n",
            "Iteration 00261    Loss -0.025779  0.000062\n",
            "Iteration 00262    Loss -0.025859  0.000062\n",
            "Iteration 00263    Loss -0.025865  0.000060\n",
            "Iteration 00264    Loss -0.026302  0.000067\n",
            "Iteration 00265    Loss -0.026495  0.000066\n",
            "Iteration 00266    Loss -0.026461  0.000066\n",
            "Iteration 00267    Loss -0.026389  0.000070\n",
            "Iteration 00268    Loss -0.026622  0.000062\n",
            "Iteration 00269    Loss -0.026737  0.000069\n",
            "Iteration 00270    Loss -0.026955  0.000074\n",
            "Iteration 00271    Loss -0.027097  0.000061\n",
            "Iteration 00272    Loss -0.027252  0.000067\n",
            "Iteration 00273    Loss -0.027331  0.000074\n",
            "Iteration 00274    Loss -0.027517  0.000061\n",
            "Iteration 00275    Loss -0.027394  0.000074\n",
            "Iteration 00276    Loss -0.027523  0.000090\n",
            "Iteration 00277    Loss -0.027557  0.000062\n",
            "Iteration 00278    Loss -0.027864  0.000058\n",
            "Iteration 00279    Loss -0.027928  0.000074\n",
            "Iteration 00280    Loss -0.028210  0.000081\n",
            "Iteration 00281    Loss -0.028192  0.000069\n",
            "Iteration 00282    Loss -0.028148  0.000069\n",
            "Iteration 00283    Loss -0.028611  0.000084\n",
            "Iteration 00284    Loss -0.028697  0.000079\n",
            "Iteration 00285    Loss -0.028671  0.000074\n",
            "Iteration 00286    Loss -0.028743  0.000083\n",
            "Iteration 00287    Loss -0.028885  0.000086\n",
            "Iteration 00288    Loss -0.029041  0.000084\n",
            "Iteration 00289    Loss -0.029225  0.000081\n",
            "Iteration 00290    Loss -0.029220  0.000087\n",
            "Iteration 00291    Loss -0.029527  0.000083\n",
            "Iteration 00292    Loss -0.029479  0.000082\n",
            "Iteration 00293    Loss -0.029676  0.000087\n",
            "Iteration 00294    Loss -0.029670  0.000096\n",
            "Iteration 00295    Loss -0.029826  0.000088\n",
            "Iteration 00296    Loss -0.029816  0.000088\n",
            "Iteration 00297    Loss -0.029863  0.000096\n",
            "Iteration 00298    Loss -0.030207  0.000099\n",
            "Iteration 00299    Loss -0.030272  0.000096\n",
            "Iteration 00300    Loss -0.030207  0.000093\n",
            "Iteration 00301    Loss -0.030467  0.000098\n",
            "Iteration 00302    Loss -0.030545  0.000097\n",
            "Iteration 00303    Loss -0.030416  0.000096\n",
            "Iteration 00304    Loss -0.030576  0.000095\n",
            "Iteration 00305    Loss -0.030839  0.000091\n",
            "Iteration 00306    Loss -0.030866  0.000093\n",
            "Iteration 00307    Loss -0.030901  0.000096\n",
            "Iteration 00308    Loss -0.030746  0.000108\n",
            "Iteration 00309    Loss -0.031181  0.000092\n",
            "Iteration 00310    Loss -0.031195  0.000107\n",
            "Iteration 00311    Loss -0.031486  0.000114\n",
            "Iteration 00312    Loss -0.031484  0.000097\n",
            "Iteration 00313    Loss -0.031602  0.000090\n",
            "Iteration 00314    Loss -0.031703  0.000107\n",
            "Iteration 00315    Loss -0.031795  0.000115\n",
            "Iteration 00316    Loss -0.031970  0.000108\n",
            "Iteration 00317    Loss -0.031983  0.000113\n",
            "Iteration 00318    Loss -0.032159  0.000117\n",
            "Iteration 00319    Loss -0.032052  0.000111\n",
            "Iteration 00320    Loss -0.032143  0.000112\n",
            "Iteration 00321    Loss -0.032426  0.000118\n",
            "Iteration 00322    Loss -0.032306  0.000114\n",
            "Iteration 00323    Loss -0.032309  0.000124\n",
            "Iteration 00324    Loss -0.032440  0.000117\n",
            "Iteration 00325    Loss -0.032634  0.000124\n",
            "Iteration 00326    Loss -0.032675  0.000130\n",
            "Iteration 00327    Loss -0.032823  0.000128\n",
            "Iteration 00328    Loss -0.032945  0.000129\n",
            "Iteration 00329    Loss -0.033107  0.000128\n",
            "Iteration 00330    Loss -0.033179  0.000139\n",
            "Iteration 00331    Loss -0.032973  0.000137\n",
            "Iteration 00332    Loss -0.033366  0.000124\n",
            "Iteration 00333    Loss -0.033379  0.000145\n",
            "Iteration 00334    Loss -0.033084  0.000149\n",
            "Iteration 00335    Loss -0.033257  0.000129\n",
            "Iteration 00336    Loss -0.033663  0.000142\n",
            "Iteration 00337    Loss -0.033746  0.000161\n",
            "Iteration 00338    Loss -0.033850  0.000136\n",
            "Iteration 00339    Loss -0.033862  0.000138\n",
            "Iteration 00340    Loss -0.034031  0.000142\n",
            "Iteration 00341    Loss -0.034047  0.000148\n",
            "Iteration 00342    Loss -0.034022  0.000143\n",
            "Iteration 00343    Loss -0.034227  0.000129\n",
            "Iteration 00344    Loss -0.034289  0.000163\n",
            "Iteration 00345    Loss -0.034273  0.000124\n",
            "Iteration 00346    Loss -0.033881  0.000137\n",
            "Iteration 00347    Loss -0.034320  0.000161\n",
            "Iteration 00348    Loss -0.034552  0.000138\n",
            "Iteration 00349    Loss -0.034685  0.000131\n",
            "Iteration 00350    Loss -0.034703  0.000151\n",
            "Iteration 00351    Loss -0.034779  0.000159\n",
            "Iteration 00352    Loss -0.034830  0.000140\n",
            "Iteration 00353    Loss -0.034890  0.000147\n",
            "Iteration 00354    Loss -0.034762  0.000130\n",
            "Iteration 00355    Loss -0.034843  0.000140\n",
            "Iteration 00356    Loss -0.035182  0.000158\n",
            "Iteration 00357    Loss -0.035193  0.000158\n",
            "Iteration 00358    Loss -0.035204  0.000156\n",
            "Iteration 00359    Loss -0.035287  0.000155\n",
            "Iteration 00360    Loss -0.035444  0.000154\n",
            "Iteration 00361    Loss -0.035383  0.000171\n",
            "Iteration 00362    Loss -0.035529  0.000160\n",
            "Iteration 00363    Loss -0.035772  0.000154\n",
            "Iteration 00364    Loss -0.035702  0.000163\n",
            "Iteration 00365    Loss -0.035359  0.000173\n",
            "Iteration 00366    Loss -0.035948  0.000155\n",
            "Iteration 00367    Loss -0.035938  0.000154\n",
            "Iteration 00368    Loss -0.035958  0.000178\n",
            "Iteration 00369    Loss -0.036146  0.000179\n",
            "Iteration 00370    Loss -0.036186  0.000161\n",
            "Iteration 00371    Loss -0.036097  0.000173\n",
            "Iteration 00372    Loss -0.036032  0.000188\n",
            "Iteration 00373    Loss -0.035598  0.000185\n",
            "Iteration 00374    Loss -0.036407  0.000192\n",
            "Iteration 00375    Loss -0.036377  0.000166\n",
            "Iteration 00376    Loss -0.036443  0.000170\n",
            "Iteration 00377    Loss -0.036515  0.000184\n",
            "Iteration 00378    Loss -0.036394  0.000176\n",
            "Iteration 00379    Loss -0.036501  0.000185\n",
            "Iteration 00380    Loss -0.036776  0.000187\n",
            "Iteration 00381    Loss -0.036848  0.000191\n",
            "Iteration 00382    Loss -0.036815  0.000190\n",
            "Iteration 00383    Loss -0.036914  0.000200\n",
            "Iteration 00384    Loss -0.037007  0.000168\n",
            "Iteration 00385    Loss -0.037059  0.000167\n",
            "Iteration 00386    Loss -0.037064  0.000204\n",
            "Iteration 00387    Loss -0.037240  0.000185\n",
            "Iteration 00388    Loss -0.037203  0.000180\n",
            "Iteration 00389    Loss -0.037310  0.000214\n",
            "Iteration 00390    Loss -0.037324  0.000186\n",
            "Iteration 00391    Loss -0.037234  0.000182\n",
            "Iteration 00392    Loss -0.037558  0.000196\n",
            "Iteration 00393    Loss -0.037545  0.000196\n",
            "Iteration 00394    Loss -0.037499  0.000204\n",
            "Iteration 00395    Loss -0.037306  0.000184\n",
            "Iteration 00396    Loss -0.037761  0.000205\n",
            "Iteration 00397    Loss -0.037850  0.000203\n",
            "Iteration 00398    Loss -0.037699  0.000208\n",
            "Iteration 00399    Loss -0.037912  0.000206\n",
            "Iteration 00400    Loss -0.037939  0.000208\n",
            "Iteration 00401    Loss -0.037980  0.000227\n",
            "Iteration 00402    Loss -0.038110  0.000205\n",
            "Iteration 00403    Loss -0.037898  0.000185\n",
            "Iteration 00404    Loss -0.038178  0.000214\n",
            "Iteration 00405    Loss -0.037867  0.000240\n",
            "Iteration 00406    Loss -0.038203  0.000184\n",
            "Iteration 00407    Loss -0.038274  0.000204\n",
            "Iteration 00408    Loss -0.038353  0.000235\n",
            "Iteration 00409    Loss -0.038462  0.000206\n",
            "Iteration 00410    Loss -0.038463  0.000216\n",
            "Iteration 00411    Loss -0.038572  0.000231\n",
            "Iteration 00412    Loss -0.038537  0.000227\n",
            "Iteration 00413    Loss -0.038631  0.000227\n",
            "Iteration 00414    Loss -0.038651  0.000238\n",
            "Iteration 00415    Loss -0.038766  0.000226\n",
            "Iteration 00416    Loss -0.038845  0.000226\n",
            "Iteration 00417    Loss -0.038248  0.000218\n",
            "Iteration 00418    Loss -0.038872  0.000261\n",
            "Iteration 00419    Loss -0.038865  0.000223\n",
            "Iteration 00420    Loss -0.039000  0.000234\n",
            "Iteration 00421    Loss -0.039069  0.000254\n",
            "Iteration 00422    Loss -0.039065  0.000241\n",
            "Iteration 00423    Loss -0.039059  0.000232\n",
            "Iteration 00424    Loss -0.039052  0.000213\n",
            "Iteration 00425    Loss -0.038997  0.000254\n",
            "Iteration 00426    Loss -0.038868  0.000239\n",
            "Iteration 00427    Loss -0.039221  0.000196\n",
            "Iteration 00428    Loss -0.039211  0.000235\n",
            "Iteration 00429    Loss -0.039324  0.000258\n",
            "Iteration 00430    Loss -0.039262  0.000233\n",
            "Iteration 00431    Loss -0.039513  0.000232\n",
            "Iteration 00432    Loss -0.039439  0.000251\n",
            "Iteration 00433    Loss -0.039580  0.000250\n",
            "Iteration 00434    Loss -0.039490  0.000239\n",
            "Iteration 00435    Loss -0.039678  0.000248\n",
            "Iteration 00436    Loss -0.039700  0.000250\n",
            "Iteration 00437    Loss -0.039657  0.000250\n",
            "Iteration 00438    Loss -0.039708  0.000250\n",
            "Iteration 00439    Loss -0.039838  0.000272\n",
            "Iteration 00440    Loss -0.039814  0.000264\n",
            "Iteration 00441    Loss -0.039702  0.000261\n",
            "Iteration 00442    Loss -0.039911  0.000255\n",
            "Iteration 00443    Loss -0.039853  0.000298\n",
            "Iteration 00444    Loss -0.039923  0.000221\n",
            "Iteration 00445    Loss -0.039869  0.000261\n",
            "Iteration 00446    Loss -0.040014  0.000294\n",
            "Iteration 00447    Loss -0.040106  0.000264\n",
            "Iteration 00448    Loss -0.040160  0.000258\n",
            "Iteration 00449    Loss -0.040163  0.000297\n",
            "Iteration 00450    Loss -0.040259  0.000256\n",
            "Iteration 00451    Loss -0.040256  0.000279\n",
            "Iteration 00452    Loss -0.040283  0.000286\n",
            "Iteration 00453    Loss -0.040299  0.000294\n",
            "Iteration 00454    Loss -0.040282  0.000274\n",
            "Iteration 00455    Loss -0.040326  0.000301\n",
            "Iteration 00456    Loss -0.040463  0.000286\n",
            "Iteration 00457    Loss -0.040571  0.000299\n",
            "Iteration 00458    Loss -0.040398  0.000295\n",
            "Iteration 00459    Loss -0.040589  0.000294\n",
            "Iteration 00460    Loss -0.040618  0.000306\n",
            "Iteration 00461    Loss -0.040636  0.000288\n",
            "Iteration 00462    Loss -0.040606  0.000299\n",
            "Iteration 00463    Loss -0.040312  0.000318\n",
            "Iteration 00464    Loss -0.040446  0.000231\n",
            "Iteration 00465    Loss -0.040723  0.000290\n",
            "Iteration 00466    Loss -0.040654  0.000331\n",
            "Iteration 00467    Loss -0.040525  0.000265\n",
            "Iteration 00468    Loss -0.040604  0.000258\n",
            "Iteration 00469    Loss -0.040497  0.000287\n",
            "Iteration 00470    Loss -0.040689  0.000277\n",
            "Iteration 00471    Loss -0.040725  0.000260\n",
            "Iteration 00472    Loss -0.040868  0.000270\n",
            "Iteration 00473    Loss -0.040959  0.000290\n",
            "Iteration 00474    Loss -0.040846  0.000281\n",
            "Iteration 00475    Loss -0.041041  0.000278\n",
            "Iteration 00476    Loss -0.041017  0.000292\n",
            "Iteration 00477    Loss -0.041029  0.000281\n",
            "Iteration 00478    Loss -0.041206  0.000264\n",
            "Iteration 00479    Loss -0.041183  0.000298\n",
            "Iteration 00480    Loss -0.041075  0.000321\n",
            "Iteration 00481    Loss -0.041246  0.000282\n",
            "Iteration 00482    Loss -0.041260  0.000270\n",
            "Iteration 00483    Loss -0.041230  0.000321\n",
            "Iteration 00484    Loss -0.041390  0.000322\n",
            "Iteration 00485    Loss -0.041398  0.000283\n",
            "Iteration 00486    Loss -0.041425  0.000320\n",
            "Iteration 00487    Loss -0.041457  0.000338\n",
            "Iteration 00488    Loss -0.041532  0.000304\n",
            "Iteration 00489    Loss -0.041566  0.000324\n",
            "Iteration 00490    Loss -0.041591  0.000342\n",
            "Iteration 00491    Loss -0.041606  0.000308\n",
            "Iteration 00492    Loss -0.041394  0.000332\n",
            "Iteration 00493    Loss -0.041592  0.000358\n",
            "Iteration 00494    Loss -0.041696  0.000331\n",
            "Iteration 00495    Loss -0.041626  0.000327\n",
            "Iteration 00496    Loss -0.041694  0.000338\n",
            "Iteration 00497    Loss -0.041569  0.000329\n",
            "Iteration 00498    Loss -0.041656  0.000323\n",
            "Iteration 00499    Loss -0.041820  0.000367\n",
            " 98% 491/500 [4:40:13<05:02, 33.63s/it]/content/SOTS/outdoor/hazy/0001_0.8_0.2.jpg\n",
            "Iteration 00000    Loss 0.373880  0.000022\n",
            "Iteration 00001    Loss 11.131274  0.000007\n",
            "Iteration 00002    Loss 0.282962  0.000004\n",
            "Iteration 00003    Loss 0.234712  0.000002\n",
            "Iteration 00004    Loss 0.200010  0.000002\n",
            "Iteration 00005    Loss 0.185122  0.000001\n",
            "Iteration 00006    Loss 0.146337  0.000001\n",
            "Iteration 00007    Loss 0.133620  0.000001\n",
            "Iteration 00008    Loss 0.111737  0.000001\n",
            "Iteration 00009    Loss 0.099122  0.000001\n",
            "Iteration 00010    Loss 0.090567  0.000001\n",
            "Iteration 00011    Loss 0.085153  0.000001\n",
            "Iteration 00012    Loss 0.080388  0.000001\n",
            "Iteration 00013    Loss 0.076732  0.000002\n",
            "Iteration 00014    Loss 0.074233  0.000002\n",
            "Iteration 00015    Loss 0.071868  0.000002\n",
            "Iteration 00016    Loss 0.069693  0.000002\n",
            "Iteration 00017    Loss 0.067843  0.000003\n",
            "Iteration 00018    Loss 0.066149  0.000003\n",
            "Iteration 00019    Loss 0.064558  0.000003\n",
            "Iteration 00020    Loss 0.063004  0.000004\n",
            "Iteration 00021    Loss 0.061862  0.000004\n",
            "Iteration 00022    Loss 0.060886  0.000006\n",
            "Iteration 00023    Loss 0.059970  0.000006\n",
            "Iteration 00024    Loss 0.059157  0.000007\n",
            "Iteration 00025    Loss 0.058361  0.000008\n",
            "Iteration 00026    Loss 0.057473  0.000008\n",
            "Iteration 00027    Loss 0.056595  0.000008\n",
            "Iteration 00028    Loss 0.055447  0.000009\n",
            "Iteration 00029    Loss 0.054667  0.000010\n",
            "Iteration 00030    Loss 0.053773  0.000010\n",
            "Iteration 00031    Loss 0.053046  0.000010\n",
            "Iteration 00032    Loss 0.052685  0.000010\n",
            "Iteration 00033    Loss 0.051675  0.000010\n",
            "Iteration 00034    Loss 0.050450  0.000010\n",
            "Iteration 00035    Loss 0.049482  0.000010\n",
            "Iteration 00036    Loss 0.048534  0.000010\n",
            "Iteration 00037    Loss 0.047860  0.000009\n",
            "Iteration 00038    Loss 0.047170  0.000010\n",
            "Iteration 00039    Loss 0.046261  0.000011\n",
            "Iteration 00040    Loss 0.045363  0.000012\n",
            "Iteration 00041    Loss 0.044651  0.000013\n",
            "Iteration 00042    Loss 0.043619  0.000014\n",
            "Iteration 00043    Loss 0.043024  0.000015\n",
            "Iteration 00044    Loss 0.041816  0.000014\n",
            "Iteration 00045    Loss 0.041344  0.000014\n",
            "Iteration 00046    Loss 0.040842  0.000015\n",
            "Iteration 00047    Loss 0.039731  0.000016\n",
            "Iteration 00048    Loss 0.038892  0.000016\n",
            "Iteration 00049    Loss 0.038459  0.000017\n",
            "Iteration 00050    Loss 0.037602  0.000017\n",
            "Iteration 00051    Loss 0.036721  0.000018\n",
            "Iteration 00052    Loss 0.036067  0.000019\n",
            "Iteration 00053    Loss 0.035468  0.000020\n",
            "Iteration 00054    Loss 0.034792  0.000022\n",
            "Iteration 00055    Loss 0.034093  0.000023\n",
            "Iteration 00056    Loss 0.033426  0.000021\n",
            "Iteration 00057    Loss 0.032774  0.000021\n",
            "Iteration 00058    Loss 0.032200  0.000024\n",
            "Iteration 00059    Loss 0.031309  0.000023\n",
            "Iteration 00060    Loss 0.030774  0.000024\n",
            "Iteration 00061    Loss 0.030504  0.000026\n",
            "Iteration 00062    Loss 0.029622  0.000026\n",
            "Iteration 00063    Loss 0.029001  0.000027\n",
            "Iteration 00064    Loss 0.028356  0.000029\n",
            "Iteration 00065    Loss 0.027877  0.000029\n",
            "Iteration 00066    Loss 0.027214  0.000033\n",
            "Iteration 00067    Loss 0.026670  0.000037\n",
            "Iteration 00068    Loss 0.025982  0.000041\n",
            "Iteration 00069    Loss 0.025616  0.000049\n",
            "Iteration 00070    Loss 0.024968  0.000052\n",
            "Iteration 00071    Loss 0.024426  0.000049\n",
            "Iteration 00072    Loss 0.023781  0.000053\n",
            "Iteration 00073    Loss 0.023382  0.000066\n",
            "Iteration 00074    Loss 0.022709  0.000077\n",
            "Iteration 00075    Loss 0.022213  0.000086\n",
            "Iteration 00076    Loss 0.021680  0.000102\n",
            "Iteration 00077    Loss 0.021072  0.000113\n",
            "Iteration 00078    Loss 0.020671  0.000120\n",
            "Iteration 00079    Loss 0.019984  0.000131\n",
            "Iteration 00080    Loss 0.019524  0.000134\n",
            "Iteration 00081    Loss 0.019005  0.000142\n",
            "Iteration 00082    Loss 0.018488  0.000159\n",
            "Iteration 00083    Loss 0.017891  0.000171\n",
            "Iteration 00084    Loss 0.017519  0.000177\n",
            "Iteration 00085    Loss 0.017179  0.000196\n",
            "Iteration 00086    Loss 0.016634  0.000198\n",
            "Iteration 00087    Loss 0.016238  0.000212\n",
            "Iteration 00088    Loss 0.015697  0.000193\n",
            "Iteration 00089    Loss 0.015327  0.000222\n",
            "Iteration 00090    Loss 0.014840  0.000275\n",
            "Iteration 00091    Loss 0.014430  0.000278\n",
            "Iteration 00092    Loss 0.014174  0.000236\n",
            "Iteration 00093    Loss 0.013412  0.000280\n",
            "Iteration 00094    Loss 0.013003  0.000318\n",
            "Iteration 00095    Loss 0.012590  0.000279\n",
            "Iteration 00096    Loss 0.012174  0.000298\n",
            "Iteration 00097    Loss 0.011736  0.000334\n",
            "Iteration 00098    Loss 0.011273  0.000353\n",
            "Iteration 00099    Loss 0.010959  0.000341\n",
            "Iteration 00100    Loss 0.010486  0.000316\n",
            "Iteration 00101    Loss 0.010023  0.000325\n",
            "Iteration 00102    Loss 0.009608  0.000329\n",
            "Iteration 00103    Loss 0.009424  0.000344\n",
            "Iteration 00104    Loss 0.008914  0.000341\n",
            "Iteration 00105    Loss 0.008624  0.000349\n",
            "Iteration 00106    Loss 0.008008  0.000349\n",
            "Iteration 00107    Loss 0.007813  0.000356\n",
            "Iteration 00108    Loss 0.007283  0.000407\n",
            "Iteration 00109    Loss 0.006978  0.000420\n",
            "Iteration 00110    Loss 0.006676  0.000411\n",
            "Iteration 00111    Loss 0.006302  0.000417\n",
            "Iteration 00112    Loss 0.005982  0.000409\n",
            "Iteration 00113    Loss 0.005525  0.000392\n",
            "Iteration 00114    Loss 0.005161  0.000369\n",
            "Iteration 00115    Loss 0.005073  0.000426\n",
            "Iteration 00116    Loss 0.004440  0.000476\n",
            "Iteration 00117    Loss 0.004052  0.000454\n",
            "Iteration 00118    Loss 0.003730  0.000435\n",
            "Iteration 00119    Loss 0.003429  0.000432\n",
            "Iteration 00120    Loss 0.003103  0.000416\n",
            "Iteration 00121    Loss 0.002934  0.000433\n",
            "Iteration 00122    Loss 0.002482  0.000465\n",
            "Iteration 00123    Loss 0.002307  0.000464\n",
            "Iteration 00124    Loss 0.001802  0.000444\n",
            "Iteration 00125    Loss 0.001891  0.000466\n",
            "Iteration 00126    Loss 0.001622  0.000484\n",
            "Iteration 00127    Loss 0.000885  0.000482\n",
            "Iteration 00128    Loss 0.000560  0.000477\n",
            "Iteration 00129    Loss 0.000148  0.000473\n",
            "Iteration 00130    Loss -0.000022  0.000478\n",
            "Iteration 00131    Loss -0.000337  0.000489\n",
            "Iteration 00132    Loss -0.000480  0.000489\n",
            "Iteration 00133    Loss -0.000833  0.000495\n",
            "Iteration 00134    Loss -0.000733  0.000501\n",
            "Iteration 00135    Loss -0.001439  0.000535\n",
            "Iteration 00136    Loss -0.001540  0.000523\n",
            "Iteration 00137    Loss -0.001654  0.000506\n",
            "Iteration 00138    Loss -0.002199  0.000474\n",
            "Iteration 00139    Loss -0.002599  0.000472\n",
            "Iteration 00140    Loss -0.002772  0.000489\n",
            "Iteration 00141    Loss -0.003182  0.000510\n",
            "Iteration 00142    Loss -0.003332  0.000519\n",
            "Iteration 00143    Loss -0.003542  0.000539\n",
            "Iteration 00144    Loss -0.003941  0.000537\n",
            "Iteration 00145    Loss -0.004270  0.000554\n",
            "Iteration 00146    Loss -0.004531  0.000591\n",
            "Iteration 00147    Loss -0.004823  0.000609\n",
            "Iteration 00148    Loss -0.004832  0.000580\n",
            "Iteration 00149    Loss -0.005369  0.000534\n",
            "Iteration 00150    Loss -0.005463  0.000524\n",
            "Iteration 00151    Loss -0.005788  0.000546\n",
            "Iteration 00152    Loss -0.006009  0.000572\n",
            "Iteration 00153    Loss -0.006351  0.000563\n",
            "Iteration 00154    Loss -0.006591  0.000548\n",
            "Iteration 00155    Loss -0.006618  0.000553\n",
            "Iteration 00156    Loss -0.006930  0.000575\n",
            "Iteration 00157    Loss -0.007257  0.000546\n",
            "Iteration 00158    Loss -0.007129  0.000547\n",
            "Iteration 00159    Loss -0.007394  0.000580\n",
            "Iteration 00160    Loss -0.007581  0.000602\n",
            "Iteration 00161    Loss -0.007818  0.000587\n",
            "Iteration 00162    Loss -0.008007  0.000598\n",
            "Iteration 00163    Loss -0.008390  0.000629\n",
            "Iteration 00164    Loss -0.008349  0.000589\n",
            "Iteration 00165    Loss -0.008891  0.000555\n",
            "Iteration 00166    Loss -0.009096  0.000617\n",
            "Iteration 00167    Loss -0.009250  0.000634\n",
            "Iteration 00168    Loss -0.009637  0.000566\n",
            "Iteration 00169    Loss -0.009633  0.000547\n",
            "Iteration 00170    Loss -0.009760  0.000572\n",
            "Iteration 00171    Loss -0.010234  0.000600\n",
            "Iteration 00172    Loss -0.010465  0.000602\n",
            "Iteration 00173    Loss -0.010563  0.000643\n",
            "Iteration 00174    Loss -0.010824  0.000701\n",
            "Iteration 00175    Loss -0.010977  0.000684\n",
            "Iteration 00176    Loss -0.011235  0.000615\n",
            "Iteration 00177    Loss -0.011352  0.000638\n",
            "Iteration 00178    Loss -0.011702  0.000677\n",
            "Iteration 00179    Loss -0.011732  0.000669\n",
            "Iteration 00180    Loss -0.011889  0.000657\n",
            "Iteration 00181    Loss -0.012310  0.000637\n",
            "Iteration 00182    Loss -0.012351  0.000634\n",
            "Iteration 00183    Loss -0.012684  0.000657\n",
            "Iteration 00184    Loss -0.012829  0.000676\n",
            "Iteration 00185    Loss -0.012986  0.000730\n",
            "Iteration 00186    Loss -0.013126  0.000719\n",
            "Iteration 00187    Loss -0.013452  0.000669\n",
            "Iteration 00188    Loss -0.013542  0.000631\n",
            "Iteration 00189    Loss -0.013832  0.000605\n",
            "Iteration 00190    Loss -0.014173  0.000646\n",
            "Iteration 00191    Loss -0.014311  0.000687\n",
            "Iteration 00192    Loss -0.014222  0.000704\n",
            "Iteration 00193    Loss -0.014575  0.000681\n",
            "Iteration 00194    Loss -0.014769  0.000671\n",
            "Iteration 00195    Loss -0.014923  0.000705\n",
            "Iteration 00196    Loss -0.014987  0.000725\n",
            "Iteration 00197    Loss -0.015133  0.000703\n",
            "Iteration 00198    Loss -0.015485  0.000695\n",
            "Iteration 00199    Loss -0.015491  0.000747\n",
            "Iteration 00200    Loss -0.015782  0.000759\n",
            "Iteration 00201    Loss -0.015950  0.000716\n",
            "Iteration 00202    Loss -0.016085  0.000658\n",
            "Iteration 00203    Loss -0.016272  0.000714\n",
            "Iteration 00204    Loss -0.016475  0.000776\n",
            "Iteration 00205    Loss -0.016383  0.000792\n",
            "Iteration 00206    Loss -0.016913  0.000774\n",
            "Iteration 00207    Loss -0.017037  0.000726\n",
            "Iteration 00208    Loss -0.017139  0.000698\n",
            "Iteration 00209    Loss -0.017466  0.000730\n",
            "Iteration 00210    Loss -0.017416  0.000756\n",
            "Iteration 00211    Loss -0.017735  0.000788\n",
            "Iteration 00212    Loss -0.018071  0.000762\n",
            "Iteration 00213    Loss -0.018080  0.000776\n",
            "Iteration 00214    Loss -0.018364  0.000813\n",
            "Iteration 00215    Loss -0.018263  0.000732\n",
            "Iteration 00216    Loss -0.018696  0.000684\n",
            "Iteration 00217    Loss -0.018650  0.000747\n",
            "Iteration 00218    Loss -0.018950  0.000772\n",
            "Iteration 00219    Loss -0.018941  0.000753\n",
            "Iteration 00220    Loss -0.019154  0.000751\n",
            "Iteration 00221    Loss -0.019294  0.000764\n",
            "Iteration 00222    Loss -0.019447  0.000756\n",
            "Iteration 00223    Loss -0.019191  0.000757\n",
            "Iteration 00224    Loss -0.019625  0.000775\n",
            "Iteration 00225    Loss -0.019925  0.000753\n",
            "Iteration 00226    Loss -0.019956  0.000772\n",
            "Iteration 00227    Loss -0.019891  0.000813\n",
            "Iteration 00228    Loss -0.019581  0.000828\n",
            "Iteration 00229    Loss -0.019989  0.000820\n",
            "Iteration 00230    Loss -0.019799  0.000860\n",
            "Iteration 00231    Loss -0.020529  0.000836\n",
            "Iteration 00232    Loss -0.020451  0.000718\n",
            "Iteration 00233    Loss -0.020832  0.000747\n",
            "Iteration 00234    Loss -0.020905  0.000824\n",
            "Iteration 00235    Loss -0.020975  0.000830\n",
            "Iteration 00236    Loss -0.021502  0.000833\n",
            "Iteration 00237    Loss -0.021542  0.000836\n",
            "Iteration 00238    Loss -0.021406  0.000808\n",
            "Iteration 00239    Loss -0.021872  0.000797\n",
            "Iteration 00240    Loss -0.022066  0.000799\n",
            "Iteration 00241    Loss -0.022032  0.000798\n",
            "Iteration 00242    Loss -0.022080  0.000776\n",
            "Iteration 00243    Loss -0.022494  0.000783\n",
            "Iteration 00244    Loss -0.022796  0.000791\n",
            "Iteration 00245    Loss -0.022713  0.000806\n",
            "Iteration 00246    Loss -0.023191  0.000844\n",
            "Iteration 00247    Loss -0.023311  0.000798\n",
            "Iteration 00248    Loss -0.023521  0.000804\n",
            "Iteration 00249    Loss -0.023503  0.000835\n",
            "Iteration 00250    Loss -0.023395  0.000858\n",
            "Iteration 00251    Loss -0.023825  0.000809\n",
            "Iteration 00252    Loss -0.023937  0.000805\n",
            "Iteration 00253    Loss -0.024204  0.000836\n",
            "Iteration 00254    Loss -0.024409  0.000877\n",
            "Iteration 00255    Loss -0.024335  0.000861\n",
            "Iteration 00256    Loss -0.024582  0.000836\n",
            "Iteration 00257    Loss -0.024678  0.000800\n",
            "Iteration 00258    Loss -0.024688  0.000842\n",
            "Iteration 00259    Loss -0.024974  0.000872\n",
            "Iteration 00260    Loss -0.024613  0.000877\n",
            "Iteration 00261    Loss -0.025387  0.000867\n",
            "Iteration 00262    Loss -0.025523  0.000895\n",
            "Iteration 00263    Loss -0.025701  0.000890\n",
            "Iteration 00264    Loss -0.025817  0.000910\n",
            "Iteration 00265    Loss -0.025854  0.000851\n",
            "Iteration 00266    Loss -0.025913  0.000892\n",
            "Iteration 00267    Loss -0.026307  0.000878\n",
            "Iteration 00268    Loss -0.026410  0.000798\n",
            "Iteration 00269    Loss -0.026558  0.000867\n",
            "Iteration 00270    Loss -0.026615  0.000884\n",
            "Iteration 00271    Loss -0.026373  0.000869\n",
            "Iteration 00272    Loss -0.026525  0.000912\n",
            "Iteration 00273    Loss -0.026713  0.000820\n",
            "Iteration 00274    Loss -0.027264  0.000849\n",
            "Iteration 00275    Loss -0.027249  0.000955\n",
            "Iteration 00276    Loss -0.027552  0.000769\n",
            "Iteration 00277    Loss -0.027163  0.000895\n",
            "Iteration 00278    Loss -0.027414  0.000901\n",
            "Iteration 00279    Loss -0.027574  0.000830\n",
            "Iteration 00280    Loss -0.027980  0.000929\n",
            "Iteration 00281    Loss -0.028162  0.000883\n",
            "Iteration 00282    Loss -0.028228  0.000797\n",
            "Iteration 00283    Loss -0.028420  0.000902\n",
            "Iteration 00284    Loss -0.028457  0.000802\n",
            "Iteration 00285    Loss -0.028745  0.000809\n",
            "Iteration 00286    Loss -0.028497  0.000926\n",
            "Iteration 00287    Loss -0.028903  0.000854\n",
            "Iteration 00288    Loss -0.028608  0.000782\n",
            "Iteration 00289    Loss -0.029146  0.000920\n",
            "Iteration 00290    Loss -0.029408  0.000875\n",
            "Iteration 00291    Loss -0.029474  0.000865\n",
            "Iteration 00292    Loss -0.029637  0.000841\n",
            "Iteration 00293    Loss -0.029868  0.000776\n",
            "Iteration 00294    Loss -0.029796  0.000853\n",
            "Iteration 00295    Loss -0.030003  0.000929\n",
            "Iteration 00296    Loss -0.030120  0.000914\n",
            "Iteration 00297    Loss -0.030203  0.000837\n",
            "Iteration 00298    Loss -0.030281  0.000831\n",
            "Iteration 00299    Loss -0.030036  0.000818\n",
            "Iteration 00300    Loss -0.030571  0.000908\n",
            "Iteration 00301    Loss -0.030863  0.000902\n",
            "Iteration 00302    Loss -0.030777  0.000874\n",
            "Iteration 00303    Loss -0.030608  0.000897\n",
            "Iteration 00304    Loss -0.030892  0.000847\n",
            "Iteration 00305    Loss -0.030766  0.000826\n",
            "Iteration 00306    Loss -0.031011  0.000853\n",
            "Iteration 00307    Loss -0.031340  0.000839\n",
            "Iteration 00308    Loss -0.031483  0.000845\n",
            "Iteration 00309    Loss -0.031615  0.000882\n",
            "Iteration 00310    Loss -0.031778  0.000888\n",
            "Iteration 00311    Loss -0.031948  0.000861\n",
            "Iteration 00312    Loss -0.032018  0.000863\n",
            "Iteration 00313    Loss -0.032086  0.000891\n",
            "Iteration 00314    Loss -0.032166  0.000903\n",
            "Iteration 00315    Loss -0.032352  0.000887\n",
            "Iteration 00316    Loss -0.032273  0.000896\n",
            "Iteration 00317    Loss -0.032247  0.000955\n",
            "Iteration 00318    Loss -0.032636  0.000929\n",
            "Iteration 00319    Loss -0.032641  0.000908\n",
            "Iteration 00320    Loss -0.032847  0.000862\n",
            "Iteration 00321    Loss -0.032873  0.001006\n",
            "Iteration 00322    Loss -0.032881  0.000853\n",
            "Iteration 00323    Loss -0.032945  0.000916\n",
            "Iteration 00324    Loss -0.033134  0.000875\n",
            "Iteration 00325    Loss -0.033267  0.000868\n",
            "Iteration 00326    Loss -0.033288  0.000967\n",
            "Iteration 00327    Loss -0.033377  0.000895\n",
            "Iteration 00328    Loss -0.033515  0.000909\n",
            "Iteration 00329    Loss -0.033529  0.000921\n",
            "Iteration 00330    Loss -0.033678  0.000902\n",
            "Iteration 00331    Loss -0.033750  0.000986\n",
            "Iteration 00332    Loss -0.033693  0.000915\n",
            "Iteration 00333    Loss -0.033899  0.000905\n",
            "Iteration 00334    Loss -0.033743  0.000954\n",
            "Iteration 00335    Loss -0.034072  0.000939\n",
            "Iteration 00336    Loss -0.034170  0.000959\n",
            "Iteration 00337    Loss -0.034296  0.000962\n",
            "Iteration 00338    Loss -0.034055  0.000904\n",
            "Iteration 00339    Loss -0.034236  0.000935\n",
            "Iteration 00340    Loss -0.033972  0.000958\n",
            "Iteration 00341    Loss -0.034408  0.000997\n",
            "Iteration 00342    Loss -0.034570  0.000872\n",
            "Iteration 00343    Loss -0.034102  0.000997\n",
            "Iteration 00344    Loss -0.034820  0.000890\n",
            "Iteration 00345    Loss -0.034694  0.000898\n",
            "Iteration 00346    Loss -0.034954  0.000949\n",
            "Iteration 00347    Loss -0.034985  0.000913\n",
            "Iteration 00348    Loss -0.034827  0.000936\n",
            "Iteration 00349    Loss -0.035124  0.000891\n",
            "Iteration 00350    Loss -0.035189  0.000833\n",
            "Iteration 00351    Loss -0.035317  0.000966\n",
            "Iteration 00352    Loss -0.035318  0.000950\n",
            "Iteration 00353    Loss -0.035507  0.000912\n",
            "Iteration 00354    Loss -0.035117  0.001020\n",
            "Iteration 00355    Loss -0.035526  0.000912\n",
            "Iteration 00356    Loss -0.035709  0.000929\n",
            "Iteration 00357    Loss -0.035673  0.000965\n",
            "Iteration 00358    Loss -0.035714  0.000881\n",
            "Iteration 00359    Loss -0.035520  0.000959\n",
            "Iteration 00360    Loss -0.035806  0.001034\n",
            "Iteration 00361    Loss -0.036022  0.000995\n",
            "Iteration 00362    Loss -0.036109  0.001020\n",
            "Iteration 00363    Loss -0.036040  0.000957\n",
            "Iteration 00364    Loss -0.036095  0.001041\n",
            "Iteration 00365    Loss -0.036103  0.001033\n",
            "Iteration 00366    Loss -0.036346  0.000980\n",
            "Iteration 00367    Loss -0.036418  0.001068\n",
            "Iteration 00368    Loss -0.036392  0.001045\n",
            "Iteration 00369    Loss -0.036588  0.001052\n",
            "Iteration 00370    Loss -0.036453  0.000987\n",
            "Iteration 00371    Loss -0.036564  0.000997\n",
            "Iteration 00372    Loss -0.036378  0.001036\n",
            "Iteration 00373    Loss -0.036711  0.001022\n",
            "Iteration 00374    Loss -0.036821  0.000969\n",
            "Iteration 00375    Loss -0.036653  0.001012\n",
            "Iteration 00376    Loss -0.036924  0.001011\n",
            "Iteration 00377    Loss -0.037058  0.000982\n",
            "Iteration 00378    Loss -0.037028  0.001032\n",
            "Iteration 00379    Loss -0.037182  0.001049\n",
            "Iteration 00380    Loss -0.037143  0.001071\n",
            "Iteration 00381    Loss -0.037304  0.001009\n",
            "Iteration 00382    Loss -0.037318  0.001105\n",
            "Iteration 00383    Loss -0.037221  0.000966\n",
            "Iteration 00384    Loss -0.037394  0.001162\n",
            "Iteration 00385    Loss -0.037375  0.001017\n",
            "Iteration 00386    Loss -0.037508  0.001054\n",
            "Iteration 00387    Loss -0.037519  0.001081\n",
            "Iteration 00388    Loss -0.037604  0.001042\n",
            "Iteration 00389    Loss -0.037671  0.001059\n",
            "Iteration 00390    Loss -0.037764  0.001038\n",
            "Iteration 00391    Loss -0.037827  0.001039\n",
            "Iteration 00392    Loss -0.037877  0.001046\n",
            "Iteration 00393    Loss -0.037929  0.001060\n",
            "Iteration 00394    Loss -0.037928  0.001099\n",
            "Iteration 00395    Loss -0.037771  0.001030\n",
            "Iteration 00396    Loss -0.038073  0.001112\n",
            "Iteration 00397    Loss -0.038175  0.001115\n",
            "Iteration 00398    Loss -0.038198  0.001103\n",
            "Iteration 00399    Loss -0.038286  0.001051\n",
            "Iteration 00400    Loss -0.038288  0.001085\n",
            "Iteration 00401    Loss -0.038343  0.001199\n",
            "Iteration 00402    Loss -0.038302  0.001022\n",
            "Iteration 00403    Loss -0.038355  0.001103\n",
            "Iteration 00404    Loss -0.038445  0.001047\n",
            "Iteration 00405    Loss -0.038506  0.001095\n",
            "Iteration 00406    Loss -0.038382  0.001030\n",
            "Iteration 00407    Loss -0.038584  0.001047\n",
            "Iteration 00408    Loss -0.038489  0.001054\n",
            "Iteration 00409    Loss -0.038620  0.001111\n",
            "Iteration 00410    Loss -0.038709  0.001013\n",
            "Iteration 00411    Loss -0.038752  0.001064\n",
            "Iteration 00412    Loss -0.038786  0.001160\n",
            "Iteration 00413    Loss -0.038816  0.001082\n",
            "Iteration 00414    Loss -0.038944  0.001064\n",
            "Iteration 00415    Loss -0.038945  0.001079\n",
            "Iteration 00416    Loss -0.039013  0.001120\n",
            "Iteration 00417    Loss -0.039017  0.001052\n",
            "Iteration 00418    Loss -0.038921  0.001081\n",
            "Iteration 00419    Loss -0.039086  0.001083\n",
            "Iteration 00420    Loss -0.039249  0.001109\n",
            "Iteration 00421    Loss -0.039228  0.001132\n",
            "Iteration 00422    Loss -0.039228  0.001096\n",
            "Iteration 00423    Loss -0.039215  0.001136\n",
            "Iteration 00424    Loss -0.039390  0.001084\n",
            "Iteration 00425    Loss -0.039373  0.001198\n",
            "Iteration 00426    Loss -0.039344  0.001068\n",
            "Iteration 00427    Loss -0.039404  0.001162\n",
            "Iteration 00428    Loss -0.039514  0.001121\n",
            "Iteration 00429    Loss -0.039523  0.001130\n",
            "Iteration 00430    Loss -0.039520  0.001073\n",
            "Iteration 00431    Loss -0.039610  0.001135\n",
            "Iteration 00432    Loss -0.039707  0.001167\n",
            "Iteration 00433    Loss -0.039695  0.001115\n",
            "Iteration 00434    Loss -0.039640  0.001154\n",
            "Iteration 00435    Loss -0.039778  0.001052\n",
            "Iteration 00436    Loss -0.039728  0.001085\n",
            "Iteration 00437    Loss -0.039870  0.001182\n",
            "Iteration 00438    Loss -0.039865  0.001113\n",
            "Iteration 00439    Loss -0.039962  0.001122\n",
            "Iteration 00440    Loss -0.039923  0.001146\n",
            "Iteration 00441    Loss -0.040020  0.001159\n",
            "Iteration 00442    Loss -0.040096  0.001121\n",
            "Iteration 00443    Loss -0.040125  0.001140\n",
            "Iteration 00444    Loss -0.040125  0.001196\n",
            "Iteration 00445    Loss -0.040133  0.001152\n",
            "Iteration 00446    Loss -0.040261  0.001146\n",
            "Iteration 00447    Loss -0.040281  0.001184\n",
            "Iteration 00448    Loss -0.040338  0.001158\n",
            "Iteration 00449    Loss -0.040251  0.001101\n",
            "Iteration 00450    Loss -0.040379  0.001185\n",
            "Iteration 00451    Loss -0.040320  0.001143\n",
            "Iteration 00452    Loss -0.040367  0.001177\n",
            "Iteration 00453    Loss -0.040436  0.001096\n",
            "Iteration 00454    Loss -0.040325  0.001248\n",
            "Iteration 00455    Loss -0.040482  0.001061\n",
            "Iteration 00456    Loss -0.040486  0.001206\n",
            "Iteration 00457    Loss -0.040635  0.001188\n",
            "Iteration 00458    Loss -0.040676  0.001154\n",
            "Iteration 00459    Loss -0.040710  0.001127\n",
            "Iteration 00460    Loss -0.040700  0.001090\n",
            "Iteration 00461    Loss -0.040715  0.001139\n",
            "Iteration 00462    Loss -0.040841  0.001135\n",
            "Iteration 00463    Loss -0.040821  0.001140\n",
            "Iteration 00464    Loss -0.040904  0.001178\n",
            "Iteration 00465    Loss -0.040861  0.001109\n",
            "Iteration 00466    Loss -0.040909  0.001152\n",
            "Iteration 00467    Loss -0.041004  0.001151\n",
            "Iteration 00468    Loss -0.041037  0.001150\n",
            "Iteration 00469    Loss -0.041037  0.001183\n",
            "Iteration 00470    Loss -0.041081  0.001171\n",
            "Iteration 00471    Loss -0.041171  0.001211\n",
            "Iteration 00472    Loss -0.041195  0.001183\n",
            "Iteration 00473    Loss -0.041223  0.001256\n",
            "Iteration 00474    Loss -0.041277  0.001139\n",
            "Iteration 00475    Loss -0.041263  0.001224\n",
            "Iteration 00476    Loss -0.041253  0.001160\n",
            "Iteration 00477    Loss -0.041210  0.001228\n",
            "Iteration 00478    Loss -0.041310  0.001197\n",
            "Iteration 00479    Loss -0.041395  0.001209\n",
            "Iteration 00480    Loss -0.041361  0.001199\n",
            "Iteration 00481    Loss -0.041456  0.001216\n",
            "Iteration 00482    Loss -0.041471  0.001165\n",
            "Iteration 00483    Loss -0.041511  0.001190\n",
            "Iteration 00484    Loss -0.041562  0.001217\n",
            "Iteration 00485    Loss -0.041580  0.001170\n",
            "Iteration 00486    Loss -0.041637  0.001229\n",
            "Iteration 00487    Loss -0.041644  0.001174\n",
            "Iteration 00488    Loss -0.041643  0.001248\n",
            "Iteration 00489    Loss -0.041702  0.001128\n",
            "Iteration 00490    Loss -0.041732  0.001227\n",
            "Iteration 00491    Loss -0.041804  0.001212\n",
            "Iteration 00492    Loss -0.041758  0.001227\n",
            "Iteration 00493    Loss -0.041791  0.001221\n",
            "Iteration 00494    Loss -0.041760  0.001197\n",
            "Iteration 00495    Loss -0.041679  0.001206\n",
            "Iteration 00496    Loss -0.041800  0.001247\n",
            "Iteration 00497    Loss -0.041785  0.001222\n",
            "Iteration 00498    Loss -0.041665  0.001253\n",
            "Iteration 00499    Loss -0.041649  0.001054\n",
            " 98% 492/500 [4:40:45<04:25, 33.24s/it]/content/SOTS/outdoor/hazy/0253_1_0.16.jpg\n",
            "Iteration 00000    Loss 0.469160  0.000016\n",
            "Iteration 00001    Loss 7.524885  0.000007\n",
            "Iteration 00002    Loss 0.705666  0.000004\n",
            "Iteration 00003    Loss 0.553471  0.000002\n",
            "Iteration 00004    Loss 0.243534  0.000002\n",
            "Iteration 00005    Loss 0.396379  0.000002\n",
            "Iteration 00006    Loss 0.193200  0.000001\n",
            "Iteration 00007    Loss 0.207898  0.000001\n",
            "Iteration 00008    Loss 0.147031  0.000001\n",
            "Iteration 00009    Loss 0.154115  0.000001\n",
            "Iteration 00010    Loss 0.132387  0.000001\n",
            "Iteration 00011    Loss 0.103184  0.000001\n",
            "Iteration 00012    Loss 0.104992  0.000001\n",
            "Iteration 00013    Loss 0.096719  0.000001\n",
            "Iteration 00014    Loss 0.085983  0.000001\n",
            "Iteration 00015    Loss 0.078543  0.000001\n",
            "Iteration 00016    Loss 0.073155  0.000001\n",
            "Iteration 00017    Loss 0.070322  0.000001\n",
            "Iteration 00018    Loss 0.067239  0.000001\n",
            "Iteration 00019    Loss 0.064916  0.000002\n",
            "Iteration 00020    Loss 0.062048  0.000002\n",
            "Iteration 00021    Loss 0.059787  0.000002\n",
            "Iteration 00022    Loss 0.057823  0.000002\n",
            "Iteration 00023    Loss 0.055715  0.000002\n",
            "Iteration 00024    Loss 0.053568  0.000002\n",
            "Iteration 00025    Loss 0.051731  0.000003\n",
            "Iteration 00026    Loss 0.050437  0.000002\n",
            "Iteration 00027    Loss 0.048962  0.000003\n",
            "Iteration 00028    Loss 0.047511  0.000003\n",
            "Iteration 00029    Loss 0.046180  0.000003\n",
            "Iteration 00030    Loss 0.045074  0.000003\n",
            "Iteration 00031    Loss 0.043981  0.000004\n",
            "Iteration 00032    Loss 0.043075  0.000004\n",
            "Iteration 00033    Loss 0.042051  0.000005\n",
            "Iteration 00034    Loss 0.040875  0.000005\n",
            "Iteration 00035    Loss 0.039894  0.000005\n",
            "Iteration 00036    Loss 0.039076  0.000005\n",
            "Iteration 00037    Loss 0.038050  0.000006\n",
            "Iteration 00038    Loss 0.037230  0.000006\n",
            "Iteration 00039    Loss 0.036264  0.000006\n",
            "Iteration 00040    Loss 0.035418  0.000006\n",
            "Iteration 00041    Loss 0.034509  0.000006\n",
            "Iteration 00042    Loss 0.033655  0.000007\n",
            "Iteration 00043    Loss 0.032776  0.000007\n",
            "Iteration 00044    Loss 0.032277  0.000008\n",
            "Iteration 00045    Loss 0.031339  0.000008\n",
            "Iteration 00046    Loss 0.030701  0.000008\n",
            "Iteration 00047    Loss 0.030059  0.000009\n",
            "Iteration 00048    Loss 0.029318  0.000009\n",
            "Iteration 00049    Loss 0.028700  0.000010\n",
            "Iteration 00050    Loss 0.028063  0.000010\n",
            "Iteration 00051    Loss 0.027417  0.000010\n",
            "Iteration 00052    Loss 0.026780  0.000010\n",
            "Iteration 00053    Loss 0.026155  0.000010\n",
            "Iteration 00054    Loss 0.025602  0.000011\n",
            "Iteration 00055    Loss 0.025041  0.000012\n",
            "Iteration 00056    Loss 0.024505  0.000013\n",
            "Iteration 00057    Loss 0.023929  0.000013\n",
            "Iteration 00058    Loss 0.023441  0.000013\n",
            "Iteration 00059    Loss 0.022874  0.000014\n",
            "Iteration 00060    Loss 0.022300  0.000014\n",
            "Iteration 00061    Loss 0.021742  0.000015\n",
            "Iteration 00062    Loss 0.021209  0.000016\n",
            "Iteration 00063    Loss 0.020760  0.000015\n",
            "Iteration 00064    Loss 0.020240  0.000017\n",
            "Iteration 00065    Loss 0.019734  0.000016\n",
            "Iteration 00066    Loss 0.019237  0.000019\n",
            "Iteration 00067    Loss 0.018772  0.000021\n",
            "Iteration 00068    Loss 0.018352  0.000019\n",
            "Iteration 00069    Loss 0.017879  0.000020\n",
            "Iteration 00070    Loss 0.017395  0.000021\n",
            "Iteration 00071    Loss 0.016920  0.000021\n",
            "Iteration 00072    Loss 0.016407  0.000022\n",
            "Iteration 00073    Loss 0.015997  0.000022\n",
            "Iteration 00074    Loss 0.015522  0.000025\n",
            "Iteration 00075    Loss 0.015142  0.000024\n",
            "Iteration 00076    Loss 0.014673  0.000025\n",
            "Iteration 00077    Loss 0.014260  0.000027\n",
            "Iteration 00078    Loss 0.013802  0.000025\n",
            "Iteration 00079    Loss 0.013348  0.000026\n",
            "Iteration 00080    Loss 0.012917  0.000029\n",
            "Iteration 00081    Loss 0.012510  0.000029\n",
            "Iteration 00082    Loss 0.012085  0.000029\n",
            "Iteration 00083    Loss 0.011631  0.000031\n",
            "Iteration 00084    Loss 0.011079  0.000033\n",
            "Iteration 00085    Loss 0.010656  0.000032\n",
            "Iteration 00086    Loss 0.010284  0.000033\n",
            "Iteration 00087    Loss 0.009926  0.000037\n",
            "Iteration 00088    Loss 0.009494  0.000032\n",
            "Iteration 00089    Loss 0.009139  0.000036\n",
            "Iteration 00090    Loss 0.008734  0.000038\n",
            "Iteration 00091    Loss 0.008368  0.000041\n",
            "Iteration 00092    Loss 0.008007  0.000039\n",
            "Iteration 00093    Loss 0.007671  0.000042\n",
            "Iteration 00094    Loss 0.007311  0.000038\n",
            "Iteration 00095    Loss 0.006939  0.000043\n",
            "Iteration 00096    Loss 0.006599  0.000041\n",
            "Iteration 00097    Loss 0.006224  0.000044\n",
            "Iteration 00098    Loss 0.005876  0.000051\n",
            "Iteration 00099    Loss 0.005594  0.000041\n",
            "Iteration 00100    Loss 0.005222  0.000044\n",
            "Iteration 00101    Loss 0.004840  0.000046\n",
            "Iteration 00102    Loss 0.004594  0.000045\n",
            "Iteration 00103    Loss 0.004217  0.000052\n",
            "Iteration 00104    Loss 0.003922  0.000049\n",
            "Iteration 00105    Loss 0.003620  0.000052\n",
            "Iteration 00106    Loss 0.003361  0.000055\n",
            "Iteration 00107    Loss 0.003057  0.000049\n",
            "Iteration 00108    Loss 0.002724  0.000052\n",
            "Iteration 00109    Loss 0.002406  0.000059\n",
            "Iteration 00110    Loss 0.002105  0.000051\n",
            "Iteration 00111    Loss 0.001760  0.000056\n",
            "Iteration 00112    Loss 0.001442  0.000055\n",
            "Iteration 00113    Loss 0.001165  0.000057\n",
            "Iteration 00114    Loss 0.000857  0.000059\n",
            "Iteration 00115    Loss 0.000545  0.000055\n",
            "Iteration 00116    Loss 0.000243  0.000062\n",
            "Iteration 00117    Loss -0.000084  0.000059\n",
            "Iteration 00118    Loss -0.000356  0.000061\n",
            "Iteration 00119    Loss -0.000645  0.000065\n",
            "Iteration 00120    Loss -0.000923  0.000064\n",
            "Iteration 00121    Loss -0.001183  0.000064\n",
            "Iteration 00122    Loss -0.001468  0.000065\n",
            "Iteration 00123    Loss -0.001703  0.000062\n",
            "Iteration 00124    Loss -0.002023  0.000069\n",
            "Iteration 00125    Loss -0.002287  0.000065\n",
            "Iteration 00126    Loss -0.002589  0.000072\n",
            "Iteration 00127    Loss -0.002730  0.000059\n",
            "Iteration 00128    Loss -0.003014  0.000073\n",
            "Iteration 00129    Loss -0.003235  0.000060\n",
            "Iteration 00130    Loss -0.003551  0.000065\n",
            "Iteration 00131    Loss -0.003822  0.000069\n",
            "Iteration 00132    Loss -0.004072  0.000062\n",
            "Iteration 00133    Loss -0.004352  0.000066\n",
            "Iteration 00134    Loss -0.004591  0.000070\n",
            "Iteration 00135    Loss -0.004849  0.000071\n",
            "Iteration 00136    Loss -0.005133  0.000073\n",
            "Iteration 00137    Loss -0.005330  0.000069\n",
            "Iteration 00138    Loss -0.005633  0.000070\n",
            "Iteration 00139    Loss -0.005844  0.000073\n",
            "Iteration 00140    Loss -0.006096  0.000071\n",
            "Iteration 00141    Loss -0.006358  0.000072\n",
            "Iteration 00142    Loss -0.006604  0.000080\n",
            "Iteration 00143    Loss -0.006795  0.000067\n",
            "Iteration 00144    Loss -0.007056  0.000078\n",
            "Iteration 00145    Loss -0.007236  0.000070\n",
            "Iteration 00146    Loss -0.007444  0.000084\n",
            "Iteration 00147    Loss -0.007648  0.000059\n",
            "Iteration 00148    Loss -0.007959  0.000069\n",
            "Iteration 00149    Loss -0.008114  0.000082\n",
            "Iteration 00150    Loss -0.008377  0.000066\n",
            "Iteration 00151    Loss -0.008601  0.000071\n",
            "Iteration 00152    Loss -0.008819  0.000075\n",
            "Iteration 00153    Loss -0.009026  0.000071\n",
            "Iteration 00154    Loss -0.009255  0.000080\n",
            "Iteration 00155    Loss -0.009515  0.000076\n",
            "Iteration 00156    Loss -0.009691  0.000071\n",
            "Iteration 00157    Loss -0.009899  0.000081\n",
            "Iteration 00158    Loss -0.010139  0.000078\n",
            "Iteration 00159    Loss -0.010322  0.000079\n",
            "Iteration 00160    Loss -0.010509  0.000081\n",
            "Iteration 00161    Loss -0.010694  0.000071\n",
            "Iteration 00162    Loss -0.010949  0.000089\n",
            "Iteration 00163    Loss -0.011174  0.000071\n",
            "Iteration 00164    Loss -0.011334  0.000085\n",
            "Iteration 00165    Loss -0.011581  0.000083\n",
            "Iteration 00166    Loss -0.011776  0.000080\n",
            "Iteration 00167    Loss -0.011976  0.000084\n",
            "Iteration 00168    Loss -0.012128  0.000074\n",
            "Iteration 00169    Loss -0.012425  0.000085\n",
            "Iteration 00170    Loss -0.012640  0.000085\n",
            "Iteration 00171    Loss -0.012859  0.000082\n",
            "Iteration 00172    Loss -0.012996  0.000089\n",
            "Iteration 00173    Loss -0.013197  0.000073\n",
            "Iteration 00174    Loss -0.013377  0.000087\n",
            "Iteration 00175    Loss -0.013585  0.000085\n",
            "Iteration 00176    Loss -0.013793  0.000091\n",
            "Iteration 00177    Loss -0.013991  0.000086\n",
            "Iteration 00178    Loss -0.014154  0.000086\n",
            "Iteration 00179    Loss -0.014324  0.000091\n",
            "Iteration 00180    Loss -0.014524  0.000081\n",
            "Iteration 00181    Loss -0.014735  0.000093\n",
            "Iteration 00182    Loss -0.014858  0.000079\n",
            "Iteration 00183    Loss -0.015017  0.000107\n",
            "Iteration 00184    Loss -0.015125  0.000061\n",
            "Iteration 00185    Loss -0.015442  0.000082\n",
            "Iteration 00186    Loss -0.015518  0.000104\n",
            "Iteration 00187    Loss -0.015629  0.000063\n",
            "Iteration 00188    Loss -0.015886  0.000072\n",
            "Iteration 00189    Loss -0.016068  0.000090\n",
            "Iteration 00190    Loss -0.016227  0.000080\n",
            "Iteration 00191    Loss -0.016438  0.000080\n",
            "Iteration 00192    Loss -0.016575  0.000089\n",
            "Iteration 00193    Loss -0.016787  0.000088\n",
            "Iteration 00194    Loss -0.016962  0.000081\n",
            "Iteration 00195    Loss -0.017140  0.000082\n",
            "Iteration 00196    Loss -0.017304  0.000085\n",
            "Iteration 00197    Loss -0.017523  0.000086\n",
            "Iteration 00198    Loss -0.017652  0.000088\n",
            "Iteration 00199    Loss -0.017816  0.000092\n",
            "Iteration 00200    Loss -0.018011  0.000093\n",
            "Iteration 00201    Loss -0.018170  0.000091\n",
            "Iteration 00202    Loss -0.018308  0.000096\n",
            "Iteration 00203    Loss -0.018466  0.000090\n",
            "Iteration 00204    Loss -0.018646  0.000092\n",
            "Iteration 00205    Loss -0.018802  0.000098\n",
            "Iteration 00206    Loss -0.018977  0.000096\n",
            "Iteration 00207    Loss -0.019139  0.000099\n",
            "Iteration 00208    Loss -0.019312  0.000096\n",
            "Iteration 00209    Loss -0.019450  0.000099\n",
            "Iteration 00210    Loss -0.019601  0.000105\n",
            "Iteration 00211    Loss -0.019770  0.000097\n",
            "Iteration 00212    Loss -0.019920  0.000107\n",
            "Iteration 00213    Loss -0.020079  0.000097\n",
            "Iteration 00214    Loss -0.020240  0.000107\n",
            "Iteration 00215    Loss -0.020406  0.000099\n",
            "Iteration 00216    Loss -0.020542  0.000114\n",
            "Iteration 00217    Loss -0.020661  0.000100\n",
            "Iteration 00218    Loss -0.020850  0.000114\n",
            "Iteration 00219    Loss -0.020961  0.000095\n",
            "Iteration 00220    Loss -0.021052  0.000120\n",
            "Iteration 00221    Loss -0.021211  0.000096\n",
            "Iteration 00222    Loss -0.021320  0.000123\n",
            "Iteration 00223    Loss -0.021471  0.000095\n",
            "Iteration 00224    Loss -0.021647  0.000116\n",
            "Iteration 00225    Loss -0.021828  0.000108\n",
            "Iteration 00226    Loss -0.021928  0.000110\n",
            "Iteration 00227    Loss -0.022071  0.000117\n",
            "Iteration 00228    Loss -0.022223  0.000104\n",
            "Iteration 00229    Loss -0.022400  0.000113\n",
            "Iteration 00230    Loss -0.022543  0.000116\n",
            "Iteration 00231    Loss -0.022633  0.000110\n",
            "Iteration 00232    Loss -0.022784  0.000126\n",
            "Iteration 00233    Loss -0.022930  0.000109\n",
            "Iteration 00234    Loss -0.023037  0.000134\n",
            "Iteration 00235    Loss -0.022956  0.000084\n",
            "Iteration 00236    Loss -0.023164  0.000108\n",
            "Iteration 00237    Loss -0.023381  0.000117\n",
            "Iteration 00238    Loss -0.023467  0.000094\n",
            "Iteration 00239    Loss -0.023646  0.000108\n",
            "Iteration 00240    Loss -0.023763  0.000106\n",
            "Iteration 00241    Loss -0.023928  0.000100\n",
            "Iteration 00242    Loss -0.024053  0.000105\n",
            "Iteration 00243    Loss -0.024200  0.000111\n",
            "Iteration 00244    Loss -0.024300  0.000104\n",
            "Iteration 00245    Loss -0.024412  0.000110\n",
            "Iteration 00246    Loss -0.024537  0.000110\n",
            "Iteration 00247    Loss -0.024713  0.000117\n",
            "Iteration 00248    Loss -0.024728  0.000109\n",
            "Iteration 00249    Loss -0.024960  0.000122\n",
            "Iteration 00250    Loss -0.025059  0.000118\n",
            "Iteration 00251    Loss -0.025246  0.000115\n",
            "Iteration 00252    Loss -0.025285  0.000115\n",
            "Iteration 00253    Loss -0.025412  0.000125\n",
            "Iteration 00254    Loss -0.025586  0.000115\n",
            "Iteration 00255    Loss -0.025715  0.000131\n",
            "Iteration 00256    Loss -0.025807  0.000123\n",
            "Iteration 00257    Loss -0.025920  0.000136\n",
            "Iteration 00258    Loss -0.026007  0.000116\n",
            "Iteration 00259    Loss -0.026188  0.000142\n",
            "Iteration 00260    Loss -0.026268  0.000116\n",
            "Iteration 00261    Loss -0.026377  0.000144\n",
            "Iteration 00262    Loss -0.026470  0.000110\n",
            "Iteration 00263    Loss -0.026616  0.000140\n",
            "Iteration 00264    Loss -0.026762  0.000113\n",
            "Iteration 00265    Loss -0.026893  0.000131\n",
            "Iteration 00266    Loss -0.026973  0.000146\n",
            "Iteration 00267    Loss -0.027024  0.000113\n",
            "Iteration 00268    Loss -0.027171  0.000130\n",
            "Iteration 00269    Loss -0.027310  0.000134\n",
            "Iteration 00270    Loss -0.027361  0.000127\n",
            "Iteration 00271    Loss -0.027497  0.000147\n",
            "Iteration 00272    Loss -0.027601  0.000116\n",
            "Iteration 00273    Loss -0.027713  0.000144\n",
            "Iteration 00274    Loss -0.027869  0.000133\n",
            "Iteration 00275    Loss -0.027980  0.000128\n",
            "Iteration 00276    Loss -0.028026  0.000152\n",
            "Iteration 00277    Loss -0.028085  0.000128\n",
            "Iteration 00278    Loss -0.028234  0.000142\n",
            "Iteration 00279    Loss -0.028351  0.000152\n",
            "Iteration 00280    Loss -0.028507  0.000134\n",
            "Iteration 00281    Loss -0.028592  0.000150\n",
            "Iteration 00282    Loss -0.028664  0.000137\n",
            "Iteration 00283    Loss -0.028751  0.000156\n",
            "Iteration 00284    Loss -0.028886  0.000131\n",
            "Iteration 00285    Loss -0.029036  0.000149\n",
            "Iteration 00286    Loss -0.029104  0.000156\n",
            "Iteration 00287    Loss -0.029128  0.000132\n",
            "Iteration 00288    Loss -0.029269  0.000155\n",
            "Iteration 00289    Loss -0.029391  0.000147\n",
            "Iteration 00290    Loss -0.029518  0.000157\n",
            "Iteration 00291    Loss -0.029555  0.000139\n",
            "Iteration 00292    Loss -0.029637  0.000158\n",
            "Iteration 00293    Loss -0.029765  0.000146\n",
            "Iteration 00294    Loss -0.029913  0.000163\n",
            "Iteration 00295    Loss -0.029901  0.000143\n",
            "Iteration 00296    Loss -0.029989  0.000170\n",
            "Iteration 00297    Loss -0.030146  0.000132\n",
            "Iteration 00298    Loss -0.030274  0.000160\n",
            "Iteration 00299    Loss -0.030258  0.000150\n",
            "Iteration 00300    Loss -0.030399  0.000156\n",
            "Iteration 00301    Loss -0.030488  0.000148\n",
            "Iteration 00302    Loss -0.030648  0.000150\n",
            "Iteration 00303    Loss -0.030675  0.000151\n",
            "Iteration 00304    Loss -0.030826  0.000161\n",
            "Iteration 00305    Loss -0.030823  0.000148\n",
            "Iteration 00306    Loss -0.030902  0.000173\n",
            "Iteration 00307    Loss -0.031071  0.000141\n",
            "Iteration 00308    Loss -0.031112  0.000171\n",
            "Iteration 00309    Loss -0.031271  0.000157\n",
            "Iteration 00310    Loss -0.031300  0.000163\n",
            "Iteration 00311    Loss -0.031440  0.000155\n",
            "Iteration 00312    Loss -0.031496  0.000161\n",
            "Iteration 00313    Loss -0.031602  0.000176\n",
            "Iteration 00314    Loss -0.031682  0.000152\n",
            "Iteration 00315    Loss -0.031681  0.000191\n",
            "Iteration 00316    Loss -0.031744  0.000135\n",
            "Iteration 00317    Loss -0.031879  0.000177\n",
            "Iteration 00318    Loss -0.031979  0.000159\n",
            "Iteration 00319    Loss -0.032126  0.000157\n",
            "Iteration 00320    Loss -0.032058  0.000169\n",
            "Iteration 00321    Loss -0.032326  0.000158\n",
            "Iteration 00322    Loss -0.032362  0.000164\n",
            "Iteration 00323    Loss -0.032412  0.000171\n",
            "Iteration 00324    Loss -0.032507  0.000156\n",
            "Iteration 00325    Loss -0.032611  0.000178\n",
            "Iteration 00326    Loss -0.032638  0.000165\n",
            "Iteration 00327    Loss -0.032723  0.000179\n",
            "Iteration 00328    Loss -0.032842  0.000173\n",
            "Iteration 00329    Loss -0.032924  0.000179\n",
            "Iteration 00330    Loss -0.033037  0.000170\n",
            "Iteration 00331    Loss -0.033047  0.000191\n",
            "Iteration 00332    Loss -0.033169  0.000162\n",
            "Iteration 00333    Loss -0.033207  0.000194\n",
            "Iteration 00334    Loss -0.033239  0.000168\n",
            "Iteration 00335    Loss -0.033400  0.000183\n",
            "Iteration 00336    Loss -0.033408  0.000174\n",
            "Iteration 00337    Loss -0.033442  0.000185\n",
            "Iteration 00338    Loss -0.033460  0.000179\n",
            "Iteration 00339    Loss -0.033720  0.000183\n",
            "Iteration 00340    Loss -0.033770  0.000166\n",
            "Iteration 00341    Loss -0.033742  0.000202\n",
            "Iteration 00342    Loss -0.033842  0.000171\n",
            "Iteration 00343    Loss -0.033879  0.000181\n",
            "Iteration 00344    Loss -0.033985  0.000172\n",
            "Iteration 00345    Loss -0.034029  0.000197\n",
            "Iteration 00346    Loss -0.034063  0.000171\n",
            "Iteration 00347    Loss -0.034164  0.000185\n",
            "Iteration 00348    Loss -0.034295  0.000196\n",
            "Iteration 00349    Loss -0.034334  0.000175\n",
            "Iteration 00350    Loss -0.034494  0.000197\n",
            "Iteration 00351    Loss -0.034475  0.000183\n",
            "Iteration 00352    Loss -0.034596  0.000202\n",
            "Iteration 00353    Loss -0.034652  0.000170\n",
            "Iteration 00354    Loss -0.034725  0.000207\n",
            "Iteration 00355    Loss -0.034779  0.000178\n",
            "Iteration 00356    Loss -0.034905  0.000193\n",
            "Iteration 00357    Loss -0.034973  0.000195\n",
            "Iteration 00358    Loss -0.034979  0.000187\n",
            "Iteration 00359    Loss -0.035093  0.000212\n",
            "Iteration 00360    Loss -0.035103  0.000174\n",
            "Iteration 00361    Loss -0.035105  0.000214\n",
            "Iteration 00362    Loss -0.035178  0.000175\n",
            "Iteration 00363    Loss -0.035416  0.000209\n",
            "Iteration 00364    Loss -0.035433  0.000205\n",
            "Iteration 00365    Loss -0.035405  0.000169\n",
            "Iteration 00366    Loss -0.035571  0.000208\n",
            "Iteration 00367    Loss -0.035592  0.000190\n",
            "Iteration 00368    Loss -0.035665  0.000192\n",
            "Iteration 00369    Loss -0.035689  0.000207\n",
            "Iteration 00370    Loss -0.035735  0.000187\n",
            "Iteration 00371    Loss -0.035839  0.000189\n",
            "Iteration 00372    Loss -0.035868  0.000213\n",
            "Iteration 00373    Loss -0.035971  0.000197\n",
            "Iteration 00374    Loss -0.036052  0.000212\n",
            "Iteration 00375    Loss -0.036048  0.000198\n",
            "Iteration 00376    Loss -0.036224  0.000208\n",
            "Iteration 00377    Loss -0.036153  0.000217\n",
            "Iteration 00378    Loss -0.036224  0.000225\n",
            "Iteration 00379    Loss -0.036305  0.000200\n",
            "Iteration 00380    Loss -0.036510  0.000213\n",
            "Iteration 00381    Loss -0.036508  0.000224\n",
            "Iteration 00382    Loss -0.036598  0.000216\n",
            "Iteration 00383    Loss -0.036629  0.000221\n",
            "Iteration 00384    Loss -0.036641  0.000224\n",
            "Iteration 00385    Loss -0.036674  0.000212\n",
            "Iteration 00386    Loss -0.036723  0.000229\n",
            "Iteration 00387    Loss -0.036853  0.000211\n",
            "Iteration 00388    Loss -0.036856  0.000235\n",
            "Iteration 00389    Loss -0.036741  0.000187\n",
            "Iteration 00390    Loss -0.036840  0.000233\n",
            "Iteration 00391    Loss -0.036948  0.000168\n",
            "Iteration 00392    Loss -0.037062  0.000186\n",
            "Iteration 00393    Loss -0.037103  0.000215\n",
            "Iteration 00394    Loss -0.037165  0.000186\n",
            "Iteration 00395    Loss -0.037197  0.000196\n",
            "Iteration 00396    Loss -0.037288  0.000212\n",
            "Iteration 00397    Loss -0.037274  0.000186\n",
            "Iteration 00398    Loss -0.037361  0.000200\n",
            "Iteration 00399    Loss -0.037425  0.000204\n",
            "Iteration 00400    Loss -0.037497  0.000215\n",
            "Iteration 00401    Loss -0.037498  0.000207\n",
            "Iteration 00402    Loss -0.037625  0.000205\n",
            "Iteration 00403    Loss -0.037625  0.000215\n",
            "Iteration 00404    Loss -0.037710  0.000215\n",
            "Iteration 00405    Loss -0.037546  0.000218\n",
            "Iteration 00406    Loss -0.037871  0.000230\n",
            "Iteration 00407    Loss -0.037728  0.000213\n",
            "Iteration 00408    Loss -0.037962  0.000236\n",
            "Iteration 00409    Loss -0.037940  0.000208\n",
            "Iteration 00410    Loss -0.037985  0.000235\n",
            "Iteration 00411    Loss -0.037999  0.000219\n",
            "Iteration 00412    Loss -0.038110  0.000238\n",
            "Iteration 00413    Loss -0.038217  0.000235\n",
            "Iteration 00414    Loss -0.038178  0.000216\n",
            "Iteration 00415    Loss -0.038237  0.000247\n",
            "Iteration 00416    Loss -0.038339  0.000212\n",
            "Iteration 00417    Loss -0.038438  0.000247\n",
            "Iteration 00418    Loss -0.038420  0.000220\n",
            "Iteration 00419    Loss -0.038377  0.000238\n",
            "Iteration 00420    Loss -0.038354  0.000235\n",
            "Iteration 00421    Loss -0.038538  0.000232\n",
            "Iteration 00422    Loss -0.038629  0.000237\n",
            "Iteration 00423    Loss -0.038695  0.000218\n",
            "Iteration 00424    Loss -0.038711  0.000251\n",
            "Iteration 00425    Loss -0.038704  0.000238\n",
            "Iteration 00426    Loss -0.038788  0.000239\n",
            "Iteration 00427    Loss -0.038902  0.000249\n",
            "Iteration 00428    Loss -0.038919  0.000229\n",
            "Iteration 00429    Loss -0.038996  0.000243\n",
            "Iteration 00430    Loss -0.038984  0.000236\n",
            "Iteration 00431    Loss -0.039005  0.000260\n",
            "Iteration 00432    Loss -0.039053  0.000224\n",
            "Iteration 00433    Loss -0.039135  0.000262\n",
            "Iteration 00434    Loss -0.039043  0.000209\n",
            "Iteration 00435    Loss -0.039217  0.000249\n",
            "Iteration 00436    Loss -0.039200  0.000238\n",
            "Iteration 00437    Loss -0.039308  0.000235\n",
            "Iteration 00438    Loss -0.039227  0.000237\n",
            "Iteration 00439    Loss -0.039392  0.000237\n",
            "Iteration 00440    Loss -0.039479  0.000248\n",
            "Iteration 00441    Loss -0.039570  0.000239\n",
            "Iteration 00442    Loss -0.039508  0.000256\n",
            "Iteration 00443    Loss -0.039621  0.000245\n",
            "Iteration 00444    Loss -0.039554  0.000257\n",
            "Iteration 00445    Loss -0.039644  0.000255\n",
            "Iteration 00446    Loss -0.039645  0.000257\n",
            "Iteration 00447    Loss -0.039776  0.000247\n",
            "Iteration 00448    Loss -0.039839  0.000270\n",
            "Iteration 00449    Loss -0.039818  0.000237\n",
            "Iteration 00450    Loss -0.039651  0.000282\n",
            "Iteration 00451    Loss -0.039668  0.000193\n",
            "Iteration 00452    Loss -0.039839  0.000239\n",
            "Iteration 00453    Loss -0.039882  0.000242\n",
            "Iteration 00454    Loss -0.039866  0.000208\n",
            "Iteration 00455    Loss -0.039927  0.000211\n",
            "Iteration 00456    Loss -0.040068  0.000244\n",
            "Iteration 00457    Loss -0.040143  0.000210\n",
            "Iteration 00458    Loss -0.040197  0.000220\n",
            "Iteration 00459    Loss -0.040197  0.000235\n",
            "Iteration 00460    Loss -0.040257  0.000240\n",
            "Iteration 00461    Loss -0.040299  0.000220\n",
            "Iteration 00462    Loss -0.040241  0.000229\n",
            "Iteration 00463    Loss -0.040396  0.000257\n",
            "Iteration 00464    Loss -0.040387  0.000230\n",
            "Iteration 00465    Loss -0.040332  0.000257\n",
            "Iteration 00466    Loss -0.040412  0.000233\n",
            "Iteration 00467    Loss -0.040525  0.000244\n",
            "Iteration 00468    Loss -0.040484  0.000231\n",
            "Iteration 00469    Loss -0.040570  0.000238\n",
            "Iteration 00470    Loss -0.040621  0.000259\n",
            "Iteration 00471    Loss -0.040679  0.000231\n",
            "Iteration 00472    Loss -0.040719  0.000254\n",
            "Iteration 00473    Loss -0.040712  0.000252\n",
            "Iteration 00474    Loss -0.040806  0.000258\n",
            "Iteration 00475    Loss -0.040746  0.000241\n",
            "Iteration 00476    Loss -0.040860  0.000255\n",
            "Iteration 00477    Loss -0.040837  0.000261\n",
            "Iteration 00478    Loss -0.040806  0.000252\n",
            "Iteration 00479    Loss -0.040981  0.000251\n",
            "Iteration 00480    Loss -0.040969  0.000273\n",
            "Iteration 00481    Loss -0.041007  0.000250\n",
            "Iteration 00482    Loss -0.041008  0.000272\n",
            "Iteration 00483    Loss -0.041056  0.000252\n",
            "Iteration 00484    Loss -0.041127  0.000272\n",
            "Iteration 00485    Loss -0.041198  0.000238\n",
            "Iteration 00486    Loss -0.041164  0.000275\n",
            "Iteration 00487    Loss -0.041199  0.000251\n",
            "Iteration 00488    Loss -0.041292  0.000257\n",
            "Iteration 00489    Loss -0.041332  0.000272\n",
            "Iteration 00490    Loss -0.041234  0.000255\n",
            "Iteration 00491    Loss -0.041388  0.000277\n",
            "Iteration 00492    Loss -0.041361  0.000247\n",
            "Iteration 00493    Loss -0.041367  0.000291\n",
            "Iteration 00494    Loss -0.041309  0.000215\n",
            "Iteration 00495    Loss -0.041439  0.000268\n",
            "Iteration 00496    Loss -0.041509  0.000252\n",
            "Iteration 00497    Loss -0.041462  0.000224\n",
            "Iteration 00498    Loss -0.041510  0.000259\n",
            "Iteration 00499    Loss -0.041580  0.000244\n",
            " 99% 493/500 [4:41:18<03:50, 32.97s/it]/content/SOTS/outdoor/hazy/1010_0.85_0.08.jpg\n",
            "Iteration 00000    Loss 0.488560  0.000049\n",
            "Iteration 00001    Loss 30.316833  0.000020\n",
            "Iteration 00002    Loss 0.539464  0.000009\n",
            "Iteration 00003    Loss 0.438024  0.000004\n",
            "Iteration 00004    Loss 0.265176  0.000002\n",
            "Iteration 00005    Loss 0.355238  0.000001\n",
            "Iteration 00006    Loss 0.225438  0.000000\n",
            "Iteration 00007    Loss 0.176415  0.000000\n",
            "Iteration 00008    Loss 0.157030  0.000000\n",
            "Iteration 00009    Loss 0.142073  0.000000\n",
            "Iteration 00010    Loss 0.129686  0.000000\n",
            "Iteration 00011    Loss 0.120423  0.000000\n",
            "Iteration 00012    Loss 0.113788  0.000000\n",
            "Iteration 00013    Loss 0.107082  0.000000\n",
            "Iteration 00014    Loss 0.101179  0.000000\n",
            "Iteration 00015    Loss 0.097462  0.000000\n",
            "Iteration 00016    Loss 0.094201  0.000000\n",
            "Iteration 00017    Loss 0.090966  0.000000\n",
            "Iteration 00018    Loss 0.088296  0.000000\n",
            "Iteration 00019    Loss 0.085609  0.000000\n",
            "Iteration 00020    Loss 0.083701  0.000000\n",
            "Iteration 00021    Loss 0.082046  0.000000\n",
            "Iteration 00022    Loss 0.080370  0.000000\n",
            "Iteration 00023    Loss 0.078946  0.000000\n",
            "Iteration 00024    Loss 0.077356  0.000000\n",
            "Iteration 00025    Loss 0.075774  0.000000\n",
            "Iteration 00026    Loss 0.074406  0.000000\n",
            "Iteration 00027    Loss 0.072873  0.000001\n",
            "Iteration 00028    Loss 0.071494  0.000001\n",
            "Iteration 00029    Loss 0.070234  0.000001\n",
            "Iteration 00030    Loss 0.068930  0.000001\n",
            "Iteration 00031    Loss 0.067419  0.000001\n",
            "Iteration 00032    Loss 0.065847  0.000001\n",
            "Iteration 00033    Loss 0.064611  0.000001\n",
            "Iteration 00034    Loss 0.063227  0.000001\n",
            "Iteration 00035    Loss 0.061862  0.000001\n",
            "Iteration 00036    Loss 0.060493  0.000001\n",
            "Iteration 00037    Loss 0.059393  0.000001\n",
            "Iteration 00038    Loss 0.058234  0.000001\n",
            "Iteration 00039    Loss 0.056927  0.000001\n",
            "Iteration 00040    Loss 0.055846  0.000001\n",
            "Iteration 00041    Loss 0.054627  0.000001\n",
            "Iteration 00042    Loss 0.053435  0.000001\n",
            "Iteration 00043    Loss 0.052257  0.000001\n",
            "Iteration 00044    Loss 0.051210  0.000001\n",
            "Iteration 00045    Loss 0.050238  0.000002\n",
            "Iteration 00046    Loss 0.049212  0.000002\n",
            "Iteration 00047    Loss 0.048225  0.000002\n",
            "Iteration 00048    Loss 0.047287  0.000002\n",
            "Iteration 00049    Loss 0.046361  0.000002\n",
            "Iteration 00050    Loss 0.045451  0.000002\n",
            "Iteration 00051    Loss 0.044473  0.000002\n",
            "Iteration 00052    Loss 0.043673  0.000002\n",
            "Iteration 00053    Loss 0.042807  0.000003\n",
            "Iteration 00054    Loss 0.041945  0.000003\n",
            "Iteration 00055    Loss 0.041127  0.000003\n",
            "Iteration 00056    Loss 0.040350  0.000003\n",
            "Iteration 00057    Loss 0.039559  0.000003\n",
            "Iteration 00058    Loss 0.038688  0.000004\n",
            "Iteration 00059    Loss 0.037878  0.000004\n",
            "Iteration 00060    Loss 0.037023  0.000004\n",
            "Iteration 00061    Loss 0.036345  0.000004\n",
            "Iteration 00062    Loss 0.035442  0.000004\n",
            "Iteration 00063    Loss 0.034790  0.000004\n",
            "Iteration 00064    Loss 0.033985  0.000005\n",
            "Iteration 00065    Loss 0.033353  0.000004\n",
            "Iteration 00066    Loss 0.032453  0.000005\n",
            "Iteration 00067    Loss 0.031859  0.000005\n",
            "Iteration 00068    Loss 0.031007  0.000005\n",
            "Iteration 00069    Loss 0.030249  0.000005\n",
            "Iteration 00070    Loss 0.029574  0.000005\n",
            "Iteration 00071    Loss 0.028745  0.000006\n",
            "Iteration 00072    Loss 0.028187  0.000006\n",
            "Iteration 00073    Loss 0.027533  0.000006\n",
            "Iteration 00074    Loss 0.026846  0.000007\n",
            "Iteration 00075    Loss 0.026270  0.000006\n",
            "Iteration 00076    Loss 0.025740  0.000007\n",
            "Iteration 00077    Loss 0.025006  0.000008\n",
            "Iteration 00078    Loss 0.024513  0.000007\n",
            "Iteration 00079    Loss 0.023782  0.000007\n",
            "Iteration 00080    Loss 0.023230  0.000009\n",
            "Iteration 00081    Loss 0.022599  0.000009\n",
            "Iteration 00082    Loss 0.021999  0.000009\n",
            "Iteration 00083    Loss 0.021498  0.000008\n",
            "Iteration 00084    Loss 0.020890  0.000009\n",
            "Iteration 00085    Loss 0.020368  0.000009\n",
            "Iteration 00086    Loss 0.019811  0.000009\n",
            "Iteration 00087    Loss 0.019326  0.000010\n",
            "Iteration 00088    Loss 0.018711  0.000011\n",
            "Iteration 00089    Loss 0.018246  0.000012\n",
            "Iteration 00090    Loss 0.017703  0.000012\n",
            "Iteration 00091    Loss 0.017216  0.000011\n",
            "Iteration 00092    Loss 0.016666  0.000011\n",
            "Iteration 00093    Loss 0.016328  0.000011\n",
            "Iteration 00094    Loss 0.015724  0.000013\n",
            "Iteration 00095    Loss 0.015164  0.000013\n",
            "Iteration 00096    Loss 0.014777  0.000012\n",
            "Iteration 00097    Loss 0.014115  0.000014\n",
            "Iteration 00098    Loss 0.013788  0.000015\n",
            "Iteration 00099    Loss 0.013231  0.000014\n",
            "Iteration 00100    Loss 0.012741  0.000014\n",
            "Iteration 00101    Loss 0.012445  0.000014\n",
            "Iteration 00102    Loss 0.011917  0.000014\n",
            "Iteration 00103    Loss 0.011444  0.000016\n",
            "Iteration 00104    Loss 0.011217  0.000015\n",
            "Iteration 00105    Loss 0.010651  0.000012\n",
            "Iteration 00106    Loss 0.010309  0.000016\n",
            "Iteration 00107    Loss 0.009824  0.000019\n",
            "Iteration 00108    Loss 0.009451  0.000014\n",
            "Iteration 00109    Loss 0.009202  0.000014\n",
            "Iteration 00110    Loss 0.008798  0.000016\n",
            "Iteration 00111    Loss 0.008171  0.000018\n",
            "Iteration 00112    Loss 0.007883  0.000017\n",
            "Iteration 00113    Loss 0.007522  0.000015\n",
            "Iteration 00114    Loss 0.006956  0.000016\n",
            "Iteration 00115    Loss 0.006501  0.000018\n",
            "Iteration 00116    Loss 0.006121  0.000019\n",
            "Iteration 00117    Loss 0.005887  0.000020\n",
            "Iteration 00118    Loss 0.005394  0.000021\n",
            "Iteration 00119    Loss 0.004964  0.000020\n",
            "Iteration 00120    Loss 0.004730  0.000019\n",
            "Iteration 00121    Loss 0.004359  0.000019\n",
            "Iteration 00122    Loss 0.003934  0.000020\n",
            "Iteration 00123    Loss 0.003607  0.000021\n",
            "Iteration 00124    Loss 0.003179  0.000020\n",
            "Iteration 00125    Loss 0.002792  0.000021\n",
            "Iteration 00126    Loss 0.002447  0.000023\n",
            "Iteration 00127    Loss 0.002107  0.000023\n",
            "Iteration 00128    Loss 0.001665  0.000021\n",
            "Iteration 00129    Loss 0.001315  0.000022\n",
            "Iteration 00130    Loss 0.000915  0.000025\n",
            "Iteration 00131    Loss 0.000646  0.000024\n",
            "Iteration 00132    Loss 0.000065  0.000022\n",
            "Iteration 00133    Loss -0.000073  0.000022\n",
            "Iteration 00134    Loss -0.000239  0.000024\n",
            "Iteration 00135    Loss -0.000877  0.000025\n",
            "Iteration 00136    Loss -0.001039  0.000023\n",
            "Iteration 00137    Loss -0.001333  0.000024\n",
            "Iteration 00138    Loss -0.001655  0.000026\n",
            "Iteration 00139    Loss -0.002003  0.000028\n",
            "Iteration 00140    Loss -0.002219  0.000026\n",
            "Iteration 00141    Loss -0.002639  0.000025\n",
            "Iteration 00142    Loss -0.002959  0.000028\n",
            "Iteration 00143    Loss -0.003440  0.000030\n",
            "Iteration 00144    Loss -0.003472  0.000025\n",
            "Iteration 00145    Loss -0.003948  0.000025\n",
            "Iteration 00146    Loss -0.004144  0.000031\n",
            "Iteration 00147    Loss -0.004037  0.000026\n",
            "Iteration 00148    Loss -0.004649  0.000025\n",
            "Iteration 00149    Loss -0.005050  0.000028\n",
            "Iteration 00150    Loss -0.005158  0.000031\n",
            "Iteration 00151    Loss -0.005510  0.000031\n",
            "Iteration 00152    Loss -0.005424  0.000027\n",
            "Iteration 00153    Loss -0.006066  0.000028\n",
            "Iteration 00154    Loss -0.006351  0.000029\n",
            "Iteration 00155    Loss -0.006653  0.000030\n",
            "Iteration 00156    Loss -0.007066  0.000028\n",
            "Iteration 00157    Loss -0.007159  0.000027\n",
            "Iteration 00158    Loss -0.007088  0.000028\n",
            "Iteration 00159    Loss -0.007770  0.000032\n",
            "Iteration 00160    Loss -0.007471  0.000031\n",
            "Iteration 00161    Loss -0.008044  0.000027\n",
            "Iteration 00162    Loss -0.008379  0.000030\n",
            "Iteration 00163    Loss -0.008708  0.000036\n",
            "Iteration 00164    Loss -0.009015  0.000035\n",
            "Iteration 00165    Loss -0.009362  0.000031\n",
            "Iteration 00166    Loss -0.009509  0.000033\n",
            "Iteration 00167    Loss -0.009906  0.000035\n",
            "Iteration 00168    Loss -0.009944  0.000032\n",
            "Iteration 00169    Loss -0.009622  0.000030\n",
            "Iteration 00170    Loss -0.010263  0.000033\n",
            "Iteration 00171    Loss -0.010605  0.000034\n",
            "Iteration 00172    Loss -0.010710  0.000031\n",
            "Iteration 00173    Loss -0.011109  0.000031\n",
            "Iteration 00174    Loss -0.011653  0.000036\n",
            "Iteration 00175    Loss -0.011488  0.000037\n",
            "Iteration 00176    Loss -0.011772  0.000032\n",
            "Iteration 00177    Loss -0.011970  0.000033\n",
            "Iteration 00178    Loss -0.012509  0.000038\n",
            "Iteration 00179    Loss -0.012399  0.000039\n",
            "Iteration 00180    Loss -0.012841  0.000036\n",
            "Iteration 00181    Loss -0.012994  0.000035\n",
            "Iteration 00182    Loss -0.013368  0.000040\n",
            "Iteration 00183    Loss -0.013472  0.000039\n",
            "Iteration 00184    Loss -0.013930  0.000034\n",
            "Iteration 00185    Loss -0.013497  0.000035\n",
            "Iteration 00186    Loss -0.013771  0.000042\n",
            "Iteration 00187    Loss -0.014362  0.000032\n",
            "Iteration 00188    Loss -0.014442  0.000032\n",
            "Iteration 00189    Loss -0.014600  0.000037\n",
            "Iteration 00190    Loss -0.015019  0.000044\n",
            "Iteration 00191    Loss -0.015274  0.000042\n",
            "Iteration 00192    Loss -0.015433  0.000036\n",
            "Iteration 00193    Loss -0.015509  0.000036\n",
            "Iteration 00194    Loss -0.016106  0.000040\n",
            "Iteration 00195    Loss -0.015341  0.000040\n",
            "Iteration 00196    Loss -0.016338  0.000037\n",
            "Iteration 00197    Loss -0.015689  0.000038\n",
            "Iteration 00198    Loss -0.016349  0.000045\n",
            "Iteration 00199    Loss -0.016812  0.000046\n",
            "Iteration 00200    Loss -0.016866  0.000040\n",
            "Iteration 00201    Loss -0.016994  0.000041\n",
            "Iteration 00202    Loss -0.017273  0.000044\n",
            "Iteration 00203    Loss -0.017242  0.000042\n",
            "Iteration 00204    Loss -0.017642  0.000041\n",
            "Iteration 00205    Loss -0.017412  0.000043\n",
            "Iteration 00206    Loss -0.017856  0.000046\n",
            "Iteration 00207    Loss -0.017935  0.000047\n",
            "Iteration 00208    Loss -0.018481  0.000044\n",
            "Iteration 00209    Loss -0.018516  0.000043\n",
            "Iteration 00210    Loss -0.018821  0.000047\n",
            "Iteration 00211    Loss -0.018954  0.000047\n",
            "Iteration 00212    Loss -0.019041  0.000046\n",
            "Iteration 00213    Loss -0.018868  0.000043\n",
            "Iteration 00214    Loss -0.019472  0.000046\n",
            "Iteration 00215    Loss -0.019935  0.000049\n",
            "Iteration 00216    Loss -0.020030  0.000048\n",
            "Iteration 00217    Loss -0.020214  0.000048\n",
            "Iteration 00218    Loss -0.020180  0.000046\n",
            "Iteration 00219    Loss -0.020263  0.000048\n",
            "Iteration 00220    Loss -0.020713  0.000044\n",
            "Iteration 00221    Loss -0.020886  0.000043\n",
            "Iteration 00222    Loss -0.021160  0.000050\n",
            "Iteration 00223    Loss -0.020756  0.000052\n",
            "Iteration 00224    Loss -0.020776  0.000052\n",
            "Iteration 00225    Loss -0.019838  0.000048\n",
            "Iteration 00226    Loss -0.021337  0.000050\n",
            "Iteration 00227    Loss -0.020993  0.000049\n",
            "Iteration 00228    Loss -0.021221  0.000043\n",
            "Iteration 00229    Loss -0.021691  0.000049\n",
            "Iteration 00230    Loss -0.021537  0.000055\n",
            "Iteration 00231    Loss -0.021787  0.000046\n",
            "Iteration 00232    Loss -0.022266  0.000048\n",
            "Iteration 00233    Loss -0.022563  0.000054\n",
            "Iteration 00234    Loss -0.022226  0.000053\n",
            "Iteration 00235    Loss -0.022876  0.000049\n",
            "Iteration 00236    Loss -0.022523  0.000050\n",
            "Iteration 00237    Loss -0.021824  0.000054\n",
            "Iteration 00238    Loss -0.023182  0.000056\n",
            "Iteration 00239    Loss -0.023431  0.000050\n",
            "Iteration 00240    Loss -0.023278  0.000049\n",
            "Iteration 00241    Loss -0.023490  0.000053\n",
            "Iteration 00242    Loss -0.023713  0.000055\n",
            "Iteration 00243    Loss -0.023464  0.000053\n",
            "Iteration 00244    Loss -0.023581  0.000053\n",
            "Iteration 00245    Loss -0.024062  0.000053\n",
            "Iteration 00246    Loss -0.023884  0.000055\n",
            "Iteration 00247    Loss -0.024091  0.000055\n",
            "Iteration 00248    Loss -0.024703  0.000053\n",
            "Iteration 00249    Loss -0.024821  0.000055\n",
            "Iteration 00250    Loss -0.024815  0.000061\n",
            "Iteration 00251    Loss -0.024933  0.000056\n",
            "Iteration 00252    Loss -0.024962  0.000052\n",
            "Iteration 00253    Loss -0.025031  0.000056\n",
            "Iteration 00254    Loss -0.025208  0.000062\n",
            "Iteration 00255    Loss -0.025625  0.000057\n",
            "Iteration 00256    Loss -0.025723  0.000053\n",
            "Iteration 00257    Loss -0.025490  0.000059\n",
            "Iteration 00258    Loss -0.025613  0.000068\n",
            "Iteration 00259    Loss -0.026037  0.000062\n",
            "Iteration 00260    Loss -0.026158  0.000059\n",
            "Iteration 00261    Loss -0.026324  0.000059\n",
            "Iteration 00262    Loss -0.026374  0.000058\n",
            "Iteration 00263    Loss -0.025961  0.000058\n",
            "Iteration 00264    Loss -0.026297  0.000066\n",
            "Iteration 00265    Loss -0.026913  0.000068\n",
            "Iteration 00266    Loss -0.026530  0.000057\n",
            "Iteration 00267    Loss -0.026385  0.000056\n",
            "Iteration 00268    Loss -0.027050  0.000066\n",
            "Iteration 00269    Loss -0.026531  0.000068\n",
            "Iteration 00270    Loss -0.026187  0.000060\n",
            "Iteration 00271    Loss -0.027349  0.000063\n",
            "Iteration 00272    Loss -0.026778  0.000069\n",
            "Iteration 00273    Loss -0.027652  0.000065\n",
            "Iteration 00274    Loss -0.027479  0.000059\n",
            "Iteration 00275    Loss -0.027518  0.000058\n",
            "Iteration 00276    Loss -0.027968  0.000074\n",
            "Iteration 00277    Loss -0.028010  0.000070\n",
            "Iteration 00278    Loss -0.028005  0.000059\n",
            "Iteration 00279    Loss -0.028192  0.000061\n",
            "Iteration 00280    Loss -0.028204  0.000067\n",
            "Iteration 00281    Loss -0.028516  0.000070\n",
            "Iteration 00282    Loss -0.028301  0.000070\n",
            "Iteration 00283    Loss -0.028602  0.000064\n",
            "Iteration 00284    Loss -0.028889  0.000068\n",
            "Iteration 00285    Loss -0.028851  0.000076\n",
            "Iteration 00286    Loss -0.028558  0.000066\n",
            "Iteration 00287    Loss -0.029247  0.000067\n",
            "Iteration 00288    Loss -0.029167  0.000075\n",
            "Iteration 00289    Loss -0.028667  0.000071\n",
            "Iteration 00290    Loss -0.029510  0.000069\n",
            "Iteration 00291    Loss -0.029560  0.000072\n",
            "Iteration 00292    Loss -0.029471  0.000073\n",
            "Iteration 00293    Loss -0.029821  0.000078\n",
            "Iteration 00294    Loss -0.029997  0.000080\n",
            "Iteration 00295    Loss -0.029602  0.000075\n",
            "Iteration 00296    Loss -0.030031  0.000080\n",
            "Iteration 00297    Loss -0.029934  0.000079\n",
            "Iteration 00298    Loss -0.030271  0.000073\n",
            "Iteration 00299    Loss -0.030297  0.000080\n",
            "Iteration 00300    Loss -0.030329  0.000086\n",
            "Iteration 00301    Loss -0.030393  0.000080\n",
            "Iteration 00302    Loss -0.030453  0.000078\n",
            "Iteration 00303    Loss -0.030098  0.000083\n",
            "Iteration 00304    Loss -0.030786  0.000091\n",
            "Iteration 00305    Loss -0.030838  0.000082\n",
            "Iteration 00306    Loss -0.030443  0.000079\n",
            "Iteration 00307    Loss -0.031067  0.000092\n",
            "Iteration 00308    Loss -0.031088  0.000077\n",
            "Iteration 00309    Loss -0.031082  0.000078\n",
            "Iteration 00310    Loss -0.031395  0.000087\n",
            "Iteration 00311    Loss -0.030640  0.000091\n",
            "Iteration 00312    Loss -0.031425  0.000086\n",
            "Iteration 00313    Loss -0.031662  0.000081\n",
            "Iteration 00314    Loss -0.031274  0.000083\n",
            "Iteration 00315    Loss -0.031846  0.000087\n",
            "Iteration 00316    Loss -0.031488  0.000082\n",
            "Iteration 00317    Loss -0.031824  0.000080\n",
            "Iteration 00318    Loss -0.032092  0.000088\n",
            "Iteration 00319    Loss -0.032125  0.000084\n",
            "Iteration 00320    Loss -0.032206  0.000084\n",
            "Iteration 00321    Loss -0.032392  0.000090\n",
            "Iteration 00322    Loss -0.032001  0.000088\n",
            "Iteration 00323    Loss -0.032517  0.000089\n",
            "Iteration 00324    Loss -0.032605  0.000085\n",
            "Iteration 00325    Loss -0.032570  0.000088\n",
            "Iteration 00326    Loss -0.032790  0.000098\n",
            "Iteration 00327    Loss -0.032724  0.000087\n",
            "Iteration 00328    Loss -0.032927  0.000091\n",
            "Iteration 00329    Loss -0.033067  0.000096\n",
            "Iteration 00330    Loss -0.032896  0.000090\n",
            "Iteration 00331    Loss -0.033195  0.000090\n",
            "Iteration 00332    Loss -0.033239  0.000095\n",
            "Iteration 00333    Loss -0.033203  0.000099\n",
            "Iteration 00334    Loss -0.033284  0.000099\n",
            "Iteration 00335    Loss -0.033476  0.000096\n",
            "Iteration 00336    Loss -0.033627  0.000100\n",
            "Iteration 00337    Loss -0.033560  0.000099\n",
            "Iteration 00338    Loss -0.033693  0.000103\n",
            "Iteration 00339    Loss -0.033415  0.000101\n",
            "Iteration 00340    Loss -0.033838  0.000101\n",
            "Iteration 00341    Loss -0.033763  0.000095\n",
            "Iteration 00342    Loss -0.033718  0.000105\n",
            "Iteration 00343    Loss -0.034068  0.000103\n",
            "Iteration 00344    Loss -0.033960  0.000101\n",
            "Iteration 00345    Loss -0.034210  0.000106\n",
            "Iteration 00346    Loss -0.033991  0.000101\n",
            "Iteration 00347    Loss -0.034337  0.000108\n",
            "Iteration 00348    Loss -0.034261  0.000105\n",
            "Iteration 00349    Loss -0.034452  0.000109\n",
            "Iteration 00350    Loss -0.034376  0.000098\n",
            "Iteration 00351    Loss -0.034497  0.000099\n",
            "Iteration 00352    Loss -0.034265  0.000107\n",
            "Iteration 00353    Loss -0.034583  0.000110\n",
            "Iteration 00354    Loss -0.034589  0.000101\n",
            "Iteration 00355    Loss -0.034805  0.000091\n",
            "Iteration 00356    Loss -0.034622  0.000102\n",
            "Iteration 00357    Loss -0.034914  0.000108\n",
            "Iteration 00358    Loss -0.035057  0.000108\n",
            "Iteration 00359    Loss -0.035119  0.000101\n",
            "Iteration 00360    Loss -0.035144  0.000106\n",
            "Iteration 00361    Loss -0.035166  0.000107\n",
            "Iteration 00362    Loss -0.035322  0.000103\n",
            "Iteration 00363    Loss -0.035148  0.000112\n",
            "Iteration 00364    Loss -0.035399  0.000113\n",
            "Iteration 00365    Loss -0.035493  0.000114\n",
            "Iteration 00366    Loss -0.035509  0.000103\n",
            "Iteration 00367    Loss -0.035605  0.000111\n",
            "Iteration 00368    Loss -0.035691  0.000117\n",
            "Iteration 00369    Loss -0.035418  0.000111\n",
            "Iteration 00370    Loss -0.035775  0.000117\n",
            "Iteration 00371    Loss -0.035831  0.000117\n",
            "Iteration 00372    Loss -0.035837  0.000119\n",
            "Iteration 00373    Loss -0.036020  0.000113\n",
            "Iteration 00374    Loss -0.036055  0.000109\n",
            "Iteration 00375    Loss -0.035978  0.000115\n",
            "Iteration 00376    Loss -0.036044  0.000119\n",
            "Iteration 00377    Loss -0.036221  0.000116\n",
            "Iteration 00378    Loss -0.036272  0.000128\n",
            "Iteration 00379    Loss -0.036335  0.000132\n",
            "Iteration 00380    Loss -0.036348  0.000115\n",
            "Iteration 00381    Loss -0.036461  0.000118\n",
            "Iteration 00382    Loss -0.036514  0.000133\n",
            "Iteration 00383    Loss -0.036475  0.000114\n",
            "Iteration 00384    Loss -0.036304  0.000130\n",
            "Iteration 00385    Loss -0.036071  0.000099\n",
            "Iteration 00386    Loss -0.036420  0.000091\n",
            "Iteration 00387    Loss -0.036544  0.000100\n",
            "Iteration 00388    Loss -0.036491  0.000099\n",
            "Iteration 00389    Loss -0.036373  0.000093\n",
            "Iteration 00390    Loss -0.036550  0.000093\n",
            "Iteration 00391    Loss -0.036709  0.000092\n",
            "Iteration 00392    Loss -0.036702  0.000089\n",
            "Iteration 00393    Loss -0.036815  0.000097\n",
            "Iteration 00394    Loss -0.036953  0.000110\n",
            "Iteration 00395    Loss -0.036743  0.000108\n",
            "Iteration 00396    Loss -0.037095  0.000111\n",
            "Iteration 00397    Loss -0.037128  0.000104\n",
            "Iteration 00398    Loss -0.037212  0.000106\n",
            "Iteration 00399    Loss -0.036629  0.000113\n",
            "Iteration 00400    Loss -0.037168  0.000125\n",
            "Iteration 00401    Loss -0.037333  0.000118\n",
            "Iteration 00402    Loss -0.037402  0.000101\n",
            "Iteration 00403    Loss -0.037197  0.000107\n",
            "Iteration 00404    Loss -0.037290  0.000114\n",
            "Iteration 00405    Loss -0.037406  0.000112\n",
            "Iteration 00406    Loss -0.037558  0.000119\n",
            "Iteration 00407    Loss -0.037601  0.000132\n",
            "Iteration 00408    Loss -0.037605  0.000120\n",
            "Iteration 00409    Loss -0.037368  0.000112\n",
            "Iteration 00410    Loss -0.037740  0.000120\n",
            "Iteration 00411    Loss -0.037634  0.000125\n",
            "Iteration 00412    Loss -0.037908  0.000128\n",
            "Iteration 00413    Loss -0.037716  0.000120\n",
            "Iteration 00414    Loss -0.037893  0.000122\n",
            "Iteration 00415    Loss -0.037973  0.000126\n",
            "Iteration 00416    Loss -0.038080  0.000132\n",
            "Iteration 00417    Loss -0.038095  0.000130\n",
            "Iteration 00418    Loss -0.037987  0.000123\n",
            "Iteration 00419    Loss -0.038152  0.000129\n",
            "Iteration 00420    Loss -0.038138  0.000137\n",
            "Iteration 00421    Loss -0.038162  0.000135\n",
            "Iteration 00422    Loss -0.038046  0.000139\n",
            "Iteration 00423    Loss -0.038269  0.000133\n",
            "Iteration 00424    Loss -0.038374  0.000135\n",
            "Iteration 00425    Loss -0.038402  0.000133\n",
            "Iteration 00426    Loss -0.038401  0.000141\n",
            "Iteration 00427    Loss -0.038436  0.000130\n",
            "Iteration 00428    Loss -0.038625  0.000131\n",
            "Iteration 00429    Loss -0.038502  0.000145\n",
            "Iteration 00430    Loss -0.038622  0.000132\n",
            "Iteration 00431    Loss -0.038694  0.000144\n",
            "Iteration 00432    Loss -0.038472  0.000134\n",
            "Iteration 00433    Loss -0.038741  0.000140\n",
            "Iteration 00434    Loss -0.038645  0.000156\n",
            "Iteration 00435    Loss -0.038619  0.000125\n",
            "Iteration 00436    Loss -0.038717  0.000127\n",
            "Iteration 00437    Loss -0.038931  0.000141\n",
            "Iteration 00438    Loss -0.038818  0.000139\n",
            "Iteration 00439    Loss -0.038941  0.000129\n",
            "Iteration 00440    Loss -0.039029  0.000125\n",
            "Iteration 00441    Loss -0.038863  0.000128\n",
            "Iteration 00442    Loss -0.039042  0.000139\n",
            "Iteration 00443    Loss -0.039090  0.000138\n",
            "Iteration 00444    Loss -0.039118  0.000132\n",
            "Iteration 00445    Loss -0.039250  0.000145\n",
            "Iteration 00446    Loss -0.039224  0.000139\n",
            "Iteration 00447    Loss -0.039307  0.000138\n",
            "Iteration 00448    Loss -0.039347  0.000152\n",
            "Iteration 00449    Loss -0.039206  0.000144\n",
            "Iteration 00450    Loss -0.039454  0.000141\n",
            "Iteration 00451    Loss -0.039536  0.000142\n",
            "Iteration 00452    Loss -0.039537  0.000146\n",
            "Iteration 00453    Loss -0.039562  0.000151\n",
            "Iteration 00454    Loss -0.039642  0.000149\n",
            "Iteration 00455    Loss -0.039703  0.000147\n",
            "Iteration 00456    Loss -0.039637  0.000156\n",
            "Iteration 00457    Loss -0.039696  0.000161\n",
            "Iteration 00458    Loss -0.039780  0.000151\n",
            "Iteration 00459    Loss -0.039778  0.000150\n",
            "Iteration 00460    Loss -0.039799  0.000157\n",
            "Iteration 00461    Loss -0.039881  0.000163\n",
            "Iteration 00462    Loss -0.039416  0.000161\n",
            "Iteration 00463    Loss -0.039744  0.000154\n",
            "Iteration 00464    Loss -0.040005  0.000158\n",
            "Iteration 00465    Loss -0.039905  0.000163\n",
            "Iteration 00466    Loss -0.040037  0.000153\n",
            "Iteration 00467    Loss -0.039987  0.000162\n",
            "Iteration 00468    Loss -0.040139  0.000157\n",
            "Iteration 00469    Loss -0.040130  0.000156\n",
            "Iteration 00470    Loss -0.040178  0.000156\n",
            "Iteration 00471    Loss -0.040205  0.000167\n",
            "Iteration 00472    Loss -0.040257  0.000152\n",
            "Iteration 00473    Loss -0.040271  0.000154\n",
            "Iteration 00474    Loss -0.040299  0.000159\n",
            "Iteration 00475    Loss -0.040196  0.000157\n",
            "Iteration 00476    Loss -0.040374  0.000171\n",
            "Iteration 00477    Loss -0.040457  0.000159\n",
            "Iteration 00478    Loss -0.040337  0.000159\n",
            "Iteration 00479    Loss -0.040525  0.000169\n",
            "Iteration 00480    Loss -0.040328  0.000161\n",
            "Iteration 00481    Loss -0.040444  0.000173\n",
            "Iteration 00482    Loss -0.040594  0.000176\n",
            "Iteration 00483    Loss -0.040636  0.000160\n",
            "Iteration 00484    Loss -0.040633  0.000171\n",
            "Iteration 00485    Loss -0.040644  0.000167\n",
            "Iteration 00486    Loss -0.040703  0.000172\n",
            "Iteration 00487    Loss -0.040722  0.000169\n",
            "Iteration 00488    Loss -0.040725  0.000177\n",
            "Iteration 00489    Loss -0.040843  0.000177\n",
            "Iteration 00490    Loss -0.040772  0.000175\n",
            "Iteration 00491    Loss -0.040729  0.000170\n",
            "Iteration 00492    Loss -0.040880  0.000182\n",
            "Iteration 00493    Loss -0.040910  0.000183\n",
            "Iteration 00494    Loss -0.040982  0.000174\n",
            "Iteration 00495    Loss -0.041037  0.000187\n",
            "Iteration 00496    Loss -0.040885  0.000169\n",
            "Iteration 00497    Loss -0.041001  0.000169\n",
            "Iteration 00498    Loss -0.040920  0.000179\n",
            "Iteration 00499    Loss -0.041092  0.000194\n",
            " 99% 494/500 [4:41:58<03:31, 35.17s/it]/content/SOTS/outdoor/hazy/1931_0.8_0.08.jpg\n",
            "Iteration 00000    Loss 0.327835  0.000044\n",
            "Iteration 00001    Loss 18.742275  0.000015\n",
            "Iteration 00002    Loss 0.257707  0.000007\n",
            "Iteration 00003    Loss 2.297749  0.000006\n",
            "Iteration 00004    Loss 0.182287  0.000003\n",
            "Iteration 00005    Loss 0.557174  0.000003\n",
            "Iteration 00006    Loss 0.160961  0.000003\n",
            "Iteration 00007    Loss 0.182064  0.000003\n",
            "Iteration 00008    Loss 0.150010  0.000003\n",
            "Iteration 00009    Loss 0.133728  0.000003\n",
            "Iteration 00010    Loss 0.128416  0.000003\n",
            "Iteration 00011    Loss 0.125807  0.000004\n",
            "Iteration 00012    Loss 0.123628  0.000004\n",
            "Iteration 00013    Loss 0.121473  0.000005\n",
            "Iteration 00014    Loss 0.119153  0.000006\n",
            "Iteration 00015    Loss 0.117566  0.000007\n",
            "Iteration 00016    Loss 0.115396  0.000008\n",
            "Iteration 00017    Loss 0.113645  0.000010\n",
            "Iteration 00018    Loss 0.111981  0.000012\n",
            "Iteration 00019    Loss 0.109588  0.000013\n",
            "Iteration 00020    Loss 0.107351  0.000015\n",
            "Iteration 00021    Loss 0.105180  0.000015\n",
            "Iteration 00022    Loss 0.103146  0.000015\n",
            "Iteration 00023    Loss 0.100296  0.000016\n",
            "Iteration 00024    Loss 0.099179  0.000018\n",
            "Iteration 00025    Loss 0.096728  0.000021\n",
            "Iteration 00026    Loss 0.094275  0.000023\n",
            "Iteration 00027    Loss 0.092859  0.000025\n",
            "Iteration 00028    Loss 0.090497  0.000027\n",
            "Iteration 00029    Loss 0.088981  0.000029\n",
            "Iteration 00030    Loss 0.087047  0.000030\n",
            "Iteration 00031    Loss 0.085617  0.000031\n",
            "Iteration 00032    Loss 0.084151  0.000034\n",
            "Iteration 00033    Loss 0.082530  0.000041\n",
            "Iteration 00034    Loss 0.081302  0.000041\n",
            "Iteration 00035    Loss 0.080139  0.000044\n",
            "Iteration 00036    Loss 0.078619  0.000047\n",
            "Iteration 00037    Loss 0.077827  0.000048\n",
            "Iteration 00038    Loss 0.076429  0.000049\n",
            "Iteration 00039    Loss 0.075307  0.000052\n",
            "Iteration 00040    Loss 0.074220  0.000054\n",
            "Iteration 00041    Loss 0.073099  0.000053\n",
            "Iteration 00042    Loss 0.072613  0.000054\n",
            "Iteration 00043    Loss 0.071325  0.000059\n",
            "Iteration 00044    Loss 0.070533  0.000060\n",
            "Iteration 00045    Loss 0.069348  0.000061\n",
            "Iteration 00046    Loss 0.068764  0.000063\n",
            "Iteration 00047    Loss 0.067756  0.000064\n",
            "Iteration 00048    Loss 0.066203  0.000062\n",
            "Iteration 00049    Loss 0.065139  0.000064\n",
            "Iteration 00050    Loss 0.064269  0.000068\n",
            "Iteration 00051    Loss 0.063753  0.000067\n",
            "Iteration 00052    Loss 0.062814  0.000072\n",
            "Iteration 00053    Loss 0.061862  0.000078\n",
            "Iteration 00054    Loss 0.060932  0.000074\n",
            "Iteration 00055    Loss 0.060360  0.000079\n",
            "Iteration 00056    Loss 0.059518  0.000086\n",
            "Iteration 00057    Loss 0.058750  0.000082\n",
            "Iteration 00058    Loss 0.057724  0.000081\n",
            "Iteration 00059    Loss 0.056776  0.000085\n",
            "Iteration 00060    Loss 0.056042  0.000085\n",
            "Iteration 00061    Loss 0.055229  0.000087\n",
            "Iteration 00062    Loss 0.054476  0.000091\n",
            "Iteration 00063    Loss 0.053758  0.000100\n",
            "Iteration 00064    Loss 0.053050  0.000098\n",
            "Iteration 00065    Loss 0.052128  0.000100\n",
            "Iteration 00066    Loss 0.051422  0.000100\n",
            "Iteration 00067    Loss 0.050561  0.000102\n",
            "Iteration 00068    Loss 0.050357  0.000110\n",
            "Iteration 00069    Loss 0.049427  0.000114\n",
            "Iteration 00070    Loss 0.049236  0.000113\n",
            "Iteration 00071    Loss 0.049210  0.000120\n",
            "Iteration 00072    Loss 0.048063  0.000122\n",
            "Iteration 00073    Loss 0.048124  0.000122\n",
            "Iteration 00074    Loss 0.046723  0.000130\n",
            "Iteration 00075    Loss 0.046215  0.000133\n",
            "Iteration 00076    Loss 0.045704  0.000132\n",
            "Iteration 00077    Loss 0.045062  0.000140\n",
            "Iteration 00078    Loss 0.044301  0.000149\n",
            "Iteration 00079    Loss 0.043736  0.000139\n",
            "Iteration 00080    Loss 0.043019  0.000139\n",
            "Iteration 00081    Loss 0.042171  0.000145\n",
            "Iteration 00082    Loss 0.041464  0.000142\n",
            "Iteration 00083    Loss 0.040718  0.000156\n",
            "Iteration 00084    Loss 0.040361  0.000170\n",
            "Iteration 00085    Loss 0.039432  0.000162\n",
            "Iteration 00086    Loss 0.038654  0.000175\n",
            "Iteration 00087    Loss 0.038144  0.000158\n",
            "Iteration 00088    Loss 0.038032  0.000176\n",
            "Iteration 00089    Loss 0.037174  0.000184\n",
            "Iteration 00090    Loss 0.036356  0.000154\n",
            "Iteration 00091    Loss 0.035590  0.000180\n",
            "Iteration 00092    Loss 0.035277  0.000197\n",
            "Iteration 00093    Loss 0.034561  0.000167\n",
            "Iteration 00094    Loss 0.033814  0.000181\n",
            "Iteration 00095    Loss 0.033402  0.000205\n",
            "Iteration 00096    Loss 0.032686  0.000187\n",
            "Iteration 00097    Loss 0.032208  0.000191\n",
            "Iteration 00098    Loss 0.031194  0.000217\n",
            "Iteration 00099    Loss 0.030738  0.000213\n",
            "Iteration 00100    Loss 0.030246  0.000204\n",
            "Iteration 00101    Loss 0.029589  0.000222\n",
            "Iteration 00102    Loss 0.028719  0.000227\n",
            "Iteration 00103    Loss 0.028397  0.000216\n",
            "Iteration 00104    Loss 0.027749  0.000231\n",
            "Iteration 00105    Loss 0.027466  0.000218\n",
            "Iteration 00106    Loss 0.026802  0.000220\n",
            "Iteration 00107    Loss 0.026079  0.000231\n",
            "Iteration 00108    Loss 0.025486  0.000249\n",
            "Iteration 00109    Loss 0.025143  0.000259\n",
            "Iteration 00110    Loss 0.024316  0.000260\n",
            "Iteration 00111    Loss 0.023209  0.000253\n",
            "Iteration 00112    Loss 0.022792  0.000266\n",
            "Iteration 00113    Loss 0.022603  0.000270\n",
            "Iteration 00114    Loss 0.022298  0.000258\n",
            "Iteration 00115    Loss 0.021286  0.000293\n",
            "Iteration 00116    Loss 0.020788  0.000247\n",
            "Iteration 00117    Loss 0.020030  0.000261\n",
            "Iteration 00118    Loss 0.019634  0.000287\n",
            "Iteration 00119    Loss 0.018982  0.000290\n",
            "Iteration 00120    Loss 0.018728  0.000287\n",
            "Iteration 00121    Loss 0.017992  0.000282\n",
            "Iteration 00122    Loss 0.017821  0.000281\n",
            "Iteration 00123    Loss 0.016971  0.000310\n",
            "Iteration 00124    Loss 0.016637  0.000313\n",
            "Iteration 00125    Loss 0.016203  0.000331\n",
            "Iteration 00126    Loss 0.015699  0.000331\n",
            "Iteration 00127    Loss 0.015003  0.000325\n",
            "Iteration 00128    Loss 0.014494  0.000342\n",
            "Iteration 00129    Loss 0.014036  0.000329\n",
            "Iteration 00130    Loss 0.013558  0.000358\n",
            "Iteration 00131    Loss 0.012845  0.000361\n",
            "Iteration 00132    Loss 0.012813  0.000343\n",
            "Iteration 00133    Loss 0.012058  0.000380\n",
            "Iteration 00134    Loss 0.012286  0.000319\n",
            "Iteration 00135    Loss 0.011362  0.000368\n",
            "Iteration 00136    Loss 0.011487  0.000369\n",
            "Iteration 00137    Loss 0.010461  0.000370\n",
            "Iteration 00138    Loss 0.011155  0.000413\n",
            "Iteration 00139    Loss 0.009711  0.000399\n",
            "Iteration 00140    Loss 0.009615  0.000399\n",
            "Iteration 00141    Loss 0.009353  0.000417\n",
            "Iteration 00142    Loss 0.008494  0.000425\n",
            "Iteration 00143    Loss 0.008130  0.000417\n",
            "Iteration 00144    Loss 0.007836  0.000436\n",
            "Iteration 00145    Loss 0.007590  0.000426\n",
            "Iteration 00146    Loss 0.007798  0.000407\n",
            "Iteration 00147    Loss 0.007184  0.000441\n",
            "Iteration 00148    Loss 0.006827  0.000466\n",
            "Iteration 00149    Loss 0.006429  0.000437\n",
            "Iteration 00150    Loss 0.006252  0.000434\n",
            "Iteration 00151    Loss 0.005668  0.000506\n",
            "Iteration 00152    Loss 0.006106  0.000395\n",
            "Iteration 00153    Loss 0.005275  0.000456\n",
            "Iteration 00154    Loss 0.004406  0.000483\n",
            "Iteration 00155    Loss 0.004789  0.000421\n",
            "Iteration 00156    Loss 0.003817  0.000425\n",
            "Iteration 00157    Loss 0.003921  0.000470\n",
            "Iteration 00158    Loss 0.003450  0.000476\n",
            "Iteration 00159    Loss 0.003575  0.000461\n",
            "Iteration 00160    Loss 0.002412  0.000505\n",
            "Iteration 00161    Loss 0.002290  0.000506\n",
            "Iteration 00162    Loss 0.002228  0.000488\n",
            "Iteration 00163    Loss 0.001635  0.000512\n",
            "Iteration 00164    Loss 0.001663  0.000521\n",
            "Iteration 00165    Loss 0.001679  0.000503\n",
            "Iteration 00166    Loss 0.001137  0.000535\n",
            "Iteration 00167    Loss 0.000616  0.000547\n",
            "Iteration 00168    Loss 0.000061  0.000566\n",
            "Iteration 00169    Loss 0.000029  0.000574\n",
            "Iteration 00170    Loss 0.000734  0.000561\n",
            "Iteration 00171    Loss -0.000850  0.000597\n",
            "Iteration 00172    Loss -0.000997  0.000581\n",
            "Iteration 00173    Loss -0.001260  0.000626\n",
            "Iteration 00174    Loss -0.001326  0.000645\n",
            "Iteration 00175    Loss -0.001863  0.000585\n",
            "Iteration 00176    Loss -0.002210  0.000655\n",
            "Iteration 00177    Loss -0.002350  0.000580\n",
            "Iteration 00178    Loss -0.002185  0.000611\n",
            "Iteration 00179    Loss -0.002813  0.000661\n",
            "Iteration 00180    Loss -0.002964  0.000590\n",
            "Iteration 00181    Loss -0.003184  0.000625\n",
            "Iteration 00182    Loss -0.003687  0.000672\n",
            "Iteration 00183    Loss -0.002801  0.000616\n",
            "Iteration 00184    Loss -0.004351  0.000676\n",
            "Iteration 00185    Loss -0.004275  0.000668\n",
            "Iteration 00186    Loss -0.004225  0.000653\n",
            "Iteration 00187    Loss -0.004629  0.000674\n",
            "Iteration 00188    Loss -0.005221  0.000620\n",
            "Iteration 00189    Loss -0.005236  0.000634\n",
            "Iteration 00190    Loss -0.005775  0.000716\n",
            "Iteration 00191    Loss -0.005475  0.000750\n",
            "Iteration 00192    Loss -0.005910  0.000720\n",
            "Iteration 00193    Loss -0.006266  0.000634\n",
            "Iteration 00194    Loss -0.006557  0.000712\n",
            "Iteration 00195    Loss -0.006598  0.000657\n",
            "Iteration 00196    Loss -0.006531  0.000747\n",
            "Iteration 00197    Loss -0.007533  0.000768\n",
            "Iteration 00198    Loss -0.007578  0.000671\n",
            "Iteration 00199    Loss -0.007728  0.000750\n",
            "Iteration 00200    Loss -0.007650  0.000668\n",
            "Iteration 00201    Loss -0.007837  0.000726\n",
            "Iteration 00202    Loss -0.008728  0.000810\n",
            "Iteration 00203    Loss -0.008973  0.000769\n",
            "Iteration 00204    Loss -0.009068  0.000742\n",
            "Iteration 00205    Loss -0.009175  0.000740\n",
            "Iteration 00206    Loss -0.009588  0.000742\n",
            "Iteration 00207    Loss -0.009731  0.000805\n",
            "Iteration 00208    Loss -0.008881  0.000814\n",
            "Iteration 00209    Loss -0.010379  0.000820\n",
            "Iteration 00210    Loss -0.010228  0.000789\n",
            "Iteration 00211    Loss -0.010719  0.000850\n",
            "Iteration 00212    Loss -0.010907  0.000781\n",
            "Iteration 00213    Loss -0.010948  0.000835\n",
            "Iteration 00214    Loss -0.011156  0.000819\n",
            "Iteration 00215    Loss -0.011692  0.000845\n",
            "Iteration 00216    Loss -0.011446  0.000883\n",
            "Iteration 00217    Loss -0.012098  0.000857\n",
            "Iteration 00218    Loss -0.012347  0.000849\n",
            "Iteration 00219    Loss -0.012109  0.000883\n",
            "Iteration 00220    Loss -0.012642  0.000873\n",
            "Iteration 00221    Loss -0.012764  0.000884\n",
            "Iteration 00222    Loss -0.013114  0.000849\n",
            "Iteration 00223    Loss -0.012870  0.000883\n",
            "Iteration 00224    Loss -0.013506  0.000977\n",
            "Iteration 00225    Loss -0.013426  0.000898\n",
            "Iteration 00226    Loss -0.013660  0.000916\n",
            "Iteration 00227    Loss -0.013316  0.000911\n",
            "Iteration 00228    Loss -0.014185  0.000933\n",
            "Iteration 00229    Loss -0.014005  0.000962\n",
            "Iteration 00230    Loss -0.014351  0.000826\n",
            "Iteration 00231    Loss -0.014634  0.000901\n",
            "Iteration 00232    Loss -0.014869  0.000738\n",
            "Iteration 00233    Loss -0.015023  0.000811\n",
            "Iteration 00234    Loss -0.014401  0.000844\n",
            "Iteration 00235    Loss -0.015575  0.000905\n",
            "Iteration 00236    Loss -0.015360  0.000896\n",
            "Iteration 00237    Loss -0.015899  0.000826\n",
            "Iteration 00238    Loss -0.015963  0.000881\n",
            "Iteration 00239    Loss -0.015705  0.000918\n",
            "Iteration 00240    Loss -0.016303  0.000924\n",
            "Iteration 00241    Loss -0.016575  0.001001\n",
            "Iteration 00242    Loss -0.016824  0.000929\n",
            "Iteration 00243    Loss -0.016967  0.000937\n",
            "Iteration 00244    Loss -0.017290  0.000954\n",
            "Iteration 00245    Loss -0.017361  0.000947\n",
            "Iteration 00246    Loss -0.017523  0.000977\n",
            "Iteration 00247    Loss -0.017554  0.000956\n",
            "Iteration 00248    Loss -0.017922  0.001014\n",
            "Iteration 00249    Loss -0.018061  0.001036\n",
            "Iteration 00250    Loss -0.018142  0.000958\n",
            "Iteration 00251    Loss -0.018350  0.001009\n",
            "Iteration 00252    Loss -0.018482  0.001066\n",
            "Iteration 00253    Loss -0.018595  0.001030\n",
            "Iteration 00254    Loss -0.018861  0.001057\n",
            "Iteration 00255    Loss -0.019092  0.001034\n",
            "Iteration 00256    Loss -0.018566  0.001106\n",
            "Iteration 00257    Loss -0.019171  0.001020\n",
            "Iteration 00258    Loss -0.019361  0.001096\n",
            "Iteration 00259    Loss -0.019569  0.000985\n",
            "Iteration 00260    Loss -0.019753  0.001044\n",
            "Iteration 00261    Loss -0.019852  0.001073\n",
            "Iteration 00262    Loss -0.019853  0.001014\n",
            "Iteration 00263    Loss -0.020227  0.001094\n",
            "Iteration 00264    Loss -0.020388  0.001069\n",
            "Iteration 00265    Loss -0.020537  0.001045\n",
            "Iteration 00266    Loss -0.020632  0.001117\n",
            "Iteration 00267    Loss -0.020691  0.001112\n",
            "Iteration 00268    Loss -0.020894  0.001139\n",
            "Iteration 00269    Loss -0.021135  0.001102\n",
            "Iteration 00270    Loss -0.021138  0.001047\n",
            "Iteration 00271    Loss -0.021222  0.001151\n",
            "Iteration 00272    Loss -0.021534  0.001023\n",
            "Iteration 00273    Loss -0.021606  0.001109\n",
            "Iteration 00274    Loss -0.021812  0.001165\n",
            "Iteration 00275    Loss -0.021848  0.001006\n",
            "Iteration 00276    Loss -0.021959  0.001019\n",
            "Iteration 00277    Loss -0.022051  0.001191\n",
            "Iteration 00278    Loss -0.022215  0.001083\n",
            "Iteration 00279    Loss -0.022426  0.001001\n",
            "Iteration 00280    Loss -0.022542  0.001166\n",
            "Iteration 00281    Loss -0.022664  0.001120\n",
            "Iteration 00282    Loss -0.022711  0.001087\n",
            "Iteration 00283    Loss -0.023021  0.001147\n",
            "Iteration 00284    Loss -0.022826  0.001167\n",
            "Iteration 00285    Loss -0.023104  0.001131\n",
            "Iteration 00286    Loss -0.023403  0.001113\n",
            "Iteration 00287    Loss -0.023472  0.001209\n",
            "Iteration 00288    Loss -0.023421  0.001042\n",
            "Iteration 00289    Loss -0.023691  0.001156\n",
            "Iteration 00290    Loss -0.023892  0.001314\n",
            "Iteration 00291    Loss -0.023628  0.001116\n",
            "Iteration 00292    Loss -0.023828  0.001140\n",
            "Iteration 00293    Loss -0.024225  0.001186\n",
            "Iteration 00294    Loss -0.024360  0.001153\n",
            "Iteration 00295    Loss -0.024555  0.001162\n",
            "Iteration 00296    Loss -0.024535  0.001205\n",
            "Iteration 00297    Loss -0.024722  0.001235\n",
            "Iteration 00298    Loss -0.024921  0.001228\n",
            "Iteration 00299    Loss -0.024826  0.001300\n",
            "Iteration 00300    Loss -0.024986  0.001254\n",
            "Iteration 00301    Loss -0.025145  0.001223\n",
            "Iteration 00302    Loss -0.025251  0.001227\n",
            "Iteration 00303    Loss -0.025411  0.001238\n",
            "Iteration 00304    Loss -0.025572  0.001217\n",
            "Iteration 00305    Loss -0.025769  0.001257\n",
            "Iteration 00306    Loss -0.025767  0.001335\n",
            "Iteration 00307    Loss -0.025894  0.001321\n",
            "Iteration 00308    Loss -0.026030  0.001256\n",
            "Iteration 00309    Loss -0.025858  0.001262\n",
            "Iteration 00310    Loss -0.026200  0.001318\n",
            "Iteration 00311    Loss -0.026326  0.001407\n",
            "Iteration 00312    Loss -0.026461  0.001272\n",
            "Iteration 00313    Loss -0.026154  0.001240\n",
            "Iteration 00314    Loss -0.026141  0.001389\n",
            "Iteration 00315    Loss -0.026716  0.001314\n",
            "Iteration 00316    Loss -0.026757  0.001291\n",
            "Iteration 00317    Loss -0.026814  0.001352\n",
            "Iteration 00318    Loss -0.026974  0.001368\n",
            "Iteration 00319    Loss -0.027175  0.001389\n",
            "Iteration 00320    Loss -0.027201  0.001471\n",
            "Iteration 00321    Loss -0.027386  0.001374\n",
            "Iteration 00322    Loss -0.027378  0.001361\n",
            "Iteration 00323    Loss -0.027373  0.001362\n",
            "Iteration 00324    Loss -0.027332  0.001470\n",
            "Iteration 00325    Loss -0.027274  0.001226\n",
            "Iteration 00326    Loss -0.027726  0.001326\n",
            "Iteration 00327    Loss -0.027582  0.001311\n",
            "Iteration 00328    Loss -0.027865  0.001378\n",
            "Iteration 00329    Loss -0.027993  0.001482\n",
            "Iteration 00330    Loss -0.028011  0.001234\n",
            "Iteration 00331    Loss -0.028247  0.001283\n",
            "Iteration 00332    Loss -0.028390  0.001459\n",
            "Iteration 00333    Loss -0.028335  0.001400\n",
            "Iteration 00334    Loss -0.028185  0.001452\n",
            "Iteration 00335    Loss -0.028650  0.001374\n",
            "Iteration 00336    Loss -0.028619  0.001512\n",
            "Iteration 00337    Loss -0.028631  0.001412\n",
            "Iteration 00338    Loss -0.028781  0.001474\n",
            "Iteration 00339    Loss -0.028804  0.001460\n",
            "Iteration 00340    Loss -0.029045  0.001452\n",
            "Iteration 00341    Loss -0.029178  0.001424\n",
            "Iteration 00342    Loss -0.029273  0.001544\n",
            "Iteration 00343    Loss -0.029434  0.001472\n",
            "Iteration 00344    Loss -0.029429  0.001672\n",
            "Iteration 00345    Loss -0.029466  0.001339\n",
            "Iteration 00346    Loss -0.029542  0.001469\n",
            "Iteration 00347    Loss -0.029599  0.001386\n",
            "Iteration 00348    Loss -0.029553  0.001553\n",
            "Iteration 00349    Loss -0.029890  0.001663\n",
            "Iteration 00350    Loss -0.029924  0.001555\n",
            "Iteration 00351    Loss -0.030058  0.001599\n",
            "Iteration 00352    Loss -0.030173  0.001638\n",
            "Iteration 00353    Loss -0.030234  0.001673\n",
            "Iteration 00354    Loss -0.030419  0.001696\n",
            "Iteration 00355    Loss -0.030335  0.001635\n",
            "Iteration 00356    Loss -0.030550  0.001855\n",
            "Iteration 00357    Loss -0.030556  0.001806\n",
            "Iteration 00358    Loss -0.030525  0.001958\n",
            "Iteration 00359    Loss -0.030583  0.001410\n",
            "Iteration 00360    Loss -0.030785  0.001858\n",
            "Iteration 00361    Loss -0.030756  0.002092\n",
            "Iteration 00362    Loss -0.030877  0.001836\n",
            "Iteration 00363    Loss -0.031089  0.001940\n",
            "Iteration 00364    Loss -0.030979  0.001972\n",
            "Iteration 00365    Loss -0.031176  0.001964\n",
            "Iteration 00366    Loss -0.031308  0.001965\n",
            "Iteration 00367    Loss -0.031259  0.002044\n",
            "Iteration 00368    Loss -0.031417  0.002210\n",
            "Iteration 00369    Loss -0.031413  0.001745\n",
            "Iteration 00370    Loss -0.031384  0.001897\n",
            "Iteration 00371    Loss -0.031474  0.001880\n",
            "Iteration 00372    Loss -0.031396  0.001956\n",
            "Iteration 00373    Loss -0.031791  0.002180\n",
            "Iteration 00374    Loss -0.031866  0.002030\n",
            "Iteration 00375    Loss -0.031865  0.002092\n",
            "Iteration 00376    Loss -0.031898  0.002228\n",
            "Iteration 00377    Loss -0.032082  0.002066\n",
            "Iteration 00378    Loss -0.032061  0.002194\n",
            "Iteration 00379    Loss -0.032111  0.002144\n",
            "Iteration 00380    Loss -0.032351  0.002358\n",
            "Iteration 00381    Loss -0.032338  0.002292\n",
            "Iteration 00382    Loss -0.032403  0.002565\n",
            "Iteration 00383    Loss -0.032419  0.002271\n",
            "Iteration 00384    Loss -0.032418  0.002408\n",
            "Iteration 00385    Loss -0.032698  0.002439\n",
            "Iteration 00386    Loss -0.032663  0.002300\n",
            "Iteration 00387    Loss -0.032634  0.002810\n",
            "Iteration 00388    Loss -0.032490  0.001870\n",
            "Iteration 00389    Loss -0.032768  0.002052\n",
            "Iteration 00390    Loss -0.032831  0.002412\n",
            "Iteration 00391    Loss -0.032917  0.002065\n",
            "Iteration 00392    Loss -0.033037  0.002321\n",
            "Iteration 00393    Loss -0.033009  0.002502\n",
            "Iteration 00394    Loss -0.033217  0.002426\n",
            "Iteration 00395    Loss -0.033260  0.002587\n",
            "Iteration 00396    Loss -0.033432  0.002557\n",
            "Iteration 00397    Loss -0.033445  0.002450\n",
            "Iteration 00398    Loss -0.033424  0.002448\n",
            "Iteration 00399    Loss -0.033519  0.002690\n",
            "Iteration 00400    Loss -0.033611  0.002643\n",
            "Iteration 00401    Loss -0.033715  0.002582\n",
            "Iteration 00402    Loss -0.033784  0.002645\n",
            "Iteration 00403    Loss -0.033923  0.002750\n",
            "Iteration 00404    Loss -0.034020  0.002495\n",
            "Iteration 00405    Loss -0.033943  0.002714\n",
            "Iteration 00406    Loss -0.033847  0.002552\n",
            "Iteration 00407    Loss -0.033894  0.002949\n",
            "Iteration 00408    Loss -0.033947  0.002042\n",
            "Iteration 00409    Loss -0.034031  0.002399\n",
            "Iteration 00410    Loss -0.034131  0.002546\n",
            "Iteration 00411    Loss -0.034184  0.002382\n",
            "Iteration 00412    Loss -0.034227  0.002757\n",
            "Iteration 00413    Loss -0.034350  0.002842\n",
            "Iteration 00414    Loss -0.034430  0.002554\n",
            "Iteration 00415    Loss -0.034513  0.002729\n",
            "Iteration 00416    Loss -0.034405  0.002788\n",
            "Iteration 00417    Loss -0.034543  0.002926\n",
            "Iteration 00418    Loss -0.034466  0.002466\n",
            "Iteration 00419    Loss -0.034757  0.002645\n",
            "Iteration 00420    Loss -0.034701  0.002920\n",
            "Iteration 00421    Loss -0.034595  0.002469\n",
            "Iteration 00422    Loss -0.034740  0.002490\n",
            "Iteration 00423    Loss -0.034855  0.002746\n",
            "Iteration 00424    Loss -0.034881  0.002650\n",
            "Iteration 00425    Loss -0.035134  0.002709\n",
            "Iteration 00426    Loss -0.035112  0.002836\n",
            "Iteration 00427    Loss -0.035103  0.002846\n",
            "Iteration 00428    Loss -0.035249  0.002781\n",
            "Iteration 00429    Loss -0.035173  0.002820\n",
            "Iteration 00430    Loss -0.035386  0.002648\n",
            "Iteration 00431    Loss -0.035462  0.002764\n",
            "Iteration 00432    Loss -0.035390  0.003008\n",
            "Iteration 00433    Loss -0.035545  0.002737\n",
            "Iteration 00434    Loss -0.035438  0.003064\n",
            "Iteration 00435    Loss -0.035632  0.002951\n",
            "Iteration 00436    Loss -0.035519  0.002902\n",
            "Iteration 00437    Loss -0.035754  0.003081\n",
            "Iteration 00438    Loss -0.035643  0.002671\n",
            "Iteration 00439    Loss -0.035804  0.002863\n",
            "Iteration 00440    Loss -0.035853  0.003027\n",
            "Iteration 00441    Loss -0.035708  0.002860\n",
            "Iteration 00442    Loss -0.036033  0.002796\n",
            "Iteration 00443    Loss -0.035869  0.003155\n",
            "Iteration 00444    Loss -0.035995  0.002629\n",
            "Iteration 00445    Loss -0.036077  0.002975\n",
            "Iteration 00446    Loss -0.035999  0.002870\n",
            "Iteration 00447    Loss -0.036196  0.002858\n",
            "Iteration 00448    Loss -0.036186  0.003088\n",
            "Iteration 00449    Loss -0.036144  0.002889\n",
            "Iteration 00450    Loss -0.036313  0.003017\n",
            "Iteration 00451    Loss -0.036341  0.002965\n",
            "Iteration 00452    Loss -0.036333  0.002935\n",
            "Iteration 00453    Loss -0.036483  0.003245\n",
            "Iteration 00454    Loss -0.036627  0.002860\n",
            "Iteration 00455    Loss -0.036550  0.003088\n",
            "Iteration 00456    Loss -0.036534  0.003397\n",
            "Iteration 00457    Loss -0.036536  0.002511\n",
            "Iteration 00458    Loss -0.036606  0.002835\n",
            "Iteration 00459    Loss -0.036745  0.003320\n",
            "Iteration 00460    Loss -0.036712  0.002810\n",
            "Iteration 00461    Loss -0.036808  0.002703\n",
            "Iteration 00462    Loss -0.036892  0.003136\n",
            "Iteration 00463    Loss -0.036824  0.003075\n",
            "Iteration 00464    Loss -0.037072  0.003044\n",
            "Iteration 00465    Loss -0.036903  0.003022\n",
            "Iteration 00466    Loss -0.037106  0.003295\n",
            "Iteration 00467    Loss -0.037133  0.003104\n",
            "Iteration 00468    Loss -0.037142  0.003217\n",
            "Iteration 00469    Loss -0.037211  0.002956\n",
            "Iteration 00470    Loss -0.037302  0.003224\n",
            "Iteration 00471    Loss -0.037322  0.003282\n",
            "Iteration 00472    Loss -0.037407  0.003203\n",
            "Iteration 00473    Loss -0.037429  0.003044\n",
            "Iteration 00474    Loss -0.037409  0.003269\n",
            "Iteration 00475    Loss -0.037507  0.003024\n",
            "Iteration 00476    Loss -0.037498  0.003345\n",
            "Iteration 00477    Loss -0.037616  0.003102\n",
            "Iteration 00478    Loss -0.037579  0.003128\n",
            "Iteration 00479    Loss -0.037580  0.003314\n",
            "Iteration 00480    Loss -0.037560  0.003090\n",
            "Iteration 00481    Loss -0.037649  0.003251\n",
            "Iteration 00482    Loss -0.037832  0.003040\n",
            "Iteration 00483    Loss -0.037754  0.003458\n",
            "Iteration 00484    Loss -0.037868  0.003041\n",
            "Iteration 00485    Loss -0.037903  0.003325\n",
            "Iteration 00486    Loss -0.037684  0.003294\n",
            "Iteration 00487    Loss -0.038009  0.003270\n",
            "Iteration 00488    Loss -0.037950  0.003176\n",
            "Iteration 00489    Loss -0.037895  0.003649\n",
            "Iteration 00490    Loss -0.037968  0.002728\n",
            "Iteration 00491    Loss -0.038025  0.003088\n",
            "Iteration 00492    Loss -0.038063  0.003632\n",
            "Iteration 00493    Loss -0.038007  0.002956\n",
            "Iteration 00494    Loss -0.038192  0.003211\n",
            "Iteration 00495    Loss -0.038141  0.003434\n",
            "Iteration 00496    Loss -0.038194  0.003019\n",
            "Iteration 00497    Loss -0.038277  0.003420\n",
            "Iteration 00498    Loss -0.038261  0.003311\n",
            "Iteration 00499    Loss -0.038426  0.003239\n",
            " 99% 495/500 [4:42:29<02:49, 33.90s/it]/content/SOTS/outdoor/hazy/0407_0.95_0.16.jpg\n",
            "Iteration 00000    Loss 0.431637  0.000041\n",
            "Iteration 00001    Loss 46.744484  0.000016\n",
            "Iteration 00002    Loss 0.367403  0.000010\n",
            "Iteration 00003    Loss 2.505448  0.000008\n",
            "Iteration 00004    Loss 0.230487  0.000003\n",
            "Iteration 00005    Loss 0.284867  0.000002\n",
            "Iteration 00006    Loss 0.192224  0.000001\n",
            "Iteration 00007    Loss 0.138428  0.000001\n",
            "Iteration 00008    Loss 0.123009  0.000000\n",
            "Iteration 00009    Loss 0.110238  0.000000\n",
            "Iteration 00010    Loss 0.099267  0.000000\n",
            "Iteration 00011    Loss 0.089370  0.000000\n",
            "Iteration 00012    Loss 0.082686  0.000000\n",
            "Iteration 00013    Loss 0.079147  0.000000\n",
            "Iteration 00014    Loss 0.074456  0.000000\n",
            "Iteration 00015    Loss 0.070580  0.000000\n",
            "Iteration 00016    Loss 0.067685  0.000000\n",
            "Iteration 00017    Loss 0.065612  0.000000\n",
            "Iteration 00018    Loss 0.063849  0.000000\n",
            "Iteration 00019    Loss 0.062205  0.000000\n",
            "Iteration 00020    Loss 0.060949  0.000000\n",
            "Iteration 00021    Loss 0.059762  0.000000\n",
            "Iteration 00022    Loss 0.058549  0.000000\n",
            "Iteration 00023    Loss 0.057194  0.000000\n",
            "Iteration 00024    Loss 0.056369  0.000000\n",
            "Iteration 00025    Loss 0.055235  0.000000\n",
            "Iteration 00026    Loss 0.054182  0.000000\n",
            "Iteration 00027    Loss 0.053252  0.000000\n",
            "Iteration 00028    Loss 0.052246  0.000000\n",
            "Iteration 00029    Loss 0.051311  0.000000\n",
            "Iteration 00030    Loss 0.050323  0.000000\n",
            "Iteration 00031    Loss 0.049324  0.000000\n",
            "Iteration 00032    Loss 0.048417  0.000000\n",
            "Iteration 00033    Loss 0.047473  0.000000\n",
            "Iteration 00034    Loss 0.046527  0.000000\n",
            "Iteration 00035    Loss 0.045736  0.000000\n",
            "Iteration 00036    Loss 0.044795  0.000000\n",
            "Iteration 00037    Loss 0.043891  0.000000\n",
            "Iteration 00038    Loss 0.043064  0.000000\n",
            "Iteration 00039    Loss 0.041958  0.000000\n",
            "Iteration 00040    Loss 0.041327  0.000000\n",
            "Iteration 00041    Loss 0.040260  0.000000\n",
            "Iteration 00042    Loss 0.039581  0.000001\n",
            "Iteration 00043    Loss 0.038686  0.000001\n",
            "Iteration 00044    Loss 0.037817  0.000000\n",
            "Iteration 00045    Loss 0.037017  0.000000\n",
            "Iteration 00046    Loss 0.036112  0.000001\n",
            "Iteration 00047    Loss 0.035301  0.000001\n",
            "Iteration 00048    Loss 0.034667  0.000001\n",
            "Iteration 00049    Loss 0.034052  0.000001\n",
            "Iteration 00050    Loss 0.033058  0.000001\n",
            "Iteration 00051    Loss 0.032554  0.000001\n",
            "Iteration 00052    Loss 0.031717  0.000001\n",
            "Iteration 00053    Loss 0.030854  0.000001\n",
            "Iteration 00054    Loss 0.030025  0.000001\n",
            "Iteration 00055    Loss 0.029520  0.000001\n",
            "Iteration 00056    Loss 0.028693  0.000001\n",
            "Iteration 00057    Loss 0.028105  0.000001\n",
            "Iteration 00058    Loss 0.027218  0.000001\n",
            "Iteration 00059    Loss 0.026650  0.000001\n",
            "Iteration 00060    Loss 0.025952  0.000001\n",
            "Iteration 00061    Loss 0.025365  0.000001\n",
            "Iteration 00062    Loss 0.024743  0.000001\n",
            "Iteration 00063    Loss 0.024040  0.000001\n",
            "Iteration 00064    Loss 0.023371  0.000001\n",
            "Iteration 00065    Loss 0.022795  0.000001\n",
            "Iteration 00066    Loss 0.022168  0.000001\n",
            "Iteration 00067    Loss 0.021571  0.000001\n",
            "Iteration 00068    Loss 0.021022  0.000001\n",
            "Iteration 00069    Loss 0.020518  0.000001\n",
            "Iteration 00070    Loss 0.019921  0.000001\n",
            "Iteration 00071    Loss 0.019302  0.000001\n",
            "Iteration 00072    Loss 0.018838  0.000002\n",
            "Iteration 00073    Loss 0.018236  0.000001\n",
            "Iteration 00074    Loss 0.017632  0.000001\n",
            "Iteration 00075    Loss 0.017102  0.000002\n",
            "Iteration 00076    Loss 0.016656  0.000001\n",
            "Iteration 00077    Loss 0.016247  0.000002\n",
            "Iteration 00078    Loss 0.015686  0.000002\n",
            "Iteration 00079    Loss 0.015126  0.000002\n",
            "Iteration 00080    Loss 0.014749  0.000002\n",
            "Iteration 00081    Loss 0.014176  0.000002\n",
            "Iteration 00082    Loss 0.013695  0.000002\n",
            "Iteration 00083    Loss 0.013225  0.000002\n",
            "Iteration 00084    Loss 0.012787  0.000002\n",
            "Iteration 00085    Loss 0.012372  0.000002\n",
            "Iteration 00086    Loss 0.011834  0.000002\n",
            "Iteration 00087    Loss 0.011487  0.000002\n",
            "Iteration 00088    Loss 0.010999  0.000002\n",
            "Iteration 00089    Loss 0.010551  0.000002\n",
            "Iteration 00090    Loss 0.010213  0.000002\n",
            "Iteration 00091    Loss 0.009653  0.000002\n",
            "Iteration 00092    Loss 0.009322  0.000002\n",
            "Iteration 00093    Loss 0.008960  0.000002\n",
            "Iteration 00094    Loss 0.008445  0.000003\n",
            "Iteration 00095    Loss 0.008011  0.000003\n",
            "Iteration 00096    Loss 0.007665  0.000003\n",
            "Iteration 00097    Loss 0.007233  0.000003\n",
            "Iteration 00098    Loss 0.006848  0.000003\n",
            "Iteration 00099    Loss 0.006465  0.000003\n",
            "Iteration 00100    Loss 0.006040  0.000003\n",
            "Iteration 00101    Loss 0.005732  0.000003\n",
            "Iteration 00102    Loss 0.005343  0.000003\n",
            "Iteration 00103    Loss 0.004964  0.000003\n",
            "Iteration 00104    Loss 0.004600  0.000003\n",
            "Iteration 00105    Loss 0.004307  0.000003\n",
            "Iteration 00106    Loss 0.003885  0.000003\n",
            "Iteration 00107    Loss 0.003531  0.000004\n",
            "Iteration 00108    Loss 0.003217  0.000004\n",
            "Iteration 00109    Loss 0.002827  0.000004\n",
            "Iteration 00110    Loss 0.002535  0.000004\n",
            "Iteration 00111    Loss 0.002238  0.000004\n",
            "Iteration 00112    Loss 0.001876  0.000004\n",
            "Iteration 00113    Loss 0.001582  0.000004\n",
            "Iteration 00114    Loss 0.001298  0.000004\n",
            "Iteration 00115    Loss 0.000905  0.000004\n",
            "Iteration 00116    Loss 0.000645  0.000005\n",
            "Iteration 00117    Loss 0.000271  0.000005\n",
            "Iteration 00118    Loss -0.000041  0.000005\n",
            "Iteration 00119    Loss -0.000305  0.000005\n",
            "Iteration 00120    Loss -0.000604  0.000005\n",
            "Iteration 00121    Loss -0.000918  0.000006\n",
            "Iteration 00122    Loss -0.001230  0.000005\n",
            "Iteration 00123    Loss -0.001499  0.000006\n",
            "Iteration 00124    Loss -0.001812  0.000005\n",
            "Iteration 00125    Loss -0.002071  0.000005\n",
            "Iteration 00126    Loss -0.002394  0.000006\n",
            "Iteration 00127    Loss -0.002660  0.000006\n",
            "Iteration 00128    Loss -0.002957  0.000006\n",
            "Iteration 00129    Loss -0.003291  0.000006\n",
            "Iteration 00130    Loss -0.003484  0.000007\n",
            "Iteration 00131    Loss -0.003766  0.000006\n",
            "Iteration 00132    Loss -0.004070  0.000006\n",
            "Iteration 00133    Loss -0.004313  0.000006\n",
            "Iteration 00134    Loss -0.004541  0.000007\n",
            "Iteration 00135    Loss -0.004769  0.000007\n",
            "Iteration 00136    Loss -0.005095  0.000007\n",
            "Iteration 00137    Loss -0.005389  0.000007\n",
            "Iteration 00138    Loss -0.005641  0.000007\n",
            "Iteration 00139    Loss -0.005858  0.000007\n",
            "Iteration 00140    Loss -0.006122  0.000008\n",
            "Iteration 00141    Loss -0.006229  0.000008\n",
            "Iteration 00142    Loss -0.006533  0.000008\n",
            "Iteration 00143    Loss -0.006899  0.000008\n",
            "Iteration 00144    Loss -0.007065  0.000008\n",
            "Iteration 00145    Loss -0.007309  0.000009\n",
            "Iteration 00146    Loss -0.007530  0.000008\n",
            "Iteration 00147    Loss -0.007804  0.000008\n",
            "Iteration 00148    Loss -0.007865  0.000008\n",
            "Iteration 00149    Loss -0.008041  0.000008\n",
            "Iteration 00150    Loss -0.008365  0.000009\n",
            "Iteration 00151    Loss -0.008674  0.000009\n",
            "Iteration 00152    Loss -0.008876  0.000009\n",
            "Iteration 00153    Loss -0.009123  0.000010\n",
            "Iteration 00154    Loss -0.009197  0.000010\n",
            "Iteration 00155    Loss -0.009463  0.000010\n",
            "Iteration 00156    Loss -0.009633  0.000010\n",
            "Iteration 00157    Loss -0.009970  0.000008\n",
            "Iteration 00158    Loss -0.010146  0.000007\n",
            "Iteration 00159    Loss -0.010326  0.000008\n",
            "Iteration 00160    Loss -0.010566  0.000009\n",
            "Iteration 00161    Loss -0.010717  0.000010\n",
            "Iteration 00162    Loss -0.010901  0.000010\n",
            "Iteration 00163    Loss -0.011131  0.000010\n",
            "Iteration 00164    Loss -0.011301  0.000009\n",
            "Iteration 00165    Loss -0.011617  0.000009\n",
            "Iteration 00166    Loss -0.011796  0.000009\n",
            "Iteration 00167    Loss -0.011911  0.000010\n",
            "Iteration 00168    Loss -0.012069  0.000010\n",
            "Iteration 00169    Loss -0.012381  0.000010\n",
            "Iteration 00170    Loss -0.012469  0.000010\n",
            "Iteration 00171    Loss -0.012742  0.000010\n",
            "Iteration 00172    Loss -0.012980  0.000010\n",
            "Iteration 00173    Loss -0.013164  0.000010\n",
            "Iteration 00174    Loss -0.013280  0.000010\n",
            "Iteration 00175    Loss -0.013533  0.000010\n",
            "Iteration 00176    Loss -0.013696  0.000011\n",
            "Iteration 00177    Loss -0.013733  0.000011\n",
            "Iteration 00178    Loss -0.014047  0.000011\n",
            "Iteration 00179    Loss -0.014228  0.000010\n",
            "Iteration 00180    Loss -0.014388  0.000010\n",
            "Iteration 00181    Loss -0.014602  0.000010\n",
            "Iteration 00182    Loss -0.014804  0.000011\n",
            "Iteration 00183    Loss -0.014887  0.000011\n",
            "Iteration 00184    Loss -0.015038  0.000011\n",
            "Iteration 00185    Loss -0.015282  0.000010\n",
            "Iteration 00186    Loss -0.015517  0.000010\n",
            "Iteration 00187    Loss -0.015678  0.000011\n",
            "Iteration 00188    Loss -0.015802  0.000012\n",
            "Iteration 00189    Loss -0.015981  0.000012\n",
            "Iteration 00190    Loss -0.016070  0.000012\n",
            "Iteration 00191    Loss -0.016292  0.000011\n",
            "Iteration 00192    Loss -0.016407  0.000011\n",
            "Iteration 00193    Loss -0.016613  0.000011\n",
            "Iteration 00194    Loss -0.016820  0.000011\n",
            "Iteration 00195    Loss -0.016986  0.000012\n",
            "Iteration 00196    Loss -0.017106  0.000012\n",
            "Iteration 00197    Loss -0.017262  0.000012\n",
            "Iteration 00198    Loss -0.017353  0.000011\n",
            "Iteration 00199    Loss -0.017624  0.000011\n",
            "Iteration 00200    Loss -0.017728  0.000011\n",
            "Iteration 00201    Loss -0.017874  0.000011\n",
            "Iteration 00202    Loss -0.017996  0.000012\n",
            "Iteration 00203    Loss -0.018214  0.000013\n",
            "Iteration 00204    Loss -0.018344  0.000013\n",
            "Iteration 00205    Loss -0.018473  0.000012\n",
            "Iteration 00206    Loss -0.018653  0.000012\n",
            "Iteration 00207    Loss -0.018847  0.000012\n",
            "Iteration 00208    Loss -0.018826  0.000012\n",
            "Iteration 00209    Loss -0.019112  0.000012\n",
            "Iteration 00210    Loss -0.019270  0.000012\n",
            "Iteration 00211    Loss -0.019445  0.000012\n",
            "Iteration 00212    Loss -0.019418  0.000013\n",
            "Iteration 00213    Loss -0.019596  0.000013\n",
            "Iteration 00214    Loss -0.019827  0.000013\n",
            "Iteration 00215    Loss -0.019947  0.000012\n",
            "Iteration 00216    Loss -0.020086  0.000012\n",
            "Iteration 00217    Loss -0.020160  0.000012\n",
            "Iteration 00218    Loss -0.020344  0.000012\n",
            "Iteration 00219    Loss -0.020494  0.000012\n",
            "Iteration 00220    Loss -0.020516  0.000013\n",
            "Iteration 00221    Loss -0.020613  0.000014\n",
            "Iteration 00222    Loss -0.020964  0.000013\n",
            "Iteration 00223    Loss -0.020977  0.000012\n",
            "Iteration 00224    Loss -0.021154  0.000012\n",
            "Iteration 00225    Loss -0.021352  0.000013\n",
            "Iteration 00226    Loss -0.021415  0.000013\n",
            "Iteration 00227    Loss -0.021585  0.000013\n",
            "Iteration 00228    Loss -0.021575  0.000013\n",
            "Iteration 00229    Loss -0.021889  0.000012\n",
            "Iteration 00230    Loss -0.021998  0.000012\n",
            "Iteration 00231    Loss -0.022116  0.000013\n",
            "Iteration 00232    Loss -0.022283  0.000013\n",
            "Iteration 00233    Loss -0.022420  0.000014\n",
            "Iteration 00234    Loss -0.022425  0.000014\n",
            "Iteration 00235    Loss -0.022636  0.000013\n",
            "Iteration 00236    Loss -0.022832  0.000013\n",
            "Iteration 00237    Loss -0.022921  0.000013\n",
            "Iteration 00238    Loss -0.022988  0.000014\n",
            "Iteration 00239    Loss -0.023119  0.000014\n",
            "Iteration 00240    Loss -0.023192  0.000014\n",
            "Iteration 00241    Loss -0.023362  0.000014\n",
            "Iteration 00242    Loss -0.023492  0.000014\n",
            "Iteration 00243    Loss -0.023645  0.000014\n",
            "Iteration 00244    Loss -0.023776  0.000014\n",
            "Iteration 00245    Loss -0.023833  0.000015\n",
            "Iteration 00246    Loss -0.023826  0.000015\n",
            "Iteration 00247    Loss -0.024122  0.000015\n",
            "Iteration 00248    Loss -0.024262  0.000015\n",
            "Iteration 00249    Loss -0.024363  0.000014\n",
            "Iteration 00250    Loss -0.024485  0.000015\n",
            "Iteration 00251    Loss -0.024598  0.000015\n",
            "Iteration 00252    Loss -0.024699  0.000015\n",
            "Iteration 00253    Loss -0.024792  0.000015\n",
            "Iteration 00254    Loss -0.024912  0.000015\n",
            "Iteration 00255    Loss -0.025102  0.000015\n",
            "Iteration 00256    Loss -0.025227  0.000015\n",
            "Iteration 00257    Loss -0.025346  0.000016\n",
            "Iteration 00258    Loss -0.025411  0.000015\n",
            "Iteration 00259    Loss -0.025472  0.000016\n",
            "Iteration 00260    Loss -0.025617  0.000016\n",
            "Iteration 00261    Loss -0.025752  0.000015\n",
            "Iteration 00262    Loss -0.025939  0.000015\n",
            "Iteration 00263    Loss -0.025980  0.000016\n",
            "Iteration 00264    Loss -0.026060  0.000016\n",
            "Iteration 00265    Loss -0.026143  0.000016\n",
            "Iteration 00266    Loss -0.026282  0.000015\n",
            "Iteration 00267    Loss -0.026382  0.000016\n",
            "Iteration 00268    Loss -0.026590  0.000016\n",
            "Iteration 00269    Loss -0.026632  0.000016\n",
            "Iteration 00270    Loss -0.026664  0.000016\n",
            "Iteration 00271    Loss -0.026832  0.000015\n",
            "Iteration 00272    Loss -0.026927  0.000016\n",
            "Iteration 00273    Loss -0.027086  0.000017\n",
            "Iteration 00274    Loss -0.027029  0.000017\n",
            "Iteration 00275    Loss -0.027335  0.000017\n",
            "Iteration 00276    Loss -0.027414  0.000017\n",
            "Iteration 00277    Loss -0.027584  0.000017\n",
            "Iteration 00278    Loss -0.027618  0.000017\n",
            "Iteration 00279    Loss -0.027662  0.000017\n",
            "Iteration 00280    Loss -0.027846  0.000017\n",
            "Iteration 00281    Loss -0.027851  0.000016\n",
            "Iteration 00282    Loss -0.028002  0.000017\n",
            "Iteration 00283    Loss -0.028065  0.000018\n",
            "Iteration 00284    Loss -0.028149  0.000016\n",
            "Iteration 00285    Loss -0.028333  0.000016\n",
            "Iteration 00286    Loss -0.028341  0.000016\n",
            "Iteration 00287    Loss -0.028462  0.000016\n",
            "Iteration 00288    Loss -0.028492  0.000017\n",
            "Iteration 00289    Loss -0.028671  0.000018\n",
            "Iteration 00290    Loss -0.028752  0.000017\n",
            "Iteration 00291    Loss -0.028885  0.000016\n",
            "Iteration 00292    Loss -0.029023  0.000016\n",
            "Iteration 00293    Loss -0.029031  0.000018\n",
            "Iteration 00294    Loss -0.029099  0.000017\n",
            "Iteration 00295    Loss -0.029175  0.000017\n",
            "Iteration 00296    Loss -0.029330  0.000017\n",
            "Iteration 00297    Loss -0.029425  0.000017\n",
            "Iteration 00298    Loss -0.029466  0.000016\n",
            "Iteration 00299    Loss -0.029588  0.000018\n",
            "Iteration 00300    Loss -0.029730  0.000020\n",
            "Iteration 00301    Loss -0.029678  0.000019\n",
            "Iteration 00302    Loss -0.029908  0.000019\n",
            "Iteration 00303    Loss -0.030028  0.000017\n",
            "Iteration 00304    Loss -0.030076  0.000016\n",
            "Iteration 00305    Loss -0.030224  0.000017\n",
            "Iteration 00306    Loss -0.030204  0.000020\n",
            "Iteration 00307    Loss -0.030413  0.000020\n",
            "Iteration 00308    Loss -0.030381  0.000018\n",
            "Iteration 00309    Loss -0.030613  0.000018\n",
            "Iteration 00310    Loss -0.030591  0.000019\n",
            "Iteration 00311    Loss -0.030730  0.000019\n",
            "Iteration 00312    Loss -0.030940  0.000020\n",
            "Iteration 00313    Loss -0.030874  0.000021\n",
            "Iteration 00314    Loss -0.031033  0.000019\n",
            "Iteration 00315    Loss -0.030951  0.000017\n",
            "Iteration 00316    Loss -0.031189  0.000018\n",
            "Iteration 00317    Loss -0.031191  0.000019\n",
            "Iteration 00318    Loss -0.031382  0.000020\n",
            "Iteration 00319    Loss -0.031516  0.000020\n",
            "Iteration 00320    Loss -0.031516  0.000019\n",
            "Iteration 00321    Loss -0.031553  0.000019\n",
            "Iteration 00322    Loss -0.031732  0.000019\n",
            "Iteration 00323    Loss -0.031823  0.000019\n",
            "Iteration 00324    Loss -0.031869  0.000020\n",
            "Iteration 00325    Loss -0.031944  0.000021\n",
            "Iteration 00326    Loss -0.032122  0.000020\n",
            "Iteration 00327    Loss -0.032134  0.000019\n",
            "Iteration 00328    Loss -0.032271  0.000020\n",
            "Iteration 00329    Loss -0.032233  0.000022\n",
            "Iteration 00330    Loss -0.032297  0.000022\n",
            "Iteration 00331    Loss -0.032469  0.000020\n",
            "Iteration 00332    Loss -0.032527  0.000020\n",
            "Iteration 00333    Loss -0.032650  0.000021\n",
            "Iteration 00334    Loss -0.032750  0.000020\n",
            "Iteration 00335    Loss -0.032713  0.000020\n",
            "Iteration 00336    Loss -0.032822  0.000020\n",
            "Iteration 00337    Loss -0.032894  0.000020\n",
            "Iteration 00338    Loss -0.033079  0.000020\n",
            "Iteration 00339    Loss -0.033041  0.000020\n",
            "Iteration 00340    Loss -0.033180  0.000020\n",
            "Iteration 00341    Loss -0.033005  0.000019\n",
            "Iteration 00342    Loss -0.033254  0.000021\n",
            "Iteration 00343    Loss -0.033353  0.000021\n",
            "Iteration 00344    Loss -0.033385  0.000020\n",
            "Iteration 00345    Loss -0.033517  0.000020\n",
            "Iteration 00346    Loss -0.033420  0.000022\n",
            "Iteration 00347    Loss -0.033769  0.000021\n",
            "Iteration 00348    Loss -0.033786  0.000021\n",
            "Iteration 00349    Loss -0.033878  0.000022\n",
            "Iteration 00350    Loss -0.033898  0.000022\n",
            "Iteration 00351    Loss -0.033982  0.000020\n",
            "Iteration 00352    Loss -0.034055  0.000019\n",
            "Iteration 00353    Loss -0.034101  0.000021\n",
            "Iteration 00354    Loss -0.034202  0.000021\n",
            "Iteration 00355    Loss -0.034210  0.000021\n",
            "Iteration 00356    Loss -0.034386  0.000021\n",
            "Iteration 00357    Loss -0.034422  0.000021\n",
            "Iteration 00358    Loss -0.034512  0.000021\n",
            "Iteration 00359    Loss -0.034487  0.000022\n",
            "Iteration 00360    Loss -0.034546  0.000025\n",
            "Iteration 00361    Loss -0.034513  0.000023\n",
            "Iteration 00362    Loss -0.034694  0.000020\n",
            "Iteration 00363    Loss -0.034879  0.000021\n",
            "Iteration 00364    Loss -0.034858  0.000022\n",
            "Iteration 00365    Loss -0.034929  0.000022\n",
            "Iteration 00366    Loss -0.035061  0.000023\n",
            "Iteration 00367    Loss -0.034991  0.000023\n",
            "Iteration 00368    Loss -0.035201  0.000022\n",
            "Iteration 00369    Loss -0.035289  0.000023\n",
            "Iteration 00370    Loss -0.035255  0.000024\n",
            "Iteration 00371    Loss -0.035211  0.000022\n",
            "Iteration 00372    Loss -0.035394  0.000023\n",
            "Iteration 00373    Loss -0.035520  0.000026\n",
            "Iteration 00374    Loss -0.035541  0.000025\n",
            "Iteration 00375    Loss -0.035614  0.000022\n",
            "Iteration 00376    Loss -0.035733  0.000024\n",
            "Iteration 00377    Loss -0.035793  0.000027\n",
            "Iteration 00378    Loss -0.035872  0.000025\n",
            "Iteration 00379    Loss -0.035942  0.000022\n",
            "Iteration 00380    Loss -0.035922  0.000022\n",
            "Iteration 00381    Loss -0.035984  0.000023\n",
            "Iteration 00382    Loss -0.036071  0.000024\n",
            "Iteration 00383    Loss -0.036185  0.000025\n",
            "Iteration 00384    Loss -0.036169  0.000025\n",
            "Iteration 00385    Loss -0.036295  0.000025\n",
            "Iteration 00386    Loss -0.036216  0.000026\n",
            "Iteration 00387    Loss -0.036344  0.000027\n",
            "Iteration 00388    Loss -0.036422  0.000026\n",
            "Iteration 00389    Loss -0.036480  0.000024\n",
            "Iteration 00390    Loss -0.036483  0.000026\n",
            "Iteration 00391    Loss -0.036571  0.000028\n",
            "Iteration 00392    Loss -0.036755  0.000027\n",
            "Iteration 00393    Loss -0.036601  0.000025\n",
            "Iteration 00394    Loss -0.036782  0.000025\n",
            "Iteration 00395    Loss -0.036877  0.000026\n",
            "Iteration 00396    Loss -0.036824  0.000025\n",
            "Iteration 00397    Loss -0.036913  0.000028\n",
            "Iteration 00398    Loss -0.037017  0.000027\n",
            "Iteration 00399    Loss -0.037001  0.000025\n",
            "Iteration 00400    Loss -0.037154  0.000028\n",
            "Iteration 00401    Loss -0.037130  0.000027\n",
            "Iteration 00402    Loss -0.037261  0.000023\n",
            "Iteration 00403    Loss -0.037310  0.000023\n",
            "Iteration 00404    Loss -0.037340  0.000026\n",
            "Iteration 00405    Loss -0.037515  0.000028\n",
            "Iteration 00406    Loss -0.037524  0.000027\n",
            "Iteration 00407    Loss -0.037484  0.000026\n",
            "Iteration 00408    Loss -0.037676  0.000028\n",
            "Iteration 00409    Loss -0.037666  0.000027\n",
            "Iteration 00410    Loss -0.037748  0.000024\n",
            "Iteration 00411    Loss -0.037747  0.000024\n",
            "Iteration 00412    Loss -0.037865  0.000026\n",
            "Iteration 00413    Loss -0.037926  0.000029\n",
            "Iteration 00414    Loss -0.037949  0.000028\n",
            "Iteration 00415    Loss -0.038067  0.000026\n",
            "Iteration 00416    Loss -0.038128  0.000027\n",
            "Iteration 00417    Loss -0.038169  0.000027\n",
            "Iteration 00418    Loss -0.038072  0.000028\n",
            "Iteration 00419    Loss -0.038228  0.000028\n",
            "Iteration 00420    Loss -0.038313  0.000030\n",
            "Iteration 00421    Loss -0.038312  0.000027\n",
            "Iteration 00422    Loss -0.038447  0.000026\n",
            "Iteration 00423    Loss -0.038453  0.000029\n",
            "Iteration 00424    Loss -0.038541  0.000031\n",
            "Iteration 00425    Loss -0.038587  0.000028\n",
            "Iteration 00426    Loss -0.038678  0.000026\n",
            "Iteration 00427    Loss -0.038715  0.000028\n",
            "Iteration 00428    Loss -0.038767  0.000031\n",
            "Iteration 00429    Loss -0.038645  0.000030\n",
            "Iteration 00430    Loss -0.038922  0.000029\n",
            "Iteration 00431    Loss -0.038887  0.000028\n",
            "Iteration 00432    Loss -0.038962  0.000028\n",
            "Iteration 00433    Loss -0.039066  0.000031\n",
            "Iteration 00434    Loss -0.039108  0.000030\n",
            "Iteration 00435    Loss -0.039176  0.000028\n",
            "Iteration 00436    Loss -0.039189  0.000029\n",
            "Iteration 00437    Loss -0.039184  0.000030\n",
            "Iteration 00438    Loss -0.039327  0.000031\n",
            "Iteration 00439    Loss -0.039326  0.000029\n",
            "Iteration 00440    Loss -0.039408  0.000029\n",
            "Iteration 00441    Loss -0.039406  0.000030\n",
            "Iteration 00442    Loss -0.039490  0.000030\n",
            "Iteration 00443    Loss -0.039494  0.000030\n",
            "Iteration 00444    Loss -0.039569  0.000032\n",
            "Iteration 00445    Loss -0.039632  0.000030\n",
            "Iteration 00446    Loss -0.039684  0.000029\n",
            "Iteration 00447    Loss -0.039655  0.000032\n",
            "Iteration 00448    Loss -0.039690  0.000031\n",
            "Iteration 00449    Loss -0.039763  0.000030\n",
            "Iteration 00450    Loss -0.039812  0.000031\n",
            "Iteration 00451    Loss -0.039864  0.000029\n",
            "Iteration 00452    Loss -0.039940  0.000030\n",
            "Iteration 00453    Loss -0.039940  0.000031\n",
            "Iteration 00454    Loss -0.039998  0.000031\n",
            "Iteration 00455    Loss -0.039998  0.000032\n",
            "Iteration 00456    Loss -0.040143  0.000029\n",
            "Iteration 00457    Loss -0.040173  0.000028\n",
            "Iteration 00458    Loss -0.040177  0.000031\n",
            "Iteration 00459    Loss -0.040255  0.000033\n",
            "Iteration 00460    Loss -0.040267  0.000030\n",
            "Iteration 00461    Loss -0.040244  0.000030\n",
            "Iteration 00462    Loss -0.040333  0.000030\n",
            "Iteration 00463    Loss -0.040363  0.000030\n",
            "Iteration 00464    Loss -0.040432  0.000033\n",
            "Iteration 00465    Loss -0.040435  0.000032\n",
            "Iteration 00466    Loss -0.040436  0.000029\n",
            "Iteration 00467    Loss -0.040539  0.000031\n",
            "Iteration 00468    Loss -0.040597  0.000032\n",
            "Iteration 00469    Loss -0.040608  0.000031\n",
            "Iteration 00470    Loss -0.040621  0.000032\n",
            "Iteration 00471    Loss -0.040728  0.000034\n",
            "Iteration 00472    Loss -0.040720  0.000032\n",
            "Iteration 00473    Loss -0.040822  0.000030\n",
            "Iteration 00474    Loss -0.040822  0.000030\n",
            "Iteration 00475    Loss -0.040898  0.000034\n",
            "Iteration 00476    Loss -0.040885  0.000034\n",
            "Iteration 00477    Loss -0.040977  0.000031\n",
            "Iteration 00478    Loss -0.040949  0.000032\n",
            "Iteration 00479    Loss -0.041035  0.000034\n",
            "Iteration 00480    Loss -0.040999  0.000033\n",
            "Iteration 00481    Loss -0.041074  0.000031\n",
            "Iteration 00482    Loss -0.041074  0.000031\n",
            "Iteration 00483    Loss -0.041146  0.000033\n",
            "Iteration 00484    Loss -0.041212  0.000034\n",
            "Iteration 00485    Loss -0.041200  0.000033\n",
            "Iteration 00486    Loss -0.041178  0.000032\n",
            "Iteration 00487    Loss -0.041313  0.000031\n",
            "Iteration 00488    Loss -0.041360  0.000032\n",
            "Iteration 00489    Loss -0.041399  0.000034\n",
            "Iteration 00490    Loss -0.041453  0.000035\n",
            "Iteration 00491    Loss -0.041412  0.000033\n",
            "Iteration 00492    Loss -0.041513  0.000033\n",
            "Iteration 00493    Loss -0.041500  0.000033\n",
            "Iteration 00494    Loss -0.041563  0.000032\n",
            "Iteration 00495    Loss -0.041586  0.000035\n",
            "Iteration 00496    Loss -0.041641  0.000035\n",
            "Iteration 00497    Loss -0.041672  0.000032\n",
            "Iteration 00498    Loss -0.041698  0.000033\n",
            "Iteration 00499    Loss -0.041732  0.000033\n",
            " 99% 496/500 [4:43:05<02:18, 34.65s/it]/content/SOTS/outdoor/hazy/1774_0.95_0.2.jpg\n",
            "Iteration 00000    Loss 0.418870  0.000028\n",
            "Iteration 00001    Loss 20.210089  0.000011\n",
            "Iteration 00002    Loss 0.455496  0.000004\n",
            "Iteration 00003    Loss 2.031816  0.000003\n",
            "Iteration 00004    Loss 0.231969  0.000002\n",
            "Iteration 00005    Loss 2.205734  0.000002\n",
            "Iteration 00006    Loss 0.480578  0.000001\n",
            "Iteration 00007    Loss 0.263232  0.000001\n",
            "Iteration 00008    Loss 0.384218  0.000001\n",
            "Iteration 00009    Loss 0.160753  0.000001\n",
            "Iteration 00010    Loss 0.128875  0.000001\n",
            "Iteration 00011    Loss 0.117713  0.000001\n",
            "Iteration 00012    Loss 0.111871  0.000001\n",
            "Iteration 00013    Loss 0.106630  0.000001\n",
            "Iteration 00014    Loss 0.102032  0.000001\n",
            "Iteration 00015    Loss 0.097557  0.000001\n",
            "Iteration 00016    Loss 0.093638  0.000002\n",
            "Iteration 00017    Loss 0.089636  0.000002\n",
            "Iteration 00018    Loss 0.086906  0.000002\n",
            "Iteration 00019    Loss 0.084988  0.000003\n",
            "Iteration 00020    Loss 0.083573  0.000003\n",
            "Iteration 00021    Loss 0.081898  0.000004\n",
            "Iteration 00022    Loss 0.080393  0.000004\n",
            "Iteration 00023    Loss 0.079006  0.000004\n",
            "Iteration 00024    Loss 0.077683  0.000005\n",
            "Iteration 00025    Loss 0.076444  0.000005\n",
            "Iteration 00026    Loss 0.075204  0.000006\n",
            "Iteration 00027    Loss 0.074161  0.000006\n",
            "Iteration 00028    Loss 0.072907  0.000006\n",
            "Iteration 00029    Loss 0.071738  0.000008\n",
            "Iteration 00030    Loss 0.070603  0.000009\n",
            "Iteration 00031    Loss 0.069426  0.000009\n",
            "Iteration 00032    Loss 0.068281  0.000011\n",
            "Iteration 00033    Loss 0.067228  0.000012\n",
            "Iteration 00034    Loss 0.066152  0.000012\n",
            "Iteration 00035    Loss 0.065033  0.000012\n",
            "Iteration 00036    Loss 0.064041  0.000014\n",
            "Iteration 00037    Loss 0.062994  0.000014\n",
            "Iteration 00038    Loss 0.061914  0.000014\n",
            "Iteration 00039    Loss 0.060970  0.000015\n",
            "Iteration 00040    Loss 0.059906  0.000015\n",
            "Iteration 00041    Loss 0.058898  0.000016\n",
            "Iteration 00042    Loss 0.057544  0.000016\n",
            "Iteration 00043    Loss 0.071427  0.000016\n",
            "Iteration 00044    Loss 0.055634  0.000018\n",
            "Iteration 00045    Loss 0.055058  0.000021\n",
            "Iteration 00046    Loss 0.054167  0.000021\n",
            "Iteration 00047    Loss 0.053291  0.000021\n",
            "Iteration 00048    Loss 0.052404  0.000023\n",
            "Iteration 00049    Loss 0.051525  0.000024\n",
            "Iteration 00050    Loss 0.050655  0.000026\n",
            "Iteration 00051    Loss 0.049782  0.000026\n",
            "Iteration 00052    Loss 0.048915  0.000027\n",
            "Iteration 00053    Loss 0.048096  0.000031\n",
            "Iteration 00054    Loss 0.047243  0.000031\n",
            "Iteration 00055    Loss 0.046413  0.000034\n",
            "Iteration 00056    Loss 0.045620  0.000036\n",
            "Iteration 00057    Loss 0.044805  0.000042\n",
            "Iteration 00058    Loss 0.043957  0.000039\n",
            "Iteration 00059    Loss 0.043165  0.000042\n",
            "Iteration 00060    Loss 0.042417  0.000043\n",
            "Iteration 00061    Loss 0.041685  0.000046\n",
            "Iteration 00062    Loss 0.040921  0.000044\n",
            "Iteration 00063    Loss 0.040172  0.000053\n",
            "Iteration 00064    Loss 0.039486  0.000054\n",
            "Iteration 00065    Loss 0.038806  0.000048\n",
            "Iteration 00066    Loss 0.038085  0.000052\n",
            "Iteration 00067    Loss 0.037329  0.000056\n",
            "Iteration 00068    Loss 0.036623  0.000059\n",
            "Iteration 00069    Loss 0.035931  0.000056\n",
            "Iteration 00070    Loss 0.035254  0.000062\n",
            "Iteration 00071    Loss 0.034587  0.000070\n",
            "Iteration 00072    Loss 0.033910  0.000062\n",
            "Iteration 00073    Loss 0.033257  0.000067\n",
            "Iteration 00074    Loss 0.032618  0.000064\n",
            "Iteration 00075    Loss 0.031940  0.000077\n",
            "Iteration 00076    Loss 0.031319  0.000064\n",
            "Iteration 00077    Loss 0.030629  0.000071\n",
            "Iteration 00078    Loss 0.029990  0.000087\n",
            "Iteration 00079    Loss 0.029395  0.000073\n",
            "Iteration 00080    Loss 0.028766  0.000074\n",
            "Iteration 00081    Loss 0.028162  0.000083\n",
            "Iteration 00082    Loss 0.027586  0.000094\n",
            "Iteration 00083    Loss 0.027234  0.000085\n",
            "Iteration 00084    Loss 0.028443  0.000079\n",
            "Iteration 00085    Loss 0.027240  0.000098\n",
            "Iteration 00086    Loss 0.026683  0.000095\n",
            "Iteration 00087    Loss 0.025506  0.000087\n",
            "Iteration 00088    Loss 0.025242  0.000092\n",
            "Iteration 00089    Loss 0.024548  0.000103\n",
            "Iteration 00090    Loss 0.023929  0.000098\n",
            "Iteration 00091    Loss 0.023456  0.000100\n",
            "Iteration 00092    Loss 0.022782  0.000095\n",
            "Iteration 00093    Loss 0.022232  0.000107\n",
            "Iteration 00094    Loss 0.021798  0.000103\n",
            "Iteration 00095    Loss 0.021292  0.000110\n",
            "Iteration 00096    Loss 0.020773  0.000109\n",
            "Iteration 00097    Loss 0.020196  0.000114\n",
            "Iteration 00098    Loss 0.019645  0.000117\n",
            "Iteration 00099    Loss 0.019172  0.000113\n",
            "Iteration 00100    Loss 0.018728  0.000127\n",
            "Iteration 00101    Loss 0.018336  0.000112\n",
            "Iteration 00102    Loss 0.017770  0.000129\n",
            "Iteration 00103    Loss 0.017280  0.000118\n",
            "Iteration 00104    Loss 0.016893  0.000118\n",
            "Iteration 00105    Loss 0.016457  0.000123\n",
            "Iteration 00106    Loss 0.015988  0.000113\n",
            "Iteration 00107    Loss 0.015568  0.000131\n",
            "Iteration 00108    Loss 0.015150  0.000144\n",
            "Iteration 00109    Loss 0.014734  0.000126\n",
            "Iteration 00110    Loss 0.014305  0.000133\n",
            "Iteration 00111    Loss 0.013923  0.000140\n",
            "Iteration 00112    Loss 0.013452  0.000135\n",
            "Iteration 00113    Loss 0.013102  0.000148\n",
            "Iteration 00114    Loss 0.012664  0.000152\n",
            "Iteration 00115    Loss 0.012259  0.000147\n",
            "Iteration 00116    Loss 0.011864  0.000150\n",
            "Iteration 00117    Loss 0.011526  0.000158\n",
            "Iteration 00118    Loss 0.011129  0.000155\n",
            "Iteration 00119    Loss 0.010780  0.000167\n",
            "Iteration 00120    Loss 0.010411  0.000159\n",
            "Iteration 00121    Loss 0.010020  0.000170\n",
            "Iteration 00122    Loss 0.009729  0.000151\n",
            "Iteration 00123    Loss 0.009394  0.000166\n",
            "Iteration 00124    Loss 0.009053  0.000161\n",
            "Iteration 00125    Loss 0.008629  0.000158\n",
            "Iteration 00126    Loss 0.008250  0.000166\n",
            "Iteration 00127    Loss 0.007885  0.000173\n",
            "Iteration 00128    Loss 0.007543  0.000167\n",
            "Iteration 00129    Loss 0.007317  0.000176\n",
            "Iteration 00130    Loss 0.006857  0.000175\n",
            "Iteration 00131    Loss 0.006574  0.000175\n",
            "Iteration 00132    Loss 0.006287  0.000180\n",
            "Iteration 00133    Loss 0.005849  0.000183\n",
            "Iteration 00134    Loss 0.005502  0.000186\n",
            "Iteration 00135    Loss 0.005241  0.000190\n",
            "Iteration 00136    Loss 0.004880  0.000191\n",
            "Iteration 00137    Loss 0.004550  0.000195\n",
            "Iteration 00138    Loss 0.004298  0.000185\n",
            "Iteration 00139    Loss 0.003931  0.000210\n",
            "Iteration 00140    Loss 0.003643  0.000203\n",
            "Iteration 00141    Loss 0.003297  0.000201\n",
            "Iteration 00142    Loss 0.002974  0.000211\n",
            "Iteration 00143    Loss 0.002705  0.000213\n",
            "Iteration 00144    Loss 0.002423  0.000190\n",
            "Iteration 00145    Loss 0.002036  0.000219\n",
            "Iteration 00146    Loss 0.001911  0.000195\n",
            "Iteration 00147    Loss 0.001502  0.000222\n",
            "Iteration 00148    Loss 0.001150  0.000192\n",
            "Iteration 00149    Loss 0.000928  0.000200\n",
            "Iteration 00150    Loss 0.000500  0.000226\n",
            "Iteration 00151    Loss 0.000215  0.000206\n",
            "Iteration 00152    Loss 0.000007  0.000215\n",
            "Iteration 00153    Loss -0.000301  0.000243\n",
            "Iteration 00154    Loss -0.000675  0.000226\n",
            "Iteration 00155    Loss -0.000972  0.000212\n",
            "Iteration 00156    Loss -0.001235  0.000235\n",
            "Iteration 00157    Loss -0.001481  0.000240\n",
            "Iteration 00158    Loss -0.001701  0.000228\n",
            "Iteration 00159    Loss -0.002004  0.000254\n",
            "Iteration 00160    Loss -0.002123  0.000232\n",
            "Iteration 00161    Loss -0.002581  0.000253\n",
            "Iteration 00162    Loss -0.002709  0.000239\n",
            "Iteration 00163    Loss -0.003052  0.000225\n",
            "Iteration 00164    Loss -0.003379  0.000234\n",
            "Iteration 00165    Loss -0.003699  0.000226\n",
            "Iteration 00166    Loss -0.003930  0.000237\n",
            "Iteration 00167    Loss -0.004213  0.000259\n",
            "Iteration 00168    Loss -0.004467  0.000232\n",
            "Iteration 00169    Loss -0.004764  0.000235\n",
            "Iteration 00170    Loss -0.004838  0.000252\n",
            "Iteration 00171    Loss -0.005377  0.000253\n",
            "Iteration 00172    Loss -0.005349  0.000245\n",
            "Iteration 00173    Loss -0.006059  0.000260\n",
            "Iteration 00174    Loss -0.005972  0.000254\n",
            "Iteration 00175    Loss -0.006114  0.000261\n",
            "Iteration 00176    Loss -0.006569  0.000274\n",
            "Iteration 00177    Loss -0.006755  0.000256\n",
            "Iteration 00178    Loss -0.007034  0.000270\n",
            "Iteration 00179    Loss -0.007390  0.000278\n",
            "Iteration 00180    Loss -0.007544  0.000275\n",
            "Iteration 00181    Loss -0.007644  0.000284\n",
            "Iteration 00182    Loss -0.008223  0.000269\n",
            "Iteration 00183    Loss -0.008232  0.000285\n",
            "Iteration 00184    Loss -0.008285  0.000279\n",
            "Iteration 00185    Loss -0.008777  0.000286\n",
            "Iteration 00186    Loss -0.008817  0.000304\n",
            "Iteration 00187    Loss -0.009165  0.000274\n",
            "Iteration 00188    Loss -0.009433  0.000301\n",
            "Iteration 00189    Loss -0.009783  0.000292\n",
            "Iteration 00190    Loss -0.009834  0.000293\n",
            "Iteration 00191    Loss -0.010019  0.000299\n",
            "Iteration 00192    Loss -0.010610  0.000292\n",
            "Iteration 00193    Loss -0.010468  0.000315\n",
            "Iteration 00194    Loss -0.011060  0.000295\n",
            "Iteration 00195    Loss -0.011049  0.000296\n",
            "Iteration 00196    Loss -0.011344  0.000310\n",
            "Iteration 00197    Loss -0.011528  0.000281\n",
            "Iteration 00198    Loss -0.012039  0.000308\n",
            "Iteration 00199    Loss -0.012132  0.000310\n",
            "Iteration 00200    Loss -0.011855  0.000291\n",
            "Iteration 00201    Loss -0.012518  0.000325\n",
            "Iteration 00202    Loss -0.012742  0.000340\n",
            "Iteration 00203    Loss -0.012792  0.000295\n",
            "Iteration 00204    Loss -0.013147  0.000313\n",
            "Iteration 00205    Loss -0.012941  0.000349\n",
            "Iteration 00206    Loss -0.013566  0.000305\n",
            "Iteration 00207    Loss -0.013639  0.000299\n",
            "Iteration 00208    Loss -0.013306  0.000354\n",
            "Iteration 00209    Loss -0.014300  0.000331\n",
            "Iteration 00210    Loss -0.014479  0.000306\n",
            "Iteration 00211    Loss -0.014381  0.000338\n",
            "Iteration 00212    Loss -0.014439  0.000342\n",
            "Iteration 00213    Loss -0.014743  0.000348\n",
            "Iteration 00214    Loss -0.015436  0.000350\n",
            "Iteration 00215    Loss -0.015098  0.000363\n",
            "Iteration 00216    Loss -0.015687  0.000368\n",
            "Iteration 00217    Loss -0.015670  0.000357\n",
            "Iteration 00218    Loss -0.015233  0.000363\n",
            "Iteration 00219    Loss -0.016369  0.000371\n",
            "Iteration 00220    Loss -0.015907  0.000351\n",
            "Iteration 00221    Loss -0.016719  0.000374\n",
            "Iteration 00222    Loss -0.016642  0.000373\n",
            "Iteration 00223    Loss -0.017134  0.000372\n",
            "Iteration 00224    Loss -0.017206  0.000404\n",
            "Iteration 00225    Loss -0.017188  0.000364\n",
            "Iteration 00226    Loss -0.017487  0.000384\n",
            "Iteration 00227    Loss -0.017853  0.000376\n",
            "Iteration 00228    Loss -0.017837  0.000381\n",
            "Iteration 00229    Loss -0.018254  0.000384\n",
            "Iteration 00230    Loss -0.018257  0.000371\n",
            "Iteration 00231    Loss -0.018297  0.000366\n",
            "Iteration 00232    Loss -0.018641  0.000417\n",
            "Iteration 00233    Loss -0.018637  0.000382\n",
            "Iteration 00234    Loss -0.019098  0.000389\n",
            "Iteration 00235    Loss -0.018994  0.000386\n",
            "Iteration 00236    Loss -0.018930  0.000413\n",
            "Iteration 00237    Loss -0.019521  0.000398\n",
            "Iteration 00238    Loss -0.019785  0.000396\n",
            "Iteration 00239    Loss -0.019692  0.000435\n",
            "Iteration 00240    Loss -0.020063  0.000388\n",
            "Iteration 00241    Loss -0.020486  0.000394\n",
            "Iteration 00242    Loss -0.020271  0.000412\n",
            "Iteration 00243    Loss -0.020790  0.000435\n",
            "Iteration 00244    Loss -0.020430  0.000402\n",
            "Iteration 00245    Loss -0.020869  0.000418\n",
            "Iteration 00246    Loss -0.021140  0.000440\n",
            "Iteration 00247    Loss -0.021338  0.000404\n",
            "Iteration 00248    Loss -0.021184  0.000443\n",
            "Iteration 00249    Loss -0.021551  0.000453\n",
            "Iteration 00250    Loss -0.021600  0.000420\n",
            "Iteration 00251    Loss -0.021886  0.000427\n",
            "Iteration 00252    Loss -0.022115  0.000464\n",
            "Iteration 00253    Loss -0.022447  0.000437\n",
            "Iteration 00254    Loss -0.022169  0.000461\n",
            "Iteration 00255    Loss -0.022680  0.000455\n",
            "Iteration 00256    Loss -0.022715  0.000460\n",
            "Iteration 00257    Loss -0.022700  0.000439\n",
            "Iteration 00258    Loss -0.022984  0.000499\n",
            "Iteration 00259    Loss -0.022340  0.000438\n",
            "Iteration 00260    Loss -0.023512  0.000447\n",
            "Iteration 00261    Loss -0.022991  0.000496\n",
            "Iteration 00262    Loss -0.022977  0.000426\n",
            "Iteration 00263    Loss -0.023495  0.000444\n",
            "Iteration 00264    Loss -0.023660  0.000452\n",
            "Iteration 00265    Loss -0.023556  0.000397\n",
            "Iteration 00266    Loss -0.023564  0.000407\n",
            "Iteration 00267    Loss -0.023976  0.000439\n",
            "Iteration 00268    Loss -0.024396  0.000416\n",
            "Iteration 00269    Loss -0.024065  0.000428\n",
            "Iteration 00270    Loss -0.024561  0.000462\n",
            "Iteration 00271    Loss -0.024778  0.000451\n",
            "Iteration 00272    Loss -0.024997  0.000442\n",
            "Iteration 00273    Loss -0.025053  0.000453\n",
            "Iteration 00274    Loss -0.025336  0.000456\n",
            "Iteration 00275    Loss -0.025343  0.000475\n",
            "Iteration 00276    Loss -0.025348  0.000467\n",
            "Iteration 00277    Loss -0.025696  0.000482\n",
            "Iteration 00278    Loss -0.025115  0.000473\n",
            "Iteration 00279    Loss -0.026092  0.000470\n",
            "Iteration 00280    Loss -0.026044  0.000489\n",
            "Iteration 00281    Loss -0.025268  0.000507\n",
            "Iteration 00282    Loss -0.025820  0.000507\n",
            "Iteration 00283    Loss -0.026481  0.000527\n",
            "Iteration 00284    Loss -0.026755  0.000524\n",
            "Iteration 00285    Loss -0.026873  0.000518\n",
            "Iteration 00286    Loss -0.026948  0.000522\n",
            "Iteration 00287    Loss -0.027086  0.000556\n",
            "Iteration 00288    Loss -0.027127  0.000480\n",
            "Iteration 00289    Loss -0.027128  0.000551\n",
            "Iteration 00290    Loss -0.027363  0.000514\n",
            "Iteration 00291    Loss -0.027388  0.000539\n",
            "Iteration 00292    Loss -0.027270  0.000466\n",
            "Iteration 00293    Loss -0.027517  0.000550\n",
            "Iteration 00294    Loss -0.027411  0.000519\n",
            "Iteration 00295    Loss -0.027943  0.000466\n",
            "Iteration 00296    Loss -0.027946  0.000519\n",
            "Iteration 00297    Loss -0.028206  0.000527\n",
            "Iteration 00298    Loss -0.027976  0.000481\n",
            "Iteration 00299    Loss -0.028366  0.000529\n",
            "Iteration 00300    Loss -0.028484  0.000551\n",
            "Iteration 00301    Loss -0.028154  0.000516\n",
            "Iteration 00302    Loss -0.028587  0.000536\n",
            "Iteration 00303    Loss -0.028794  0.000563\n",
            "Iteration 00304    Loss -0.028448  0.000541\n",
            "Iteration 00305    Loss -0.029155  0.000560\n",
            "Iteration 00306    Loss -0.029147  0.000577\n",
            "Iteration 00307    Loss -0.029387  0.000537\n",
            "Iteration 00308    Loss -0.029074  0.000563\n",
            "Iteration 00309    Loss -0.029503  0.000606\n",
            "Iteration 00310    Loss -0.029676  0.000588\n",
            "Iteration 00311    Loss -0.029755  0.000578\n",
            "Iteration 00312    Loss -0.029754  0.000608\n",
            "Iteration 00313    Loss -0.029794  0.000577\n",
            "Iteration 00314    Loss -0.029919  0.000625\n",
            "Iteration 00315    Loss -0.029988  0.000607\n",
            "Iteration 00316    Loss -0.029920  0.000580\n",
            "Iteration 00317    Loss -0.030413  0.000613\n",
            "Iteration 00318    Loss -0.030137  0.000606\n",
            "Iteration 00319    Loss -0.030594  0.000608\n",
            "Iteration 00320    Loss -0.030260  0.000606\n",
            "Iteration 00321    Loss -0.030700  0.000608\n",
            "Iteration 00322    Loss -0.030918  0.000591\n",
            "Iteration 00323    Loss -0.030975  0.000644\n",
            "Iteration 00324    Loss -0.031024  0.000605\n",
            "Iteration 00325    Loss -0.031105  0.000648\n",
            "Iteration 00326    Loss -0.031245  0.000643\n",
            "Iteration 00327    Loss -0.031395  0.000601\n",
            "Iteration 00328    Loss -0.031339  0.000666\n",
            "Iteration 00329    Loss -0.031396  0.000566\n",
            "Iteration 00330    Loss -0.031560  0.000611\n",
            "Iteration 00331    Loss -0.030922  0.000649\n",
            "Iteration 00332    Loss -0.031744  0.000569\n",
            "Iteration 00333    Loss -0.031798  0.000583\n",
            "Iteration 00334    Loss -0.031819  0.000647\n",
            "Iteration 00335    Loss -0.032076  0.000599\n",
            "Iteration 00336    Loss -0.031919  0.000621\n",
            "Iteration 00337    Loss -0.032235  0.000668\n",
            "Iteration 00338    Loss -0.032173  0.000590\n",
            "Iteration 00339    Loss -0.032445  0.000638\n",
            "Iteration 00340    Loss -0.032167  0.000661\n",
            "Iteration 00341    Loss -0.032624  0.000632\n",
            "Iteration 00342    Loss -0.032590  0.000666\n",
            "Iteration 00343    Loss -0.032316  0.000639\n",
            "Iteration 00344    Loss -0.032682  0.000660\n",
            "Iteration 00345    Loss -0.032996  0.000654\n",
            "Iteration 00346    Loss -0.033026  0.000654\n",
            "Iteration 00347    Loss -0.033023  0.000670\n",
            "Iteration 00348    Loss -0.033243  0.000692\n",
            "Iteration 00349    Loss -0.033244  0.000657\n",
            "Iteration 00350    Loss -0.032994  0.000647\n",
            "Iteration 00351    Loss -0.033351  0.000663\n",
            "Iteration 00352    Loss -0.033437  0.000648\n",
            "Iteration 00353    Loss -0.033275  0.000716\n",
            "Iteration 00354    Loss -0.033532  0.000634\n",
            "Iteration 00355    Loss -0.033707  0.000690\n",
            "Iteration 00356    Loss -0.033796  0.000656\n",
            "Iteration 00357    Loss -0.033885  0.000691\n",
            "Iteration 00358    Loss -0.034005  0.000693\n",
            "Iteration 00359    Loss -0.034072  0.000666\n",
            "Iteration 00360    Loss -0.034083  0.000679\n",
            "Iteration 00361    Loss -0.034125  0.000674\n",
            "Iteration 00362    Loss -0.034120  0.000665\n",
            "Iteration 00363    Loss -0.034253  0.000686\n",
            "Iteration 00364    Loss -0.034324  0.000657\n",
            "Iteration 00365    Loss -0.034286  0.000668\n",
            "Iteration 00366    Loss -0.034429  0.000646\n",
            "Iteration 00367    Loss -0.034551  0.000650\n",
            "Iteration 00368    Loss -0.034560  0.000695\n",
            "Iteration 00369    Loss -0.034738  0.000668\n",
            "Iteration 00370    Loss -0.034781  0.000691\n",
            "Iteration 00371    Loss -0.034735  0.000633\n",
            "Iteration 00372    Loss -0.034803  0.000733\n",
            "Iteration 00373    Loss -0.034998  0.000647\n",
            "Iteration 00374    Loss -0.035127  0.000694\n",
            "Iteration 00375    Loss -0.035053  0.000705\n",
            "Iteration 00376    Loss -0.035058  0.000660\n",
            "Iteration 00377    Loss -0.035324  0.000694\n",
            "Iteration 00378    Loss -0.035315  0.000705\n",
            "Iteration 00379    Loss -0.035410  0.000691\n",
            "Iteration 00380    Loss -0.035553  0.000742\n",
            "Iteration 00381    Loss -0.035558  0.000671\n",
            "Iteration 00382    Loss -0.035606  0.000745\n",
            "Iteration 00383    Loss -0.035622  0.000680\n",
            "Iteration 00384    Loss -0.035665  0.000719\n",
            "Iteration 00385    Loss -0.035800  0.000698\n",
            "Iteration 00386    Loss -0.035820  0.000679\n",
            "Iteration 00387    Loss -0.035882  0.000712\n",
            "Iteration 00388    Loss -0.035903  0.000693\n",
            "Iteration 00389    Loss -0.036029  0.000672\n",
            "Iteration 00390    Loss -0.036095  0.000748\n",
            "Iteration 00391    Loss -0.036152  0.000657\n",
            "Iteration 00392    Loss -0.036150  0.000743\n",
            "Iteration 00393    Loss -0.036290  0.000696\n",
            "Iteration 00394    Loss -0.036103  0.000681\n",
            "Iteration 00395    Loss -0.036018  0.000748\n",
            "Iteration 00396    Loss -0.036452  0.000707\n",
            "Iteration 00397    Loss -0.036577  0.000699\n",
            "Iteration 00398    Loss -0.036636  0.000760\n",
            "Iteration 00399    Loss -0.036620  0.000728\n",
            "Iteration 00400    Loss -0.036770  0.000743\n",
            "Iteration 00401    Loss -0.036755  0.000720\n",
            "Iteration 00402    Loss -0.036842  0.000778\n",
            "Iteration 00403    Loss -0.036851  0.000727\n",
            "Iteration 00404    Loss -0.036942  0.000755\n",
            "Iteration 00405    Loss -0.037032  0.000703\n",
            "Iteration 00406    Loss -0.037080  0.000785\n",
            "Iteration 00407    Loss -0.037063  0.000717\n",
            "Iteration 00408    Loss -0.037009  0.000739\n",
            "Iteration 00409    Loss -0.037032  0.000668\n",
            "Iteration 00410    Loss -0.037172  0.000769\n",
            "Iteration 00411    Loss -0.037258  0.000689\n",
            "Iteration 00412    Loss -0.037287  0.000669\n",
            "Iteration 00413    Loss -0.037385  0.000752\n",
            "Iteration 00414    Loss -0.037454  0.000725\n",
            "Iteration 00415    Loss -0.037498  0.000666\n",
            "Iteration 00416    Loss -0.037597  0.000749\n",
            "Iteration 00417    Loss -0.037597  0.000724\n",
            "Iteration 00418    Loss -0.037515  0.000719\n",
            "Iteration 00419    Loss -0.037697  0.000744\n",
            "Iteration 00420    Loss -0.037784  0.000774\n",
            "Iteration 00421    Loss -0.037585  0.000714\n",
            "Iteration 00422    Loss -0.037831  0.000750\n",
            "Iteration 00423    Loss -0.037543  0.000795\n",
            "Iteration 00424    Loss -0.037850  0.000661\n",
            "Iteration 00425    Loss -0.037963  0.000744\n",
            "Iteration 00426    Loss -0.038034  0.000740\n",
            "Iteration 00427    Loss -0.038068  0.000730\n",
            "Iteration 00428    Loss -0.037607  0.000766\n",
            "Iteration 00429    Loss -0.038148  0.000733\n",
            "Iteration 00430    Loss -0.038257  0.000738\n",
            "Iteration 00431    Loss -0.037777  0.000747\n",
            "Iteration 00432    Loss -0.038324  0.000756\n",
            "Iteration 00433    Loss -0.038366  0.000728\n",
            "Iteration 00434    Loss -0.038460  0.000766\n",
            "Iteration 00435    Loss -0.038463  0.000741\n",
            "Iteration 00436    Loss -0.038515  0.000772\n",
            "Iteration 00437    Loss -0.038586  0.000750\n",
            "Iteration 00438    Loss -0.038595  0.000780\n",
            "Iteration 00439    Loss -0.038625  0.000725\n",
            "Iteration 00440    Loss -0.038670  0.000789\n",
            "Iteration 00441    Loss -0.038690  0.000715\n",
            "Iteration 00442    Loss -0.038794  0.000776\n",
            "Iteration 00443    Loss -0.038826  0.000748\n",
            "Iteration 00444    Loss -0.038889  0.000752\n",
            "Iteration 00445    Loss -0.038919  0.000771\n",
            "Iteration 00446    Loss -0.038934  0.000731\n",
            "Iteration 00447    Loss -0.039013  0.000770\n",
            "Iteration 00448    Loss -0.039042  0.000775\n",
            "Iteration 00449    Loss -0.039103  0.000730\n",
            "Iteration 00450    Loss -0.038829  0.000798\n",
            "Iteration 00451    Loss -0.039171  0.000783\n",
            "Iteration 00452    Loss -0.039185  0.000756\n",
            "Iteration 00453    Loss -0.039246  0.000811\n",
            "Iteration 00454    Loss -0.039351  0.000757\n",
            "Iteration 00455    Loss -0.039356  0.000791\n",
            "Iteration 00456    Loss -0.039394  0.000789\n",
            "Iteration 00457    Loss -0.039462  0.000810\n",
            "Iteration 00458    Loss -0.039462  0.000765\n",
            "Iteration 00459    Loss -0.039487  0.000808\n",
            "Iteration 00460    Loss -0.039467  0.000784\n",
            "Iteration 00461    Loss -0.039426  0.000746\n",
            "Iteration 00462    Loss -0.039609  0.000821\n",
            "Iteration 00463    Loss -0.039659  0.000752\n",
            "Iteration 00464    Loss -0.039609  0.000795\n",
            "Iteration 00465    Loss -0.039728  0.000777\n",
            "Iteration 00466    Loss -0.039760  0.000779\n",
            "Iteration 00467    Loss -0.039839  0.000791\n",
            "Iteration 00468    Loss -0.039841  0.000794\n",
            "Iteration 00469    Loss -0.039885  0.000812\n",
            "Iteration 00470    Loss -0.039931  0.000794\n",
            "Iteration 00471    Loss -0.039759  0.000811\n",
            "Iteration 00472    Loss -0.039975  0.000763\n",
            "Iteration 00473    Loss -0.039986  0.000806\n",
            "Iteration 00474    Loss -0.040069  0.000803\n",
            "Iteration 00475    Loss -0.040082  0.000805\n",
            "Iteration 00476    Loss -0.040107  0.000752\n",
            "Iteration 00477    Loss -0.040208  0.000811\n",
            "Iteration 00478    Loss -0.040071  0.000822\n",
            "Iteration 00479    Loss -0.040245  0.000777\n",
            "Iteration 00480    Loss -0.040313  0.000826\n",
            "Iteration 00481    Loss -0.040331  0.000811\n",
            "Iteration 00482    Loss -0.040320  0.000840\n",
            "Iteration 00483    Loss -0.040379  0.000769\n",
            "Iteration 00484    Loss -0.040351  0.000874\n",
            "Iteration 00485    Loss -0.040365  0.000739\n",
            "Iteration 00486    Loss -0.040458  0.000768\n",
            "Iteration 00487    Loss -0.040477  0.000845\n",
            "Iteration 00488    Loss -0.040488  0.000714\n",
            "Iteration 00489    Loss -0.040562  0.000821\n",
            "Iteration 00490    Loss -0.040552  0.000797\n",
            "Iteration 00491    Loss -0.040613  0.000811\n",
            "Iteration 00492    Loss -0.040639  0.000754\n",
            "Iteration 00493    Loss -0.040686  0.000832\n",
            "Iteration 00494    Loss -0.040711  0.000820\n",
            "Iteration 00495    Loss -0.040776  0.000781\n",
            "Iteration 00496    Loss -0.040817  0.000815\n",
            "Iteration 00497    Loss -0.040758  0.000827\n",
            "Iteration 00498    Loss -0.040905  0.000785\n",
            "Iteration 00499    Loss -0.040923  0.000829\n",
            " 99% 497/500 [4:43:46<01:49, 36.39s/it]/content/SOTS/outdoor/hazy/1712_0.8_0.08.jpg\n",
            "Iteration 00000    Loss 0.421134  0.000038\n",
            "Iteration 00001    Loss 29.547771  0.000012\n",
            "Iteration 00002    Loss 0.720071  0.000005\n",
            "Iteration 00003    Loss 0.350051  0.000002\n",
            "Iteration 00004    Loss 2.071842  0.000002\n",
            "Iteration 00005    Loss 0.195048  0.000002\n",
            "Iteration 00006    Loss 0.208531  0.000002\n",
            "Iteration 00007    Loss 0.172387  0.000001\n",
            "Iteration 00008    Loss 0.140047  0.000001\n",
            "Iteration 00009    Loss 0.115891  0.000002\n",
            "Iteration 00010    Loss 0.101742  0.000002\n",
            "Iteration 00011    Loss 0.091945  0.000002\n",
            "Iteration 00012    Loss 0.084812  0.000002\n",
            "Iteration 00013    Loss 0.078532  0.000001\n",
            "Iteration 00014    Loss 0.074376  0.000002\n",
            "Iteration 00015    Loss 0.070741  0.000003\n",
            "Iteration 00016    Loss 0.067793  0.000003\n",
            "Iteration 00017    Loss 0.064805  0.000003\n",
            "Iteration 00018    Loss 0.062374  0.000004\n",
            "Iteration 00019    Loss 0.060599  0.000005\n",
            "Iteration 00020    Loss 0.058569  0.000006\n",
            "Iteration 00021    Loss 0.056999  0.000007\n",
            "Iteration 00022    Loss 0.055510  0.000008\n",
            "Iteration 00023    Loss 0.053915  0.000009\n",
            "Iteration 00024    Loss 0.052581  0.000010\n",
            "Iteration 00025    Loss 0.051416  0.000010\n",
            "Iteration 00026    Loss 0.050126  0.000009\n",
            "Iteration 00027    Loss 0.048896  0.000010\n",
            "Iteration 00028    Loss 0.047850  0.000012\n",
            "Iteration 00029    Loss 0.046605  0.000014\n",
            "Iteration 00030    Loss 0.045505  0.000016\n",
            "Iteration 00031    Loss 0.044459  0.000017\n",
            "Iteration 00032    Loss 0.043459  0.000022\n",
            "Iteration 00033    Loss 0.042317  0.000018\n",
            "Iteration 00034    Loss 0.041236  0.000018\n",
            "Iteration 00035    Loss 0.040144  0.000020\n",
            "Iteration 00036    Loss 0.039133  0.000020\n",
            "Iteration 00037    Loss 0.038103  0.000019\n",
            "Iteration 00038    Loss 0.036927  0.000021\n",
            "Iteration 00039    Loss 0.035929  0.000024\n",
            "Iteration 00040    Loss 0.034931  0.000023\n",
            "Iteration 00041    Loss 0.033982  0.000025\n",
            "Iteration 00042    Loss 0.032891  0.000029\n",
            "Iteration 00043    Loss 0.031939  0.000027\n",
            "Iteration 00044    Loss 0.031027  0.000027\n",
            "Iteration 00045    Loss 0.030025  0.000028\n",
            "Iteration 00046    Loss 0.029067  0.000027\n",
            "Iteration 00047    Loss 0.028132  0.000029\n",
            "Iteration 00048    Loss 0.027187  0.000030\n",
            "Iteration 00049    Loss 0.026480  0.000031\n",
            "Iteration 00050    Loss 0.025641  0.000030\n",
            "Iteration 00051    Loss 0.024675  0.000035\n",
            "Iteration 00052    Loss 0.023859  0.000030\n",
            "Iteration 00053    Loss 0.023087  0.000037\n",
            "Iteration 00054    Loss 0.022174  0.000031\n",
            "Iteration 00055    Loss 0.021297  0.000034\n",
            "Iteration 00056    Loss 0.020273  0.000036\n",
            "Iteration 00057    Loss 0.019339  0.000035\n",
            "Iteration 00058    Loss 0.018504  0.000039\n",
            "Iteration 00059    Loss 0.017607  0.000042\n",
            "Iteration 00060    Loss 0.016611  0.000037\n",
            "Iteration 00061    Loss 0.015881  0.000040\n",
            "Iteration 00062    Loss 0.015170  0.000043\n",
            "Iteration 00063    Loss 0.014352  0.000037\n",
            "Iteration 00064    Loss 0.013574  0.000040\n",
            "Iteration 00065    Loss 0.012981  0.000044\n",
            "Iteration 00066    Loss 0.012146  0.000041\n",
            "Iteration 00067    Loss 0.011413  0.000044\n",
            "Iteration 00068    Loss 0.010731  0.000048\n",
            "Iteration 00069    Loss 0.010075  0.000046\n",
            "Iteration 00070    Loss 0.009342  0.000044\n",
            "Iteration 00071    Loss 0.008662  0.000051\n",
            "Iteration 00072    Loss 0.008007  0.000050\n",
            "Iteration 00073    Loss 0.007369  0.000046\n",
            "Iteration 00074    Loss 0.006788  0.000048\n",
            "Iteration 00075    Loss 0.006097  0.000053\n",
            "Iteration 00076    Loss 0.005497  0.000052\n",
            "Iteration 00077    Loss 0.004844  0.000056\n",
            "Iteration 00078    Loss 0.004170  0.000054\n",
            "Iteration 00079    Loss 0.003592  0.000052\n",
            "Iteration 00080    Loss 0.002883  0.000057\n",
            "Iteration 00081    Loss 0.002241  0.000051\n",
            "Iteration 00082    Loss 0.001812  0.000064\n",
            "Iteration 00083    Loss 0.001487  0.000042\n",
            "Iteration 00084    Loss 0.000758  0.000054\n",
            "Iteration 00085    Loss 0.000283  0.000058\n",
            "Iteration 00086    Loss -0.000421  0.000054\n",
            "Iteration 00087    Loss -0.000847  0.000054\n",
            "Iteration 00088    Loss -0.001405  0.000057\n",
            "Iteration 00089    Loss -0.001902  0.000067\n",
            "Iteration 00090    Loss -0.002405  0.000066\n",
            "Iteration 00091    Loss -0.003011  0.000060\n",
            "Iteration 00092    Loss -0.003527  0.000063\n",
            "Iteration 00093    Loss -0.004000  0.000068\n",
            "Iteration 00094    Loss -0.004478  0.000067\n",
            "Iteration 00095    Loss -0.004971  0.000064\n",
            "Iteration 00096    Loss -0.005385  0.000064\n",
            "Iteration 00097    Loss -0.005844  0.000066\n",
            "Iteration 00098    Loss -0.006280  0.000071\n",
            "Iteration 00099    Loss -0.006694  0.000074\n",
            "Iteration 00100    Loss -0.007115  0.000073\n",
            "Iteration 00101    Loss -0.007518  0.000068\n",
            "Iteration 00102    Loss -0.007938  0.000076\n",
            "Iteration 00103    Loss -0.008326  0.000082\n",
            "Iteration 00104    Loss -0.008653  0.000077\n",
            "Iteration 00105    Loss -0.009094  0.000077\n",
            "Iteration 00106    Loss -0.009491  0.000078\n",
            "Iteration 00107    Loss -0.009848  0.000080\n",
            "Iteration 00108    Loss -0.010183  0.000083\n",
            "Iteration 00109    Loss -0.010595  0.000079\n",
            "Iteration 00110    Loss -0.010964  0.000084\n",
            "Iteration 00111    Loss -0.011324  0.000089\n",
            "Iteration 00112    Loss -0.011657  0.000086\n",
            "Iteration 00113    Loss -0.011958  0.000095\n",
            "Iteration 00114    Loss -0.012249  0.000087\n",
            "Iteration 00115    Loss -0.012632  0.000091\n",
            "Iteration 00116    Loss -0.012963  0.000093\n",
            "Iteration 00117    Loss -0.013239  0.000091\n",
            "Iteration 00118    Loss -0.013534  0.000096\n",
            "Iteration 00119    Loss -0.013849  0.000095\n",
            "Iteration 00120    Loss -0.014146  0.000103\n",
            "Iteration 00121    Loss -0.014411  0.000105\n",
            "Iteration 00122    Loss -0.014662  0.000097\n",
            "Iteration 00123    Loss -0.014916  0.000113\n",
            "Iteration 00124    Loss -0.015073  0.000103\n",
            "Iteration 00125    Loss -0.015517  0.000106\n",
            "Iteration 00126    Loss -0.015830  0.000119\n",
            "Iteration 00127    Loss -0.016109  0.000105\n",
            "Iteration 00128    Loss -0.016357  0.000117\n",
            "Iteration 00129    Loss -0.016638  0.000115\n",
            "Iteration 00130    Loss -0.016882  0.000113\n",
            "Iteration 00131    Loss -0.017147  0.000115\n",
            "Iteration 00132    Loss -0.017397  0.000114\n",
            "Iteration 00133    Loss -0.017645  0.000121\n",
            "Iteration 00134    Loss -0.017920  0.000126\n",
            "Iteration 00135    Loss -0.018208  0.000121\n",
            "Iteration 00136    Loss -0.018424  0.000127\n",
            "Iteration 00137    Loss -0.018674  0.000129\n",
            "Iteration 00138    Loss -0.018877  0.000129\n",
            "Iteration 00139    Loss -0.019059  0.000135\n",
            "Iteration 00140    Loss -0.019242  0.000124\n",
            "Iteration 00141    Loss -0.019488  0.000142\n",
            "Iteration 00142    Loss -0.019730  0.000141\n",
            "Iteration 00143    Loss -0.020047  0.000142\n",
            "Iteration 00144    Loss -0.020248  0.000147\n",
            "Iteration 00145    Loss -0.020452  0.000134\n",
            "Iteration 00146    Loss -0.020599  0.000167\n",
            "Iteration 00147    Loss -0.020849  0.000133\n",
            "Iteration 00148    Loss -0.021043  0.000159\n",
            "Iteration 00149    Loss -0.021132  0.000156\n",
            "Iteration 00150    Loss -0.021417  0.000147\n",
            "Iteration 00151    Loss -0.021673  0.000164\n",
            "Iteration 00152    Loss -0.021853  0.000175\n",
            "Iteration 00153    Loss -0.022068  0.000160\n",
            "Iteration 00154    Loss -0.022254  0.000165\n",
            "Iteration 00155    Loss -0.022487  0.000187\n",
            "Iteration 00156    Loss -0.022655  0.000183\n",
            "Iteration 00157    Loss -0.022757  0.000169\n",
            "Iteration 00158    Loss -0.022837  0.000173\n",
            "Iteration 00159    Loss -0.022934  0.000176\n",
            "Iteration 00160    Loss -0.022758  0.000184\n",
            "Iteration 00161    Loss -0.023200  0.000140\n",
            "Iteration 00162    Loss -0.023292  0.000152\n",
            "Iteration 00163    Loss -0.023363  0.000193\n",
            "Iteration 00164    Loss -0.023637  0.000168\n",
            "Iteration 00165    Loss -0.023912  0.000133\n",
            "Iteration 00166    Loss -0.024133  0.000152\n",
            "Iteration 00167    Loss -0.024321  0.000172\n",
            "Iteration 00168    Loss -0.024559  0.000175\n",
            "Iteration 00169    Loss -0.024675  0.000172\n",
            "Iteration 00170    Loss -0.024824  0.000177\n",
            "Iteration 00171    Loss -0.024909  0.000174\n",
            "Iteration 00172    Loss -0.025146  0.000164\n",
            "Iteration 00173    Loss -0.025365  0.000178\n",
            "Iteration 00174    Loss -0.025494  0.000196\n",
            "Iteration 00175    Loss -0.025651  0.000198\n",
            "Iteration 00176    Loss -0.025721  0.000199\n",
            "Iteration 00177    Loss -0.025941  0.000184\n",
            "Iteration 00178    Loss -0.026105  0.000187\n",
            "Iteration 00179    Loss -0.026341  0.000198\n",
            "Iteration 00180    Loss -0.026453  0.000217\n",
            "Iteration 00181    Loss -0.026557  0.000228\n",
            "Iteration 00182    Loss -0.026712  0.000204\n",
            "Iteration 00183    Loss -0.026855  0.000194\n",
            "Iteration 00184    Loss -0.027013  0.000221\n",
            "Iteration 00185    Loss -0.027219  0.000225\n",
            "Iteration 00186    Loss -0.027372  0.000236\n",
            "Iteration 00187    Loss -0.027420  0.000245\n",
            "Iteration 00188    Loss -0.027506  0.000217\n",
            "Iteration 00189    Loss -0.027624  0.000213\n",
            "Iteration 00190    Loss -0.027739  0.000225\n",
            "Iteration 00191    Loss -0.027924  0.000233\n",
            "Iteration 00192    Loss -0.028013  0.000240\n",
            "Iteration 00193    Loss -0.028195  0.000231\n",
            "Iteration 00194    Loss -0.028378  0.000255\n",
            "Iteration 00195    Loss -0.028467  0.000254\n",
            "Iteration 00196    Loss -0.028552  0.000238\n",
            "Iteration 00197    Loss -0.028709  0.000261\n",
            "Iteration 00198    Loss -0.028804  0.000261\n",
            "Iteration 00199    Loss -0.028950  0.000269\n",
            "Iteration 00200    Loss -0.029092  0.000254\n",
            "Iteration 00201    Loss -0.029223  0.000265\n",
            "Iteration 00202    Loss -0.029344  0.000273\n",
            "Iteration 00203    Loss -0.029394  0.000243\n",
            "Iteration 00204    Loss -0.029530  0.000270\n",
            "Iteration 00205    Loss -0.029661  0.000262\n",
            "Iteration 00206    Loss -0.029704  0.000278\n",
            "Iteration 00207    Loss -0.029906  0.000280\n",
            "Iteration 00208    Loss -0.030008  0.000267\n",
            "Iteration 00209    Loss -0.030097  0.000278\n",
            "Iteration 00210    Loss -0.030172  0.000295\n",
            "Iteration 00211    Loss -0.030128  0.000272\n",
            "Iteration 00212    Loss -0.030175  0.000279\n",
            "Iteration 00213    Loss -0.030372  0.000261\n",
            "Iteration 00214    Loss -0.030525  0.000269\n",
            "Iteration 00215    Loss -0.030590  0.000293\n",
            "Iteration 00216    Loss -0.030695  0.000292\n",
            "Iteration 00217    Loss -0.030816  0.000278\n",
            "Iteration 00218    Loss -0.030894  0.000277\n",
            "Iteration 00219    Loss -0.031078  0.000292\n",
            "Iteration 00220    Loss -0.031205  0.000301\n",
            "Iteration 00221    Loss -0.031279  0.000279\n",
            "Iteration 00222    Loss -0.031398  0.000305\n",
            "Iteration 00223    Loss -0.031488  0.000316\n",
            "Iteration 00224    Loss -0.031586  0.000293\n",
            "Iteration 00225    Loss -0.031673  0.000303\n",
            "Iteration 00226    Loss -0.031767  0.000308\n",
            "Iteration 00227    Loss -0.031833  0.000315\n",
            "Iteration 00228    Loss -0.031909  0.000277\n",
            "Iteration 00229    Loss -0.031933  0.000327\n",
            "Iteration 00230    Loss -0.032000  0.000287\n",
            "Iteration 00231    Loss -0.032167  0.000297\n",
            "Iteration 00232    Loss -0.032257  0.000311\n",
            "Iteration 00233    Loss -0.032404  0.000307\n",
            "Iteration 00234    Loss -0.032458  0.000294\n",
            "Iteration 00235    Loss -0.032553  0.000333\n",
            "Iteration 00236    Loss -0.032639  0.000345\n",
            "Iteration 00237    Loss -0.032544  0.000281\n",
            "Iteration 00238    Loss -0.032821  0.000331\n",
            "Iteration 00239    Loss -0.032893  0.000330\n",
            "Iteration 00240    Loss -0.032902  0.000311\n",
            "Iteration 00241    Loss -0.033086  0.000322\n",
            "Iteration 00242    Loss -0.033109  0.000308\n",
            "Iteration 00243    Loss -0.033163  0.000327\n",
            "Iteration 00244    Loss -0.033262  0.000307\n",
            "Iteration 00245    Loss -0.033404  0.000343\n",
            "Iteration 00246    Loss -0.033402  0.000329\n",
            "Iteration 00247    Loss -0.033437  0.000307\n",
            "Iteration 00248    Loss -0.033651  0.000334\n",
            "Iteration 00249    Loss -0.033727  0.000344\n",
            "Iteration 00250    Loss -0.033732  0.000338\n",
            "Iteration 00251    Loss -0.033797  0.000311\n",
            "Iteration 00252    Loss -0.033796  0.000341\n",
            "Iteration 00253    Loss -0.033948  0.000357\n",
            "Iteration 00254    Loss -0.034064  0.000333\n",
            "Iteration 00255    Loss -0.034098  0.000329\n",
            "Iteration 00256    Loss -0.034251  0.000344\n",
            "Iteration 00257    Loss -0.034308  0.000353\n",
            "Iteration 00258    Loss -0.034411  0.000361\n",
            "Iteration 00259    Loss -0.034466  0.000359\n",
            "Iteration 00260    Loss -0.034531  0.000353\n",
            "Iteration 00261    Loss -0.034652  0.000355\n",
            "Iteration 00262    Loss -0.034710  0.000346\n",
            "Iteration 00263    Loss -0.034798  0.000377\n",
            "Iteration 00264    Loss -0.034851  0.000367\n",
            "Iteration 00265    Loss -0.034905  0.000370\n",
            "Iteration 00266    Loss -0.034992  0.000372\n",
            "Iteration 00267    Loss -0.035025  0.000383\n",
            "Iteration 00268    Loss -0.035053  0.000356\n",
            "Iteration 00269    Loss -0.035130  0.000387\n",
            "Iteration 00270    Loss -0.035187  0.000348\n",
            "Iteration 00271    Loss -0.035209  0.000382\n",
            "Iteration 00272    Loss -0.035213  0.000330\n",
            "Iteration 00273    Loss -0.035199  0.000372\n",
            "Iteration 00274    Loss -0.035371  0.000343\n",
            "Iteration 00275    Loss -0.035457  0.000348\n",
            "Iteration 00276    Loss -0.035457  0.000362\n",
            "Iteration 00277    Loss -0.035587  0.000356\n",
            "Iteration 00278    Loss -0.035689  0.000365\n",
            "Iteration 00279    Loss -0.035760  0.000369\n",
            "Iteration 00280    Loss -0.035824  0.000384\n",
            "Iteration 00281    Loss -0.035900  0.000395\n",
            "Iteration 00282    Loss -0.035931  0.000366\n",
            "Iteration 00283    Loss -0.036067  0.000378\n",
            "Iteration 00284    Loss -0.036140  0.000396\n",
            "Iteration 00285    Loss -0.036195  0.000383\n",
            "Iteration 00286    Loss -0.036248  0.000384\n",
            "Iteration 00287    Loss -0.036327  0.000391\n",
            "Iteration 00288    Loss -0.036403  0.000409\n",
            "Iteration 00289    Loss -0.036452  0.000392\n",
            "Iteration 00290    Loss -0.036520  0.000394\n",
            "Iteration 00291    Loss -0.036568  0.000410\n",
            "Iteration 00292    Loss -0.036626  0.000431\n",
            "Iteration 00293    Loss -0.036662  0.000369\n",
            "Iteration 00294    Loss -0.036710  0.000440\n",
            "Iteration 00295    Loss -0.036781  0.000411\n",
            "Iteration 00296    Loss -0.036655  0.000386\n",
            "Iteration 00297    Loss -0.036863  0.000441\n",
            "Iteration 00298    Loss -0.036711  0.000395\n",
            "Iteration 00299    Loss -0.036763  0.000422\n",
            "Iteration 00300    Loss -0.036855  0.000362\n",
            "Iteration 00301    Loss -0.036978  0.000386\n",
            "Iteration 00302    Loss -0.037033  0.000423\n",
            "Iteration 00303    Loss -0.037101  0.000392\n",
            "Iteration 00304    Loss -0.037156  0.000401\n",
            "Iteration 00305    Loss -0.037276  0.000395\n",
            "Iteration 00306    Loss -0.037246  0.000415\n",
            "Iteration 00307    Loss -0.037341  0.000421\n",
            "Iteration 00308    Loss -0.037448  0.000420\n",
            "Iteration 00309    Loss -0.037476  0.000457\n",
            "Iteration 00310    Loss -0.037574  0.000437\n",
            "Iteration 00311    Loss -0.037629  0.000424\n",
            "Iteration 00312    Loss -0.037636  0.000447\n",
            "Iteration 00313    Loss -0.037703  0.000421\n",
            "Iteration 00314    Loss -0.037796  0.000417\n",
            "Iteration 00315    Loss -0.037726  0.000469\n",
            "Iteration 00316    Loss -0.037902  0.000424\n",
            "Iteration 00317    Loss -0.037942  0.000458\n",
            "Iteration 00318    Loss -0.037919  0.000474\n",
            "Iteration 00319    Loss -0.038018  0.000456\n",
            "Iteration 00320    Loss -0.038060  0.000432\n",
            "Iteration 00321    Loss -0.038136  0.000472\n",
            "Iteration 00322    Loss -0.038184  0.000469\n",
            "Iteration 00323    Loss -0.038192  0.000455\n",
            "Iteration 00324    Loss -0.038295  0.000454\n",
            "Iteration 00325    Loss -0.038321  0.000481\n",
            "Iteration 00326    Loss -0.038341  0.000454\n",
            "Iteration 00327    Loss -0.038366  0.000456\n",
            "Iteration 00328    Loss -0.038376  0.000465\n",
            "Iteration 00329    Loss -0.038426  0.000462\n",
            "Iteration 00330    Loss -0.038518  0.000448\n",
            "Iteration 00331    Loss -0.038600  0.000495\n",
            "Iteration 00332    Loss -0.038567  0.000440\n",
            "Iteration 00333    Loss -0.038659  0.000484\n",
            "Iteration 00334    Loss -0.038551  0.000444\n",
            "Iteration 00335    Loss -0.038478  0.000511\n",
            "Iteration 00336    Loss -0.038208  0.000313\n",
            "Iteration 00337    Loss -0.038607  0.000348\n",
            "Iteration 00338    Loss -0.038578  0.000436\n",
            "Iteration 00339    Loss -0.038720  0.000388\n",
            "Iteration 00340    Loss -0.038640  0.000352\n",
            "Iteration 00341    Loss -0.038851  0.000430\n",
            "Iteration 00342    Loss -0.038880  0.000467\n",
            "Iteration 00343    Loss -0.038970  0.000392\n",
            "Iteration 00344    Loss -0.039004  0.000375\n",
            "Iteration 00345    Loss -0.039108  0.000447\n",
            "Iteration 00346    Loss -0.039096  0.000484\n",
            "Iteration 00347    Loss -0.039215  0.000442\n",
            "Iteration 00348    Loss -0.039211  0.000427\n",
            "Iteration 00349    Loss -0.039343  0.000451\n",
            "Iteration 00350    Loss -0.039352  0.000478\n",
            "Iteration 00351    Loss -0.039423  0.000485\n",
            "Iteration 00352    Loss -0.039292  0.000451\n",
            "Iteration 00353    Loss -0.039481  0.000477\n",
            "Iteration 00354    Loss -0.039497  0.000483\n",
            "Iteration 00355    Loss -0.039543  0.000468\n",
            "Iteration 00356    Loss -0.039534  0.000503\n",
            "Iteration 00357    Loss -0.039629  0.000487\n",
            "Iteration 00358    Loss -0.039642  0.000487\n",
            "Iteration 00359    Loss -0.039642  0.000493\n",
            "Iteration 00360    Loss -0.039615  0.000473\n",
            "Iteration 00361    Loss -0.039696  0.000485\n",
            "Iteration 00362    Loss -0.039780  0.000485\n",
            "Iteration 00363    Loss -0.039815  0.000491\n",
            "Iteration 00364    Loss -0.039915  0.000494\n",
            "Iteration 00365    Loss -0.039916  0.000514\n",
            "Iteration 00366    Loss -0.040005  0.000483\n",
            "Iteration 00367    Loss -0.040029  0.000494\n",
            "Iteration 00368    Loss -0.039997  0.000523\n",
            "Iteration 00369    Loss -0.040055  0.000525\n",
            "Iteration 00370    Loss -0.040183  0.000499\n",
            "Iteration 00371    Loss -0.040242  0.000512\n",
            "Iteration 00372    Loss -0.040287  0.000517\n",
            "Iteration 00373    Loss -0.040301  0.000527\n",
            "Iteration 00374    Loss -0.040320  0.000543\n",
            "Iteration 00375    Loss -0.040329  0.000499\n",
            "Iteration 00376    Loss -0.040290  0.000526\n",
            "Iteration 00377    Loss -0.040369  0.000532\n",
            "Iteration 00378    Loss -0.040416  0.000529\n",
            "Iteration 00379    Loss -0.040444  0.000492\n",
            "Iteration 00380    Loss -0.040424  0.000575\n",
            "Iteration 00381    Loss -0.040418  0.000477\n",
            "Iteration 00382    Loss -0.040547  0.000506\n",
            "Iteration 00383    Loss -0.040602  0.000567\n",
            "Iteration 00384    Loss -0.040665  0.000522\n",
            "Iteration 00385    Loss -0.040642  0.000515\n",
            "Iteration 00386    Loss -0.040641  0.000533\n",
            "Iteration 00387    Loss -0.040565  0.000558\n",
            "Iteration 00388    Loss -0.040754  0.000513\n",
            "Iteration 00389    Loss -0.040791  0.000503\n",
            "Iteration 00390    Loss -0.040834  0.000607\n",
            "Iteration 00391    Loss -0.040751  0.000492\n",
            "Iteration 00392    Loss -0.040833  0.000517\n",
            "Iteration 00393    Loss -0.040840  0.000570\n",
            "Iteration 00394    Loss -0.040863  0.000534\n",
            "Iteration 00395    Loss -0.040917  0.000513\n",
            "Iteration 00396    Loss -0.040987  0.000500\n",
            "Iteration 00397    Loss -0.041071  0.000549\n",
            "Iteration 00398    Loss -0.041088  0.000569\n",
            "Iteration 00399    Loss -0.041065  0.000491\n",
            "Iteration 00400    Loss -0.041175  0.000543\n",
            "Iteration 00401    Loss -0.041097  0.000575\n",
            "Iteration 00402    Loss -0.041068  0.000511\n",
            "Iteration 00403    Loss -0.041199  0.000522\n",
            "Iteration 00404    Loss -0.041213  0.000567\n",
            "Iteration 00405    Loss -0.041159  0.000557\n",
            "Iteration 00406    Loss -0.041301  0.000491\n",
            "Iteration 00407    Loss -0.041339  0.000571\n",
            "Iteration 00408    Loss -0.041339  0.000557\n",
            "Iteration 00409    Loss -0.041411  0.000512\n",
            "Iteration 00410    Loss -0.041400  0.000586\n",
            "Iteration 00411    Loss -0.041442  0.000505\n",
            "Iteration 00412    Loss -0.041497  0.000567\n",
            "Iteration 00413    Loss -0.041534  0.000585\n",
            "Iteration 00414    Loss -0.041571  0.000528\n",
            "Iteration 00415    Loss -0.041590  0.000573\n",
            "Iteration 00416    Loss -0.041644  0.000567\n",
            "Iteration 00417    Loss -0.041644  0.000576\n",
            "Iteration 00418    Loss -0.041690  0.000549\n",
            "Iteration 00419    Loss -0.041756  0.000587\n",
            "Iteration 00420    Loss -0.041808  0.000610\n",
            "Iteration 00421    Loss -0.041783  0.000560\n",
            "Iteration 00422    Loss -0.041837  0.000605\n",
            "Iteration 00423    Loss -0.041870  0.000570\n",
            "Iteration 00424    Loss -0.041897  0.000620\n",
            "Iteration 00425    Loss -0.041946  0.000578\n",
            "Iteration 00426    Loss -0.042005  0.000612\n",
            "Iteration 00427    Loss -0.042043  0.000608\n",
            "Iteration 00428    Loss -0.042048  0.000589\n",
            "Iteration 00429    Loss -0.041979  0.000651\n",
            "Iteration 00430    Loss -0.042039  0.000555\n",
            "Iteration 00431    Loss -0.042033  0.000639\n",
            "Iteration 00432    Loss -0.041986  0.000551\n",
            "Iteration 00433    Loss -0.042028  0.000606\n",
            "Iteration 00434    Loss -0.042041  0.000558\n",
            "Iteration 00435    Loss -0.041997  0.000554\n",
            "Iteration 00436    Loss -0.042092  0.000644\n",
            "Iteration 00437    Loss -0.042104  0.000528\n",
            "Iteration 00438    Loss -0.042234  0.000608\n",
            "Iteration 00439    Loss -0.042197  0.000619\n",
            "Iteration 00440    Loss -0.042216  0.000560\n",
            "Iteration 00441    Loss -0.042118  0.000580\n",
            "Iteration 00442    Loss -0.042306  0.000649\n",
            "Iteration 00443    Loss -0.042312  0.000574\n",
            "Iteration 00444    Loss -0.042391  0.000603\n",
            "Iteration 00445    Loss -0.042404  0.000603\n",
            "Iteration 00446    Loss -0.042397  0.000596\n",
            "Iteration 00447    Loss -0.042463  0.000627\n",
            "Iteration 00448    Loss -0.042501  0.000634\n",
            "Iteration 00449    Loss -0.042501  0.000600\n",
            "Iteration 00450    Loss -0.042546  0.000626\n",
            "Iteration 00451    Loss -0.042614  0.000623\n",
            "Iteration 00452    Loss -0.042545  0.000642\n",
            "Iteration 00453    Loss -0.042653  0.000624\n",
            "Iteration 00454    Loss -0.042648  0.000651\n",
            "Iteration 00455    Loss -0.042672  0.000580\n",
            "Iteration 00456    Loss -0.042676  0.000674\n",
            "Iteration 00457    Loss -0.042741  0.000602\n",
            "Iteration 00458    Loss -0.042754  0.000660\n",
            "Iteration 00459    Loss -0.042760  0.000648\n",
            "Iteration 00460    Loss -0.042720  0.000622\n",
            "Iteration 00461    Loss -0.042682  0.000611\n",
            "Iteration 00462    Loss -0.042722  0.000652\n",
            "Iteration 00463    Loss -0.042706  0.000568\n",
            "Iteration 00464    Loss -0.042798  0.000583\n",
            "Iteration 00465    Loss -0.042833  0.000627\n",
            "Iteration 00466    Loss -0.042861  0.000617\n",
            "Iteration 00467    Loss -0.042885  0.000614\n",
            "Iteration 00468    Loss -0.042931  0.000630\n",
            "Iteration 00469    Loss -0.042953  0.000669\n",
            "Iteration 00470    Loss -0.043015  0.000634\n",
            "Iteration 00471    Loss -0.043041  0.000656\n",
            "Iteration 00472    Loss -0.043020  0.000650\n",
            "Iteration 00473    Loss -0.043089  0.000649\n",
            "Iteration 00474    Loss -0.043033  0.000676\n",
            "Iteration 00475    Loss -0.042961  0.000563\n",
            "Iteration 00476    Loss -0.043040  0.000658\n",
            "Iteration 00477    Loss -0.043010  0.000624\n",
            "Iteration 00478    Loss -0.042956  0.000594\n",
            "Iteration 00479    Loss -0.043102  0.000596\n",
            "Iteration 00480    Loss -0.043101  0.000675\n",
            "Iteration 00481    Loss -0.043183  0.000628\n",
            "Iteration 00482    Loss -0.043197  0.000588\n",
            "Iteration 00483    Loss -0.043217  0.000663\n",
            "Iteration 00484    Loss -0.043215  0.000621\n",
            "Iteration 00485    Loss -0.043248  0.000615\n",
            "Iteration 00486    Loss -0.043293  0.000674\n",
            "Iteration 00487    Loss -0.043284  0.000636\n",
            "Iteration 00488    Loss -0.043326  0.000627\n",
            "Iteration 00489    Loss -0.043411  0.000689\n",
            "Iteration 00490    Loss -0.043436  0.000673\n",
            "Iteration 00491    Loss -0.043423  0.000657\n",
            "Iteration 00492    Loss -0.043457  0.000680\n",
            "Iteration 00493    Loss -0.043516  0.000680\n",
            "Iteration 00494    Loss -0.043506  0.000664\n",
            "Iteration 00495    Loss -0.043501  0.000680\n",
            "Iteration 00496    Loss -0.043547  0.000730\n",
            "Iteration 00497    Loss -0.043491  0.000607\n",
            "Iteration 00498    Loss -0.043465  0.000722\n",
            "Iteration 00499    Loss -0.043450  0.000578\n",
            "100% 498/500 [4:44:26<01:15, 37.66s/it]/content/SOTS/outdoor/hazy/0107_0.9_0.08.jpg\n",
            "Iteration 00000    Loss 0.361236  0.000017\n",
            "Iteration 00001    Loss 15.531331  0.000005\n",
            "Iteration 00002    Loss 0.280293  0.000002\n",
            "Iteration 00003    Loss 3.946477  0.000001\n",
            "Iteration 00004    Loss 0.347167  0.000001\n",
            "Iteration 00005    Loss 0.253835  0.000001\n",
            "Iteration 00006    Loss 0.169884  0.000001\n",
            "Iteration 00007    Loss 0.120997  0.000001\n",
            "Iteration 00008    Loss 0.112653  0.000001\n",
            "Iteration 00009    Loss 0.099710  0.000001\n",
            "Iteration 00010    Loss 0.085999  0.000000\n",
            "Iteration 00011    Loss 0.079309  0.000001\n",
            "Iteration 00012    Loss 0.074889  0.000001\n",
            "Iteration 00013    Loss 0.070946  0.000001\n",
            "Iteration 00014    Loss 0.067643  0.000001\n",
            "Iteration 00015    Loss 0.064781  0.000001\n",
            "Iteration 00016    Loss 0.062179  0.000001\n",
            "Iteration 00017    Loss 0.060123  0.000001\n",
            "Iteration 00018    Loss 0.058057  0.000001\n",
            "Iteration 00019    Loss 0.056110  0.000001\n",
            "Iteration 00020    Loss 0.054247  0.000001\n",
            "Iteration 00021    Loss 0.052716  0.000001\n",
            "Iteration 00022    Loss 0.051300  0.000001\n",
            "Iteration 00023    Loss 0.049382  0.000001\n",
            "Iteration 00024    Loss 0.048342  0.000001\n",
            "Iteration 00025    Loss 0.047141  0.000001\n",
            "Iteration 00026    Loss 0.045688  0.000001\n",
            "Iteration 00027    Loss 0.044742  0.000001\n",
            "Iteration 00028    Loss 0.043184  0.000001\n",
            "Iteration 00029    Loss 0.042377  0.000001\n",
            "Iteration 00030    Loss 0.041419  0.000001\n",
            "Iteration 00031    Loss 0.040272  0.000001\n",
            "Iteration 00032    Loss 0.040144  0.000001\n",
            "Iteration 00033    Loss 0.038082  0.000001\n",
            "Iteration 00034    Loss 0.037273  0.000001\n",
            "Iteration 00035    Loss 0.036559  0.000001\n",
            "Iteration 00036    Loss 0.035757  0.000001\n",
            "Iteration 00037    Loss 0.034853  0.000001\n",
            "Iteration 00038    Loss 0.033982  0.000001\n",
            "Iteration 00039    Loss 0.033178  0.000002\n",
            "Iteration 00040    Loss 0.032101  0.000002\n",
            "Iteration 00041    Loss 0.031352  0.000002\n",
            "Iteration 00042    Loss 0.030519  0.000002\n",
            "Iteration 00043    Loss 0.029583  0.000002\n",
            "Iteration 00044    Loss 0.028719  0.000002\n",
            "Iteration 00045    Loss 0.028072  0.000002\n",
            "Iteration 00046    Loss 0.027298  0.000002\n",
            "Iteration 00047    Loss 0.026690  0.000002\n",
            "Iteration 00048    Loss 0.026031  0.000003\n",
            "Iteration 00049    Loss 0.025136  0.000002\n",
            "Iteration 00050    Loss 0.024463  0.000003\n",
            "Iteration 00051    Loss 0.023865  0.000003\n",
            "Iteration 00052    Loss 0.023251  0.000003\n",
            "Iteration 00053    Loss 0.022502  0.000003\n",
            "Iteration 00054    Loss 0.021868  0.000004\n",
            "Iteration 00055    Loss 0.021146  0.000004\n",
            "Iteration 00056    Loss 0.020543  0.000005\n",
            "Iteration 00057    Loss 0.020125  0.000005\n",
            "Iteration 00058    Loss 0.019612  0.000005\n",
            "Iteration 00059    Loss 0.018683  0.000005\n",
            "Iteration 00060    Loss 0.018456  0.000005\n",
            "Iteration 00061    Loss 0.017783  0.000005\n",
            "Iteration 00062    Loss 0.017024  0.000005\n",
            "Iteration 00063    Loss 0.016710  0.000006\n",
            "Iteration 00064    Loss 0.016086  0.000006\n",
            "Iteration 00065    Loss 0.015399  0.000006\n",
            "Iteration 00066    Loss 0.015068  0.000006\n",
            "Iteration 00067    Loss 0.014431  0.000008\n",
            "Iteration 00068    Loss 0.014049  0.000007\n",
            "Iteration 00069    Loss 0.013469  0.000007\n",
            "Iteration 00070    Loss 0.012983  0.000008\n",
            "Iteration 00071    Loss 0.012460  0.000007\n",
            "Iteration 00072    Loss 0.012007  0.000008\n",
            "Iteration 00073    Loss 0.011411  0.000008\n",
            "Iteration 00074    Loss 0.010785  0.000008\n",
            "Iteration 00075    Loss 0.010573  0.000009\n",
            "Iteration 00076    Loss 0.010029  0.000008\n",
            "Iteration 00077    Loss 0.009590  0.000009\n",
            "Iteration 00078    Loss 0.009104  0.000009\n",
            "Iteration 00079    Loss 0.008598  0.000010\n",
            "Iteration 00080    Loss 0.008107  0.000010\n",
            "Iteration 00081    Loss 0.007778  0.000010\n",
            "Iteration 00082    Loss 0.007423  0.000012\n",
            "Iteration 00083    Loss 0.007076  0.000010\n",
            "Iteration 00084    Loss 0.006676  0.000012\n",
            "Iteration 00085    Loss 0.006225  0.000009\n",
            "Iteration 00086    Loss 0.005788  0.000011\n",
            "Iteration 00087    Loss 0.005277  0.000011\n",
            "Iteration 00088    Loss 0.004957  0.000011\n",
            "Iteration 00089    Loss 0.004523  0.000013\n",
            "Iteration 00090    Loss 0.004103  0.000012\n",
            "Iteration 00091    Loss 0.003783  0.000014\n",
            "Iteration 00092    Loss 0.003403  0.000014\n",
            "Iteration 00093    Loss 0.002876  0.000012\n",
            "Iteration 00094    Loss 0.002583  0.000013\n",
            "Iteration 00095    Loss 0.002293  0.000013\n",
            "Iteration 00096    Loss 0.001752  0.000013\n",
            "Iteration 00097    Loss 0.001430  0.000014\n",
            "Iteration 00098    Loss 0.001099  0.000016\n",
            "Iteration 00099    Loss 0.000762  0.000016\n",
            "Iteration 00100    Loss 0.000464  0.000016\n",
            "Iteration 00101    Loss 0.000094  0.000016\n",
            "Iteration 00102    Loss -0.000288  0.000016\n",
            "Iteration 00103    Loss -0.000675  0.000017\n",
            "Iteration 00104    Loss -0.000996  0.000019\n",
            "Iteration 00105    Loss -0.001338  0.000018\n",
            "Iteration 00106    Loss -0.001788  0.000017\n",
            "Iteration 00107    Loss -0.001987  0.000016\n",
            "Iteration 00108    Loss -0.002349  0.000018\n",
            "Iteration 00109    Loss -0.002730  0.000017\n",
            "Iteration 00110    Loss -0.002962  0.000020\n",
            "Iteration 00111    Loss -0.003368  0.000019\n",
            "Iteration 00112    Loss -0.003626  0.000019\n",
            "Iteration 00113    Loss -0.003963  0.000020\n",
            "Iteration 00114    Loss -0.004375  0.000020\n",
            "Iteration 00115    Loss -0.004659  0.000021\n",
            "Iteration 00116    Loss -0.004773  0.000024\n",
            "Iteration 00117    Loss -0.005160  0.000023\n",
            "Iteration 00118    Loss -0.005371  0.000026\n",
            "Iteration 00119    Loss -0.005759  0.000022\n",
            "Iteration 00120    Loss -0.006165  0.000023\n",
            "Iteration 00121    Loss -0.006405  0.000024\n",
            "Iteration 00122    Loss -0.006542  0.000021\n",
            "Iteration 00123    Loss -0.006924  0.000024\n",
            "Iteration 00124    Loss -0.007276  0.000023\n",
            "Iteration 00125    Loss -0.007580  0.000024\n",
            "Iteration 00126    Loss -0.007681  0.000027\n",
            "Iteration 00127    Loss -0.008138  0.000025\n",
            "Iteration 00128    Loss -0.008426  0.000027\n",
            "Iteration 00129    Loss -0.008657  0.000029\n",
            "Iteration 00130    Loss -0.008865  0.000030\n",
            "Iteration 00131    Loss -0.009195  0.000031\n",
            "Iteration 00132    Loss -0.009448  0.000030\n",
            "Iteration 00133    Loss -0.009721  0.000028\n",
            "Iteration 00134    Loss -0.009966  0.000029\n",
            "Iteration 00135    Loss -0.010137  0.000028\n",
            "Iteration 00136    Loss -0.010564  0.000031\n",
            "Iteration 00137    Loss -0.010806  0.000031\n",
            "Iteration 00138    Loss -0.011043  0.000030\n",
            "Iteration 00139    Loss -0.011126  0.000038\n",
            "Iteration 00140    Loss -0.011412  0.000034\n",
            "Iteration 00141    Loss -0.011625  0.000036\n",
            "Iteration 00142    Loss -0.012081  0.000029\n",
            "Iteration 00143    Loss -0.012354  0.000032\n",
            "Iteration 00144    Loss -0.012445  0.000038\n",
            "Iteration 00145    Loss -0.012810  0.000033\n",
            "Iteration 00146    Loss -0.013055  0.000035\n",
            "Iteration 00147    Loss -0.013281  0.000038\n",
            "Iteration 00148    Loss -0.013375  0.000036\n",
            "Iteration 00149    Loss -0.013779  0.000040\n",
            "Iteration 00150    Loss -0.013881  0.000040\n",
            "Iteration 00151    Loss -0.014155  0.000036\n",
            "Iteration 00152    Loss -0.014456  0.000043\n",
            "Iteration 00153    Loss -0.014750  0.000041\n",
            "Iteration 00154    Loss -0.014866  0.000039\n",
            "Iteration 00155    Loss -0.015071  0.000043\n",
            "Iteration 00156    Loss -0.015478  0.000043\n",
            "Iteration 00157    Loss -0.015607  0.000041\n",
            "Iteration 00158    Loss -0.015937  0.000043\n",
            "Iteration 00159    Loss -0.016150  0.000044\n",
            "Iteration 00160    Loss -0.016188  0.000044\n",
            "Iteration 00161    Loss -0.016308  0.000047\n",
            "Iteration 00162    Loss -0.016500  0.000044\n",
            "Iteration 00163    Loss -0.016788  0.000044\n",
            "Iteration 00164    Loss -0.017163  0.000048\n",
            "Iteration 00165    Loss -0.017290  0.000044\n",
            "Iteration 00166    Loss -0.017438  0.000043\n",
            "Iteration 00167    Loss -0.017565  0.000046\n",
            "Iteration 00168    Loss -0.017785  0.000046\n",
            "Iteration 00169    Loss -0.018111  0.000048\n",
            "Iteration 00170    Loss -0.018332  0.000051\n",
            "Iteration 00171    Loss -0.018513  0.000053\n",
            "Iteration 00172    Loss -0.018693  0.000049\n",
            "Iteration 00173    Loss -0.018996  0.000051\n",
            "Iteration 00174    Loss -0.019170  0.000054\n",
            "Iteration 00175    Loss -0.019295  0.000054\n",
            "Iteration 00176    Loss -0.019427  0.000049\n",
            "Iteration 00177    Loss -0.019695  0.000050\n",
            "Iteration 00178    Loss -0.019943  0.000051\n",
            "Iteration 00179    Loss -0.019935  0.000050\n",
            "Iteration 00180    Loss -0.020147  0.000056\n",
            "Iteration 00181    Loss -0.020441  0.000058\n",
            "Iteration 00182    Loss -0.020617  0.000046\n",
            "Iteration 00183    Loss -0.020699  0.000047\n",
            "Iteration 00184    Loss -0.020876  0.000050\n",
            "Iteration 00185    Loss -0.021054  0.000056\n",
            "Iteration 00186    Loss -0.021279  0.000062\n",
            "Iteration 00187    Loss -0.021444  0.000049\n",
            "Iteration 00188    Loss -0.021548  0.000052\n",
            "Iteration 00189    Loss -0.021779  0.000059\n",
            "Iteration 00190    Loss -0.022022  0.000054\n",
            "Iteration 00191    Loss -0.022148  0.000055\n",
            "Iteration 00192    Loss -0.022248  0.000058\n",
            "Iteration 00193    Loss -0.022358  0.000057\n",
            "Iteration 00194    Loss -0.022595  0.000058\n",
            "Iteration 00195    Loss -0.022754  0.000055\n",
            "Iteration 00196    Loss -0.023127  0.000051\n",
            "Iteration 00197    Loss -0.023144  0.000055\n",
            "Iteration 00198    Loss -0.023199  0.000057\n",
            "Iteration 00199    Loss -0.023433  0.000060\n",
            "Iteration 00200    Loss -0.023774  0.000059\n",
            "Iteration 00201    Loss -0.023754  0.000061\n",
            "Iteration 00202    Loss -0.023923  0.000062\n",
            "Iteration 00203    Loss -0.024126  0.000058\n",
            "Iteration 00204    Loss -0.024333  0.000056\n",
            "Iteration 00205    Loss -0.024539  0.000059\n",
            "Iteration 00206    Loss -0.024546  0.000064\n",
            "Iteration 00207    Loss -0.024728  0.000064\n",
            "Iteration 00208    Loss -0.024951  0.000064\n",
            "Iteration 00209    Loss -0.025024  0.000062\n",
            "Iteration 00210    Loss -0.025145  0.000059\n",
            "Iteration 00211    Loss -0.025494  0.000061\n",
            "Iteration 00212    Loss -0.025522  0.000065\n",
            "Iteration 00213    Loss -0.025582  0.000064\n",
            "Iteration 00214    Loss -0.025817  0.000062\n",
            "Iteration 00215    Loss -0.025936  0.000064\n",
            "Iteration 00216    Loss -0.025996  0.000068\n",
            "Iteration 00217    Loss -0.026223  0.000065\n",
            "Iteration 00218    Loss -0.026253  0.000064\n",
            "Iteration 00219    Loss -0.026610  0.000066\n",
            "Iteration 00220    Loss -0.026682  0.000076\n",
            "Iteration 00221    Loss -0.026966  0.000070\n",
            "Iteration 00222    Loss -0.026837  0.000061\n",
            "Iteration 00223    Loss -0.026955  0.000063\n",
            "Iteration 00224    Loss -0.027009  0.000070\n",
            "Iteration 00225    Loss -0.027333  0.000072\n",
            "Iteration 00226    Loss -0.027413  0.000070\n",
            "Iteration 00227    Loss -0.027657  0.000064\n",
            "Iteration 00228    Loss -0.027708  0.000066\n",
            "Iteration 00229    Loss -0.027821  0.000071\n",
            "Iteration 00230    Loss -0.027982  0.000068\n",
            "Iteration 00231    Loss -0.028061  0.000064\n",
            "Iteration 00232    Loss -0.028415  0.000068\n",
            "Iteration 00233    Loss -0.028354  0.000073\n",
            "Iteration 00234    Loss -0.028337  0.000072\n",
            "Iteration 00235    Loss -0.028625  0.000072\n",
            "Iteration 00236    Loss -0.028683  0.000067\n",
            "Iteration 00237    Loss -0.028776  0.000071\n",
            "Iteration 00238    Loss -0.028898  0.000069\n",
            "Iteration 00239    Loss -0.029024  0.000072\n",
            "Iteration 00240    Loss -0.029338  0.000065\n",
            "Iteration 00241    Loss -0.029278  0.000069\n",
            "Iteration 00242    Loss -0.029516  0.000071\n",
            "Iteration 00243    Loss -0.029621  0.000070\n",
            "Iteration 00244    Loss -0.029520  0.000073\n",
            "Iteration 00245    Loss -0.030097  0.000073\n",
            "Iteration 00246    Loss -0.030105  0.000071\n",
            "Iteration 00247    Loss -0.030229  0.000073\n",
            "Iteration 00248    Loss -0.030078  0.000071\n",
            "Iteration 00249    Loss -0.030295  0.000070\n",
            "Iteration 00250    Loss -0.030544  0.000072\n",
            "Iteration 00251    Loss -0.030372  0.000072\n",
            "Iteration 00252    Loss -0.030664  0.000076\n",
            "Iteration 00253    Loss -0.030865  0.000077\n",
            "Iteration 00254    Loss -0.030670  0.000076\n",
            "Iteration 00255    Loss -0.030943  0.000074\n",
            "Iteration 00256    Loss -0.031139  0.000077\n",
            "Iteration 00257    Loss -0.031019  0.000079\n",
            "Iteration 00258    Loss -0.031207  0.000077\n",
            "Iteration 00259    Loss -0.031445  0.000076\n",
            "Iteration 00260    Loss -0.031340  0.000074\n",
            "Iteration 00261    Loss -0.031626  0.000078\n",
            "Iteration 00262    Loss -0.031354  0.000076\n",
            "Iteration 00263    Loss -0.031718  0.000071\n",
            "Iteration 00264    Loss -0.031992  0.000078\n",
            "Iteration 00265    Loss -0.032086  0.000075\n",
            "Iteration 00266    Loss -0.032184  0.000072\n",
            "Iteration 00267    Loss -0.032027  0.000074\n",
            "Iteration 00268    Loss -0.032313  0.000081\n",
            "Iteration 00269    Loss -0.032342  0.000074\n",
            "Iteration 00270    Loss -0.032267  0.000069\n",
            "Iteration 00271    Loss -0.032747  0.000076\n",
            "Iteration 00272    Loss -0.032562  0.000081\n",
            "Iteration 00273    Loss -0.032689  0.000078\n",
            "Iteration 00274    Loss -0.032918  0.000077\n",
            "Iteration 00275    Loss -0.032618  0.000076\n",
            "Iteration 00276    Loss -0.033136  0.000074\n",
            "Iteration 00277    Loss -0.033242  0.000071\n",
            "Iteration 00278    Loss -0.033070  0.000082\n",
            "Iteration 00279    Loss -0.033047  0.000079\n",
            "Iteration 00280    Loss -0.033521  0.000075\n",
            "Iteration 00281    Loss -0.033444  0.000078\n",
            "Iteration 00282    Loss -0.033574  0.000081\n",
            "Iteration 00283    Loss -0.033634  0.000079\n",
            "Iteration 00284    Loss -0.033616  0.000075\n",
            "Iteration 00285    Loss -0.034004  0.000077\n",
            "Iteration 00286    Loss -0.033855  0.000077\n",
            "Iteration 00287    Loss -0.034098  0.000076\n",
            "Iteration 00288    Loss -0.034241  0.000076\n",
            "Iteration 00289    Loss -0.034202  0.000076\n",
            "Iteration 00290    Loss -0.034326  0.000076\n",
            "Iteration 00291    Loss -0.034515  0.000071\n",
            "Iteration 00292    Loss -0.034491  0.000072\n",
            "Iteration 00293    Loss -0.034439  0.000073\n",
            "Iteration 00294    Loss -0.034650  0.000065\n",
            "Iteration 00295    Loss -0.034703  0.000070\n",
            "Iteration 00296    Loss -0.034924  0.000079\n",
            "Iteration 00297    Loss -0.034738  0.000069\n",
            "Iteration 00298    Loss -0.035011  0.000066\n",
            "Iteration 00299    Loss -0.034995  0.000075\n",
            "Iteration 00300    Loss -0.035094  0.000076\n",
            "Iteration 00301    Loss -0.035089  0.000069\n",
            "Iteration 00302    Loss -0.035250  0.000070\n",
            "Iteration 00303    Loss -0.035371  0.000074\n",
            "Iteration 00304    Loss -0.035533  0.000075\n",
            "Iteration 00305    Loss -0.035480  0.000079\n",
            "Iteration 00306    Loss -0.035218  0.000078\n",
            "Iteration 00307    Loss -0.035487  0.000067\n",
            "Iteration 00308    Loss -0.035622  0.000069\n",
            "Iteration 00309    Loss -0.035856  0.000072\n",
            "Iteration 00310    Loss -0.035962  0.000079\n",
            "Iteration 00311    Loss -0.036011  0.000078\n",
            "Iteration 00312    Loss -0.036051  0.000071\n",
            "Iteration 00313    Loss -0.036254  0.000069\n",
            "Iteration 00314    Loss -0.036345  0.000070\n",
            "Iteration 00315    Loss -0.036105  0.000073\n",
            "Iteration 00316    Loss -0.036143  0.000073\n",
            "Iteration 00317    Loss -0.036484  0.000071\n",
            "Iteration 00318    Loss -0.036596  0.000074\n",
            "Iteration 00319    Loss -0.036470  0.000077\n",
            "Iteration 00320    Loss -0.036561  0.000071\n",
            "Iteration 00321    Loss -0.036802  0.000070\n",
            "Iteration 00322    Loss -0.036942  0.000079\n",
            "Iteration 00323    Loss -0.036784  0.000081\n",
            "Iteration 00324    Loss -0.036891  0.000073\n",
            "Iteration 00325    Loss -0.036906  0.000077\n",
            "Iteration 00326    Loss -0.036941  0.000080\n",
            "Iteration 00327    Loss -0.037072  0.000075\n",
            "Iteration 00328    Loss -0.036988  0.000078\n",
            "Iteration 00329    Loss -0.037065  0.000080\n",
            "Iteration 00330    Loss -0.037296  0.000078\n",
            "Iteration 00331    Loss -0.037447  0.000075\n",
            "Iteration 00332    Loss -0.037445  0.000078\n",
            "Iteration 00333    Loss -0.037325  0.000085\n",
            "Iteration 00334    Loss -0.037357  0.000080\n",
            "Iteration 00335    Loss -0.037505  0.000074\n",
            "Iteration 00336    Loss -0.037631  0.000077\n",
            "Iteration 00337    Loss -0.037665  0.000077\n",
            "Iteration 00338    Loss -0.037775  0.000077\n",
            "Iteration 00339    Loss -0.037830  0.000082\n",
            "Iteration 00340    Loss -0.037841  0.000082\n",
            "Iteration 00341    Loss -0.037755  0.000074\n",
            "Iteration 00342    Loss -0.037998  0.000074\n",
            "Iteration 00343    Loss -0.037955  0.000081\n",
            "Iteration 00344    Loss -0.037920  0.000084\n",
            "Iteration 00345    Loss -0.038203  0.000075\n",
            "Iteration 00346    Loss -0.038266  0.000074\n",
            "Iteration 00347    Loss -0.037930  0.000073\n",
            "Iteration 00348    Loss -0.038303  0.000073\n",
            "Iteration 00349    Loss -0.038428  0.000079\n",
            "Iteration 00350    Loss -0.038441  0.000079\n",
            "Iteration 00351    Loss -0.038600  0.000077\n",
            "Iteration 00352    Loss -0.038569  0.000079\n",
            "Iteration 00353    Loss -0.038674  0.000080\n",
            "Iteration 00354    Loss -0.038707  0.000076\n",
            "Iteration 00355    Loss -0.038617  0.000086\n",
            "Iteration 00356    Loss -0.038908  0.000089\n",
            "Iteration 00357    Loss -0.038716  0.000071\n",
            "Iteration 00358    Loss -0.038919  0.000073\n",
            "Iteration 00359    Loss -0.038934  0.000084\n",
            "Iteration 00360    Loss -0.038883  0.000080\n",
            "Iteration 00361    Loss -0.039136  0.000079\n",
            "Iteration 00362    Loss -0.039108  0.000082\n",
            "Iteration 00363    Loss -0.039106  0.000081\n",
            "Iteration 00364    Loss -0.039240  0.000082\n",
            "Iteration 00365    Loss -0.039316  0.000080\n",
            "Iteration 00366    Loss -0.039355  0.000081\n",
            "Iteration 00367    Loss -0.039476  0.000087\n",
            "Iteration 00368    Loss -0.039507  0.000079\n",
            "Iteration 00369    Loss -0.039524  0.000075\n",
            "Iteration 00370    Loss -0.039532  0.000084\n",
            "Iteration 00371    Loss -0.039448  0.000087\n",
            "Iteration 00372    Loss -0.039646  0.000082\n",
            "Iteration 00373    Loss -0.039714  0.000079\n",
            "Iteration 00374    Loss -0.039548  0.000077\n",
            "Iteration 00375    Loss -0.039591  0.000076\n",
            "Iteration 00376    Loss -0.039780  0.000080\n",
            "Iteration 00377    Loss -0.039869  0.000085\n",
            "Iteration 00378    Loss -0.039821  0.000084\n",
            "Iteration 00379    Loss -0.040055  0.000083\n",
            "Iteration 00380    Loss -0.039897  0.000083\n",
            "Iteration 00381    Loss -0.039855  0.000084\n",
            "Iteration 00382    Loss -0.040172  0.000085\n",
            "Iteration 00383    Loss -0.040063  0.000080\n",
            "Iteration 00384    Loss -0.040047  0.000083\n",
            "Iteration 00385    Loss -0.040086  0.000082\n",
            "Iteration 00386    Loss -0.040270  0.000086\n",
            "Iteration 00387    Loss -0.040341  0.000088\n",
            "Iteration 00388    Loss -0.040320  0.000090\n",
            "Iteration 00389    Loss -0.040361  0.000088\n",
            "Iteration 00390    Loss -0.040212  0.000084\n",
            "Iteration 00391    Loss -0.040274  0.000090\n",
            "Iteration 00392    Loss -0.040639  0.000089\n",
            "Iteration 00393    Loss -0.040434  0.000085\n",
            "Iteration 00394    Loss -0.040309  0.000089\n",
            "Iteration 00395    Loss -0.040524  0.000089\n",
            "Iteration 00396    Loss -0.040629  0.000086\n",
            "Iteration 00397    Loss -0.040691  0.000094\n",
            "Iteration 00398    Loss -0.040686  0.000098\n",
            "Iteration 00399    Loss -0.040841  0.000095\n",
            "Iteration 00400    Loss -0.040597  0.000091\n",
            "Iteration 00401    Loss -0.040959  0.000085\n",
            "Iteration 00402    Loss -0.040638  0.000088\n",
            "Iteration 00403    Loss -0.040799  0.000095\n",
            "Iteration 00404    Loss -0.040849  0.000088\n",
            "Iteration 00405    Loss -0.041106  0.000091\n",
            "Iteration 00406    Loss -0.041129  0.000101\n",
            "Iteration 00407    Loss -0.040951  0.000093\n",
            "Iteration 00408    Loss -0.041082  0.000088\n",
            "Iteration 00409    Loss -0.041176  0.000097\n",
            "Iteration 00410    Loss -0.041179  0.000100\n",
            "Iteration 00411    Loss -0.041346  0.000089\n",
            "Iteration 00412    Loss -0.041379  0.000091\n",
            "Iteration 00413    Loss -0.041333  0.000101\n",
            "Iteration 00414    Loss -0.041368  0.000093\n",
            "Iteration 00415    Loss -0.041499  0.000093\n",
            "Iteration 00416    Loss -0.041395  0.000096\n",
            "Iteration 00417    Loss -0.041436  0.000093\n",
            "Iteration 00418    Loss -0.041564  0.000101\n",
            "Iteration 00419    Loss -0.041601  0.000093\n",
            "Iteration 00420    Loss -0.041652  0.000095\n",
            "Iteration 00421    Loss -0.041600  0.000107\n",
            "Iteration 00422    Loss -0.041678  0.000099\n",
            "Iteration 00423    Loss -0.041737  0.000097\n",
            "Iteration 00424    Loss -0.041578  0.000100\n",
            "Iteration 00425    Loss -0.041780  0.000099\n",
            "Iteration 00426    Loss -0.041772  0.000094\n",
            "Iteration 00427    Loss -0.041793  0.000105\n",
            "Iteration 00428    Loss -0.041857  0.000106\n",
            "Iteration 00429    Loss -0.041693  0.000104\n",
            "Iteration 00430    Loss -0.041951  0.000108\n",
            "Iteration 00431    Loss -0.042008  0.000094\n",
            "Iteration 00432    Loss -0.042071  0.000096\n",
            "Iteration 00433    Loss -0.041950  0.000103\n",
            "Iteration 00434    Loss -0.041970  0.000099\n",
            "Iteration 00435    Loss -0.042143  0.000104\n",
            "Iteration 00436    Loss -0.041849  0.000105\n",
            "Iteration 00437    Loss -0.042138  0.000102\n",
            "Iteration 00438    Loss -0.042227  0.000104\n",
            "Iteration 00439    Loss -0.042219  0.000106\n",
            "Iteration 00440    Loss -0.042212  0.000107\n",
            "Iteration 00441    Loss -0.042089  0.000103\n",
            "Iteration 00442    Loss -0.042321  0.000099\n",
            "Iteration 00443    Loss -0.042332  0.000110\n",
            "Iteration 00444    Loss -0.042377  0.000113\n",
            "Iteration 00445    Loss -0.042383  0.000101\n",
            "Iteration 00446    Loss -0.042442  0.000106\n",
            "Iteration 00447    Loss -0.042379  0.000101\n",
            "Iteration 00448    Loss -0.042459  0.000099\n",
            "Iteration 00449    Loss -0.042452  0.000102\n",
            "Iteration 00450    Loss -0.042473  0.000106\n",
            "Iteration 00451    Loss -0.042530  0.000113\n",
            "Iteration 00452    Loss -0.042577  0.000101\n",
            "Iteration 00453    Loss -0.042622  0.000104\n",
            "Iteration 00454    Loss -0.042733  0.000116\n",
            "Iteration 00455    Loss -0.042638  0.000100\n",
            "Iteration 00456    Loss -0.042773  0.000104\n",
            "Iteration 00457    Loss -0.042777  0.000107\n",
            "Iteration 00458    Loss -0.042827  0.000107\n",
            "Iteration 00459    Loss -0.042813  0.000113\n",
            "Iteration 00460    Loss -0.042833  0.000099\n",
            "Iteration 00461    Loss -0.042807  0.000109\n",
            "Iteration 00462    Loss -0.042925  0.000120\n",
            "Iteration 00463    Loss -0.042612  0.000110\n",
            "Iteration 00464    Loss -0.042920  0.000102\n",
            "Iteration 00465    Loss -0.042959  0.000094\n",
            "Iteration 00466    Loss -0.042889  0.000104\n",
            "Iteration 00467    Loss -0.043048  0.000112\n",
            "Iteration 00468    Loss -0.043012  0.000108\n",
            "Iteration 00469    Loss -0.043011  0.000113\n",
            "Iteration 00470    Loss -0.043107  0.000114\n",
            "Iteration 00471    Loss -0.043143  0.000108\n",
            "Iteration 00472    Loss -0.043117  0.000107\n",
            "Iteration 00473    Loss -0.042963  0.000110\n",
            "Iteration 00474    Loss -0.042788  0.000115\n",
            "Iteration 00475    Loss -0.043194  0.000123\n",
            "Iteration 00476    Loss -0.043248  0.000108\n",
            "Iteration 00477    Loss -0.043299  0.000111\n",
            "Iteration 00478    Loss -0.043351  0.000109\n",
            "Iteration 00479    Loss -0.043279  0.000111\n",
            "Iteration 00480    Loss -0.043299  0.000116\n",
            "Iteration 00481    Loss -0.043213  0.000108\n",
            "Iteration 00482    Loss -0.043423  0.000113\n",
            "Iteration 00483    Loss -0.043465  0.000125\n",
            "Iteration 00484    Loss -0.043482  0.000113\n",
            "Iteration 00485    Loss -0.043375  0.000106\n",
            "Iteration 00486    Loss -0.043510  0.000115\n",
            "Iteration 00487    Loss -0.043538  0.000112\n",
            "Iteration 00488    Loss -0.043507  0.000111\n",
            "Iteration 00489    Loss -0.043607  0.000127\n",
            "Iteration 00490    Loss -0.043505  0.000114\n",
            "Iteration 00491    Loss -0.043642  0.000110\n",
            "Iteration 00492    Loss -0.043594  0.000117\n",
            "Iteration 00493    Loss -0.043596  0.000115\n",
            "Iteration 00494    Loss -0.043723  0.000104\n",
            "Iteration 00495    Loss -0.043782  0.000115\n",
            "Iteration 00496    Loss -0.043616  0.000120\n",
            "Iteration 00497    Loss -0.043726  0.000114\n",
            "Iteration 00498    Loss -0.043691  0.000108\n",
            "Iteration 00499    Loss -0.043793  0.000124\n",
            "100% 499/500 [4:44:59<00:36, 36.09s/it]/content/SOTS/outdoor/hazy/0147_1_0.16.jpg\n",
            "Iteration 00000    Loss 0.491092  0.000071\n",
            "Iteration 00001    Loss 6.014663  0.000029\n",
            "Iteration 00002    Loss 0.377683  0.000009\n",
            "Iteration 00003    Loss 0.325721  0.000008\n",
            "Iteration 00004    Loss 0.289942  0.000004\n",
            "Iteration 00005    Loss 0.252392  0.000003\n",
            "Iteration 00006    Loss 0.216058  0.000002\n",
            "Iteration 00007    Loss 0.187093  0.000001\n",
            "Iteration 00008    Loss 0.169110  0.000001\n",
            "Iteration 00009    Loss 0.152894  0.000001\n",
            "Iteration 00010    Loss 0.139767  0.000001\n",
            "Iteration 00011    Loss 0.131370  0.000001\n",
            "Iteration 00012    Loss 0.120577  0.000001\n",
            "Iteration 00013    Loss 0.113351  0.000001\n",
            "Iteration 00014    Loss 0.107959  0.000001\n",
            "Iteration 00015    Loss 0.102662  0.000001\n",
            "Iteration 00016    Loss 0.098717  0.000001\n",
            "Iteration 00017    Loss 0.095896  0.000001\n",
            "Iteration 00018    Loss 0.092125  0.000001\n",
            "Iteration 00019    Loss 0.090055  0.000001\n",
            "Iteration 00020    Loss 0.086879  0.000001\n",
            "Iteration 00021    Loss 0.085043  0.000001\n",
            "Iteration 00022    Loss 0.083096  0.000001\n",
            "Iteration 00023    Loss 0.081328  0.000001\n",
            "Iteration 00024    Loss 0.079442  0.000001\n",
            "Iteration 00025    Loss 0.077927  0.000001\n",
            "Iteration 00026    Loss 0.076386  0.000001\n",
            "Iteration 00027    Loss 0.074753  0.000001\n",
            "Iteration 00028    Loss 0.073250  0.000001\n",
            "Iteration 00029    Loss 0.072002  0.000001\n",
            "Iteration 00030    Loss 0.070701  0.000001\n",
            "Iteration 00031    Loss 0.069431  0.000001\n",
            "Iteration 00032    Loss 0.068186  0.000001\n",
            "Iteration 00033    Loss 0.066930  0.000002\n",
            "Iteration 00034    Loss 0.065853  0.000002\n",
            "Iteration 00035    Loss 0.064698  0.000002\n",
            "Iteration 00036    Loss 0.063656  0.000002\n",
            "Iteration 00037    Loss 0.062599  0.000002\n",
            "Iteration 00038    Loss 0.061465  0.000002\n",
            "Iteration 00039    Loss 0.060428  0.000003\n",
            "Iteration 00040    Loss 0.059407  0.000003\n",
            "Iteration 00041    Loss 0.058400  0.000003\n",
            "Iteration 00042    Loss 0.057407  0.000003\n",
            "Iteration 00043    Loss 0.056506  0.000003\n",
            "Iteration 00044    Loss 0.055539  0.000003\n",
            "Iteration 00045    Loss 0.054583  0.000004\n",
            "Iteration 00046    Loss 0.053651  0.000004\n",
            "Iteration 00047    Loss 0.052791  0.000004\n",
            "Iteration 00048    Loss 0.051908  0.000005\n",
            "Iteration 00049    Loss 0.050950  0.000005\n",
            "Iteration 00050    Loss 0.050160  0.000005\n",
            "Iteration 00051    Loss 0.049314  0.000006\n",
            "Iteration 00052    Loss 0.048485  0.000006\n",
            "Iteration 00053    Loss 0.047670  0.000006\n",
            "Iteration 00054    Loss 0.046897  0.000006\n",
            "Iteration 00055    Loss 0.046036  0.000007\n",
            "Iteration 00056    Loss 0.045257  0.000007\n",
            "Iteration 00057    Loss 0.044491  0.000007\n",
            "Iteration 00058    Loss 0.043751  0.000008\n",
            "Iteration 00059    Loss 0.043010  0.000008\n",
            "Iteration 00060    Loss 0.042232  0.000009\n",
            "Iteration 00061    Loss 0.041512  0.000008\n",
            "Iteration 00062    Loss 0.040784  0.000009\n",
            "Iteration 00063    Loss 0.040081  0.000009\n",
            "Iteration 00064    Loss 0.039327  0.000010\n",
            "Iteration 00065    Loss 0.038659  0.000010\n",
            "Iteration 00066    Loss 0.038006  0.000010\n",
            "Iteration 00067    Loss 0.037296  0.000011\n",
            "Iteration 00068    Loss 0.036648  0.000011\n",
            "Iteration 00069    Loss 0.036045  0.000011\n",
            "Iteration 00070    Loss 0.035367  0.000011\n",
            "Iteration 00071    Loss 0.034702  0.000012\n",
            "Iteration 00072    Loss 0.034157  0.000012\n",
            "Iteration 00073    Loss 0.033561  0.000012\n",
            "Iteration 00074    Loss 0.032850  0.000012\n",
            "Iteration 00075    Loss 0.032142  0.000013\n",
            "Iteration 00076    Loss 0.031491  0.000013\n",
            "Iteration 00077    Loss 0.030940  0.000013\n",
            "Iteration 00078    Loss 0.030327  0.000014\n",
            "Iteration 00079    Loss 0.029717  0.000014\n",
            "Iteration 00080    Loss 0.029193  0.000015\n",
            "Iteration 00081    Loss 0.028619  0.000015\n",
            "Iteration 00082    Loss 0.028022  0.000016\n",
            "Iteration 00083    Loss 0.027381  0.000016\n",
            "Iteration 00084    Loss 0.026900  0.000018\n",
            "Iteration 00085    Loss 0.026362  0.000019\n",
            "Iteration 00086    Loss 0.025735  0.000017\n",
            "Iteration 00087    Loss 0.025169  0.000018\n",
            "Iteration 00088    Loss 0.024559  0.000020\n",
            "Iteration 00089    Loss 0.024106  0.000020\n",
            "Iteration 00090    Loss 0.023602  0.000021\n",
            "Iteration 00091    Loss 0.023081  0.000021\n",
            "Iteration 00092    Loss 0.022515  0.000020\n",
            "Iteration 00093    Loss 0.021926  0.000023\n",
            "Iteration 00094    Loss 0.021437  0.000022\n",
            "Iteration 00095    Loss 0.020971  0.000023\n",
            "Iteration 00096    Loss 0.020352  0.000025\n",
            "Iteration 00097    Loss 0.019907  0.000024\n",
            "Iteration 00098    Loss 0.019362  0.000025\n",
            "Iteration 00099    Loss 0.018851  0.000022\n",
            "Iteration 00100    Loss 0.018396  0.000025\n",
            "Iteration 00101    Loss 0.017851  0.000027\n",
            "Iteration 00102    Loss 0.017436  0.000026\n",
            "Iteration 00103    Loss 0.016956  0.000030\n",
            "Iteration 00104    Loss 0.016561  0.000023\n",
            "Iteration 00105    Loss 0.016067  0.000031\n",
            "Iteration 00106    Loss 0.015534  0.000024\n",
            "Iteration 00107    Loss 0.015108  0.000030\n",
            "Iteration 00108    Loss 0.014710  0.000035\n",
            "Iteration 00109    Loss 0.014259  0.000026\n",
            "Iteration 00110    Loss 0.013795  0.000030\n",
            "Iteration 00111    Loss 0.013340  0.000037\n",
            "Iteration 00112    Loss 0.012941  0.000028\n",
            "Iteration 00113    Loss 0.012500  0.000032\n",
            "Iteration 00114    Loss 0.012130  0.000037\n",
            "Iteration 00115    Loss 0.011643  0.000034\n",
            "Iteration 00116    Loss 0.011250  0.000030\n",
            "Iteration 00117    Loss 0.010788  0.000035\n",
            "Iteration 00118    Loss 0.010394  0.000039\n",
            "Iteration 00119    Loss 0.009993  0.000036\n",
            "Iteration 00120    Loss 0.009590  0.000035\n",
            "Iteration 00121    Loss 0.009222  0.000039\n",
            "Iteration 00122    Loss 0.008781  0.000043\n",
            "Iteration 00123    Loss 0.008424  0.000041\n",
            "Iteration 00124    Loss 0.008040  0.000038\n",
            "Iteration 00125    Loss 0.007662  0.000043\n",
            "Iteration 00126    Loss 0.007313  0.000044\n",
            "Iteration 00127    Loss 0.006916  0.000046\n",
            "Iteration 00128    Loss 0.006556  0.000044\n",
            "Iteration 00129    Loss 0.006194  0.000044\n",
            "Iteration 00130    Loss 0.005816  0.000048\n",
            "Iteration 00131    Loss 0.005419  0.000050\n",
            "Iteration 00132    Loss 0.005056  0.000048\n",
            "Iteration 00133    Loss 0.004713  0.000050\n",
            "Iteration 00134    Loss 0.004435  0.000056\n",
            "Iteration 00135    Loss 0.004021  0.000048\n",
            "Iteration 00136    Loss 0.003715  0.000057\n",
            "Iteration 00137    Loss 0.003397  0.000053\n",
            "Iteration 00138    Loss 0.003008  0.000062\n",
            "Iteration 00139    Loss 0.002693  0.000058\n",
            "Iteration 00140    Loss 0.002366  0.000060\n",
            "Iteration 00141    Loss 0.002027  0.000063\n",
            "Iteration 00142    Loss 0.001764  0.000062\n",
            "Iteration 00143    Loss 0.001475  0.000062\n",
            "Iteration 00144    Loss 0.001176  0.000058\n",
            "Iteration 00145    Loss 0.000759  0.000070\n",
            "Iteration 00146    Loss 0.000543  0.000055\n",
            "Iteration 00147    Loss 0.000176  0.000067\n",
            "Iteration 00148    Loss -0.000219  0.000064\n",
            "Iteration 00149    Loss -0.000396  0.000055\n",
            "Iteration 00150    Loss -0.000802  0.000061\n",
            "Iteration 00151    Loss -0.001049  0.000072\n",
            "Iteration 00152    Loss -0.001401  0.000068\n",
            "Iteration 00153    Loss -0.001681  0.000060\n",
            "Iteration 00154    Loss -0.002001  0.000072\n",
            "Iteration 00155    Loss -0.002333  0.000070\n",
            "Iteration 00156    Loss -0.002608  0.000076\n",
            "Iteration 00157    Loss -0.002913  0.000065\n",
            "Iteration 00158    Loss -0.003187  0.000079\n",
            "Iteration 00159    Loss -0.003480  0.000079\n",
            "Iteration 00160    Loss -0.003723  0.000070\n",
            "Iteration 00161    Loss -0.004015  0.000077\n",
            "Iteration 00162    Loss -0.004266  0.000086\n",
            "Iteration 00163    Loss -0.004530  0.000086\n",
            "Iteration 00164    Loss -0.004843  0.000089\n",
            "Iteration 00165    Loss -0.005154  0.000079\n",
            "Iteration 00166    Loss -0.005425  0.000089\n",
            "Iteration 00167    Loss -0.005691  0.000082\n",
            "Iteration 00168    Loss -0.005956  0.000092\n",
            "Iteration 00169    Loss -0.006234  0.000084\n",
            "Iteration 00170    Loss -0.006458  0.000090\n",
            "Iteration 00171    Loss -0.006693  0.000089\n",
            "Iteration 00172    Loss -0.006965  0.000099\n",
            "Iteration 00173    Loss -0.007351  0.000091\n",
            "Iteration 00174    Loss -0.007593  0.000105\n",
            "Iteration 00175    Loss -0.007849  0.000091\n",
            "Iteration 00176    Loss -0.008133  0.000111\n",
            "Iteration 00177    Loss -0.008416  0.000092\n",
            "Iteration 00178    Loss -0.008700  0.000109\n",
            "Iteration 00179    Loss -0.008964  0.000104\n",
            "Iteration 00180    Loss -0.009275  0.000099\n",
            "Iteration 00181    Loss -0.009504  0.000111\n",
            "Iteration 00182    Loss -0.009792  0.000094\n",
            "Iteration 00183    Loss -0.010088  0.000111\n",
            "Iteration 00184    Loss -0.010308  0.000110\n",
            "Iteration 00185    Loss -0.010572  0.000109\n",
            "Iteration 00186    Loss -0.010829  0.000107\n",
            "Iteration 00187    Loss -0.011124  0.000119\n",
            "Iteration 00188    Loss -0.011355  0.000107\n",
            "Iteration 00189    Loss -0.011595  0.000120\n",
            "Iteration 00190    Loss -0.011845  0.000116\n",
            "Iteration 00191    Loss -0.012084  0.000116\n",
            "Iteration 00192    Loss -0.012320  0.000117\n",
            "Iteration 00193    Loss -0.012523  0.000129\n",
            "Iteration 00194    Loss -0.012767  0.000102\n",
            "Iteration 00195    Loss -0.012962  0.000133\n",
            "Iteration 00196    Loss -0.013126  0.000094\n",
            "Iteration 00197    Loss -0.013397  0.000127\n",
            "Iteration 00198    Loss -0.013586  0.000114\n",
            "Iteration 00199    Loss -0.013830  0.000107\n",
            "Iteration 00200    Loss -0.014053  0.000117\n",
            "Iteration 00201    Loss -0.014258  0.000128\n",
            "Iteration 00202    Loss -0.014475  0.000107\n",
            "Iteration 00203    Loss -0.014710  0.000119\n",
            "Iteration 00204    Loss -0.014919  0.000127\n",
            "Iteration 00205    Loss -0.015133  0.000117\n",
            "Iteration 00206    Loss -0.015317  0.000127\n",
            "Iteration 00207    Loss -0.015499  0.000129\n",
            "Iteration 00208    Loss -0.015725  0.000128\n",
            "Iteration 00209    Loss -0.015883  0.000135\n",
            "Iteration 00210    Loss -0.016105  0.000127\n",
            "Iteration 00211    Loss -0.016311  0.000134\n",
            "Iteration 00212    Loss -0.016500  0.000142\n",
            "Iteration 00213    Loss -0.016701  0.000128\n",
            "Iteration 00214    Loss -0.016899  0.000142\n",
            "Iteration 00215    Loss -0.017085  0.000145\n",
            "Iteration 00216    Loss -0.017255  0.000124\n",
            "Iteration 00217    Loss -0.017429  0.000152\n",
            "Iteration 00218    Loss -0.017596  0.000129\n",
            "Iteration 00219    Loss -0.017812  0.000143\n",
            "Iteration 00220    Loss -0.017992  0.000145\n",
            "Iteration 00221    Loss -0.018049  0.000111\n",
            "Iteration 00222    Loss -0.018328  0.000132\n",
            "Iteration 00223    Loss -0.018491  0.000141\n",
            "Iteration 00224    Loss -0.018673  0.000117\n",
            "Iteration 00225    Loss -0.018838  0.000122\n",
            "Iteration 00226    Loss -0.018988  0.000123\n",
            "Iteration 00227    Loss -0.019240  0.000141\n",
            "Iteration 00228    Loss -0.019394  0.000130\n",
            "Iteration 00229    Loss -0.019574  0.000126\n",
            "Iteration 00230    Loss -0.019765  0.000146\n",
            "Iteration 00231    Loss -0.019927  0.000135\n",
            "Iteration 00232    Loss -0.020110  0.000124\n",
            "Iteration 00233    Loss -0.020267  0.000149\n",
            "Iteration 00234    Loss -0.020433  0.000153\n",
            "Iteration 00235    Loss -0.020581  0.000140\n",
            "Iteration 00236    Loss -0.020748  0.000153\n",
            "Iteration 00237    Loss -0.020932  0.000141\n",
            "Iteration 00238    Loss -0.021077  0.000157\n",
            "Iteration 00239    Loss -0.021205  0.000140\n",
            "Iteration 00240    Loss -0.021356  0.000147\n",
            "Iteration 00241    Loss -0.021541  0.000161\n",
            "Iteration 00242    Loss -0.021699  0.000139\n",
            "Iteration 00243    Loss -0.021834  0.000157\n",
            "Iteration 00244    Loss -0.021984  0.000139\n",
            "Iteration 00245    Loss -0.022152  0.000151\n",
            "Iteration 00246    Loss -0.022334  0.000162\n",
            "Iteration 00247    Loss -0.022473  0.000149\n",
            "Iteration 00248    Loss -0.022658  0.000160\n",
            "Iteration 00249    Loss -0.022767  0.000153\n",
            "Iteration 00250    Loss -0.022949  0.000149\n",
            "Iteration 00251    Loss -0.023093  0.000164\n",
            "Iteration 00252    Loss -0.023252  0.000159\n",
            "Iteration 00253    Loss -0.023372  0.000170\n",
            "Iteration 00254    Loss -0.023535  0.000160\n",
            "Iteration 00255    Loss -0.023691  0.000174\n",
            "Iteration 00256    Loss -0.023781  0.000166\n",
            "Iteration 00257    Loss -0.023923  0.000173\n",
            "Iteration 00258    Loss -0.024067  0.000148\n",
            "Iteration 00259    Loss -0.024190  0.000185\n",
            "Iteration 00260    Loss -0.024265  0.000125\n",
            "Iteration 00261    Loss -0.024486  0.000173\n",
            "Iteration 00262    Loss -0.024651  0.000185\n",
            "Iteration 00263    Loss -0.024767  0.000135\n",
            "Iteration 00264    Loss -0.024912  0.000155\n",
            "Iteration 00265    Loss -0.025037  0.000182\n",
            "Iteration 00266    Loss -0.025155  0.000145\n",
            "Iteration 00267    Loss -0.025320  0.000173\n",
            "Iteration 00268    Loss -0.025443  0.000187\n",
            "Iteration 00269    Loss -0.025600  0.000158\n",
            "Iteration 00270    Loss -0.025752  0.000164\n",
            "Iteration 00271    Loss -0.025851  0.000196\n",
            "Iteration 00272    Loss -0.025943  0.000163\n",
            "Iteration 00273    Loss -0.026045  0.000174\n",
            "Iteration 00274    Loss -0.026172  0.000174\n",
            "Iteration 00275    Loss -0.026325  0.000174\n",
            "Iteration 00276    Loss -0.026486  0.000165\n",
            "Iteration 00277    Loss -0.026610  0.000179\n",
            "Iteration 00278    Loss -0.026731  0.000181\n",
            "Iteration 00279    Loss -0.026817  0.000153\n",
            "Iteration 00280    Loss -0.026999  0.000192\n",
            "Iteration 00281    Loss -0.027140  0.000186\n",
            "Iteration 00282    Loss -0.027246  0.000165\n",
            "Iteration 00283    Loss -0.027316  0.000198\n",
            "Iteration 00284    Loss -0.027412  0.000169\n",
            "Iteration 00285    Loss -0.027586  0.000183\n",
            "Iteration 00286    Loss -0.027702  0.000188\n",
            "Iteration 00287    Loss -0.027796  0.000178\n",
            "Iteration 00288    Loss -0.027902  0.000191\n",
            "Iteration 00289    Loss -0.028017  0.000179\n",
            "Iteration 00290    Loss -0.028161  0.000173\n",
            "Iteration 00291    Loss -0.028244  0.000195\n",
            "Iteration 00292    Loss -0.028376  0.000167\n",
            "Iteration 00293    Loss -0.028502  0.000173\n",
            "Iteration 00294    Loss -0.028617  0.000193\n",
            "Iteration 00295    Loss -0.028721  0.000178\n",
            "Iteration 00296    Loss -0.028829  0.000184\n",
            "Iteration 00297    Loss -0.028946  0.000188\n",
            "Iteration 00298    Loss -0.029056  0.000191\n",
            "Iteration 00299    Loss -0.029145  0.000197\n",
            "Iteration 00300    Loss -0.029259  0.000174\n",
            "Iteration 00301    Loss -0.029372  0.000196\n",
            "Iteration 00302    Loss -0.029463  0.000190\n",
            "Iteration 00303    Loss -0.029580  0.000201\n",
            "Iteration 00304    Loss -0.029707  0.000202\n",
            "Iteration 00305    Loss -0.029796  0.000184\n",
            "Iteration 00306    Loss -0.029887  0.000211\n",
            "Iteration 00307    Loss -0.030000  0.000197\n",
            "Iteration 00308    Loss -0.030117  0.000197\n",
            "Iteration 00309    Loss -0.030210  0.000213\n",
            "Iteration 00310    Loss -0.030280  0.000186\n",
            "Iteration 00311    Loss -0.030408  0.000220\n",
            "Iteration 00312    Loss -0.030505  0.000193\n",
            "Iteration 00313    Loss -0.030567  0.000207\n",
            "Iteration 00314    Loss -0.030641  0.000200\n",
            "Iteration 00315    Loss -0.030775  0.000203\n",
            "Iteration 00316    Loss -0.030884  0.000191\n",
            "Iteration 00317    Loss -0.030967  0.000214\n",
            "Iteration 00318    Loss -0.031051  0.000186\n",
            "Iteration 00319    Loss -0.031155  0.000210\n",
            "Iteration 00320    Loss -0.031186  0.000194\n",
            "Iteration 00321    Loss -0.031326  0.000205\n",
            "Iteration 00322    Loss -0.031439  0.000192\n",
            "Iteration 00323    Loss -0.031531  0.000210\n",
            "Iteration 00324    Loss -0.031679  0.000205\n",
            "Iteration 00325    Loss -0.031736  0.000184\n",
            "Iteration 00326    Loss -0.031792  0.000227\n",
            "Iteration 00327    Loss -0.031945  0.000194\n",
            "Iteration 00328    Loss -0.032003  0.000197\n",
            "Iteration 00329    Loss -0.032119  0.000211\n",
            "Iteration 00330    Loss -0.032233  0.000217\n",
            "Iteration 00331    Loss -0.032322  0.000211\n",
            "Iteration 00332    Loss -0.032382  0.000207\n",
            "Iteration 00333    Loss -0.032494  0.000225\n",
            "Iteration 00334    Loss -0.032578  0.000217\n",
            "Iteration 00335    Loss -0.032662  0.000229\n",
            "Iteration 00336    Loss -0.032726  0.000222\n",
            "Iteration 00337    Loss -0.032784  0.000219\n",
            "Iteration 00338    Loss -0.032834  0.000211\n",
            "Iteration 00339    Loss -0.032974  0.000234\n",
            "Iteration 00340    Loss -0.033076  0.000209\n",
            "Iteration 00341    Loss -0.033139  0.000236\n",
            "Iteration 00342    Loss -0.033225  0.000221\n",
            "Iteration 00343    Loss -0.033288  0.000215\n",
            "Iteration 00344    Loss -0.033282  0.000247\n",
            "Iteration 00345    Loss -0.033302  0.000180\n",
            "Iteration 00346    Loss -0.033414  0.000224\n",
            "Iteration 00347    Loss -0.033566  0.000211\n",
            "Iteration 00348    Loss -0.033649  0.000197\n",
            "Iteration 00349    Loss -0.033718  0.000211\n",
            "Iteration 00350    Loss -0.033834  0.000213\n",
            "Iteration 00351    Loss -0.033911  0.000195\n",
            "Iteration 00352    Loss -0.033998  0.000220\n",
            "Iteration 00353    Loss -0.034064  0.000204\n",
            "Iteration 00354    Loss -0.034144  0.000217\n",
            "Iteration 00355    Loss -0.034232  0.000213\n",
            "Iteration 00356    Loss -0.034302  0.000210\n",
            "Iteration 00357    Loss -0.034406  0.000238\n",
            "Iteration 00358    Loss -0.034470  0.000217\n",
            "Iteration 00359    Loss -0.034558  0.000222\n",
            "Iteration 00360    Loss -0.034635  0.000227\n",
            "Iteration 00361    Loss -0.034714  0.000233\n",
            "Iteration 00362    Loss -0.034786  0.000222\n",
            "Iteration 00363    Loss -0.034887  0.000234\n",
            "Iteration 00364    Loss -0.034947  0.000231\n",
            "Iteration 00365    Loss -0.035022  0.000233\n",
            "Iteration 00366    Loss -0.035073  0.000256\n",
            "Iteration 00367    Loss -0.035152  0.000211\n",
            "Iteration 00368    Loss -0.035215  0.000256\n",
            "Iteration 00369    Loss -0.035261  0.000218\n",
            "Iteration 00370    Loss -0.035318  0.000263\n",
            "Iteration 00371    Loss -0.035327  0.000215\n",
            "Iteration 00372    Loss -0.035447  0.000232\n",
            "Iteration 00373    Loss -0.035544  0.000245\n",
            "Iteration 00374    Loss -0.035586  0.000229\n",
            "Iteration 00375    Loss -0.035670  0.000242\n",
            "Iteration 00376    Loss -0.035723  0.000221\n",
            "Iteration 00377    Loss -0.035860  0.000225\n",
            "Iteration 00378    Loss -0.035894  0.000249\n",
            "Iteration 00379    Loss -0.035943  0.000215\n",
            "Iteration 00380    Loss -0.036053  0.000250\n",
            "Iteration 00381    Loss -0.036108  0.000253\n",
            "Iteration 00382    Loss -0.036154  0.000216\n",
            "Iteration 00383    Loss -0.036245  0.000265\n",
            "Iteration 00384    Loss -0.036311  0.000238\n",
            "Iteration 00385    Loss -0.036373  0.000243\n",
            "Iteration 00386    Loss -0.036456  0.000253\n",
            "Iteration 00387    Loss -0.036479  0.000264\n",
            "Iteration 00388    Loss -0.036385  0.000211\n",
            "Iteration 00389    Loss -0.036388  0.000287\n",
            "Iteration 00390    Loss -0.036442  0.000175\n",
            "Iteration 00391    Loss -0.036581  0.000208\n",
            "Iteration 00392    Loss -0.036666  0.000234\n",
            "Iteration 00393    Loss -0.036681  0.000208\n",
            "Iteration 00394    Loss -0.036817  0.000213\n",
            "Iteration 00395    Loss -0.036852  0.000238\n",
            "Iteration 00396    Loss -0.036973  0.000222\n",
            "Iteration 00397    Loss -0.036991  0.000217\n",
            "Iteration 00398    Loss -0.037088  0.000237\n",
            "Iteration 00399    Loss -0.037164  0.000228\n",
            "Iteration 00400    Loss -0.037239  0.000233\n",
            "Iteration 00401    Loss -0.037306  0.000248\n",
            "Iteration 00402    Loss -0.037350  0.000241\n",
            "Iteration 00403    Loss -0.037430  0.000227\n",
            "Iteration 00404    Loss -0.037480  0.000242\n",
            "Iteration 00405    Loss -0.037559  0.000245\n",
            "Iteration 00406    Loss -0.037602  0.000242\n",
            "Iteration 00407    Loss -0.037690  0.000255\n",
            "Iteration 00408    Loss -0.037746  0.000252\n",
            "Iteration 00409    Loss -0.037786  0.000255\n",
            "Iteration 00410    Loss -0.037858  0.000259\n",
            "Iteration 00411    Loss -0.037921  0.000253\n",
            "Iteration 00412    Loss -0.037967  0.000251\n",
            "Iteration 00413    Loss -0.038021  0.000263\n",
            "Iteration 00414    Loss -0.038086  0.000256\n",
            "Iteration 00415    Loss -0.038146  0.000260\n",
            "Iteration 00416    Loss -0.038202  0.000266\n",
            "Iteration 00417    Loss -0.038248  0.000275\n",
            "Iteration 00418    Loss -0.038304  0.000258\n",
            "Iteration 00419    Loss -0.038349  0.000272\n",
            "Iteration 00420    Loss -0.038404  0.000263\n",
            "Iteration 00421    Loss -0.038456  0.000284\n",
            "Iteration 00422    Loss -0.038502  0.000256\n",
            "Iteration 00423    Loss -0.038562  0.000283\n",
            "Iteration 00424    Loss -0.038607  0.000265\n",
            "Iteration 00425    Loss -0.038647  0.000290\n",
            "Iteration 00426    Loss -0.038696  0.000251\n",
            "Iteration 00427    Loss -0.038732  0.000290\n",
            "Iteration 00428    Loss -0.038730  0.000273\n",
            "Iteration 00429    Loss -0.038713  0.000253\n",
            "Iteration 00430    Loss -0.038663  0.000289\n",
            "Iteration 00431    Loss -0.038771  0.000176\n",
            "Iteration 00432    Loss -0.038904  0.000255\n",
            "Iteration 00433    Loss -0.038837  0.000280\n",
            "Iteration 00434    Loss -0.038979  0.000200\n",
            "Iteration 00435    Loss -0.038994  0.000226\n",
            "Iteration 00436    Loss -0.039044  0.000268\n",
            "Iteration 00437    Loss -0.039155  0.000244\n",
            "Iteration 00438    Loss -0.039149  0.000223\n",
            "Iteration 00439    Loss -0.039270  0.000244\n",
            "Iteration 00440    Loss -0.039282  0.000252\n",
            "Iteration 00441    Loss -0.039376  0.000258\n",
            "Iteration 00442    Loss -0.039373  0.000267\n",
            "Iteration 00443    Loss -0.039459  0.000250\n",
            "Iteration 00444    Loss -0.039498  0.000273\n",
            "Iteration 00445    Loss -0.039529  0.000255\n",
            "Iteration 00446    Loss -0.039593  0.000250\n",
            "Iteration 00447    Loss -0.039656  0.000281\n",
            "Iteration 00448    Loss -0.039694  0.000287\n",
            "Iteration 00449    Loss -0.039745  0.000257\n",
            "Iteration 00450    Loss -0.039781  0.000281\n",
            "Iteration 00451    Loss -0.039808  0.000289\n",
            "Iteration 00452    Loss -0.039845  0.000257\n",
            "Iteration 00453    Loss -0.039855  0.000288\n",
            "Iteration 00454    Loss -0.039880  0.000248\n",
            "Iteration 00455    Loss -0.039934  0.000295\n",
            "Iteration 00456    Loss -0.040019  0.000261\n",
            "Iteration 00457    Loss -0.040084  0.000259\n",
            "Iteration 00458    Loss -0.040113  0.000284\n",
            "Iteration 00459    Loss -0.040148  0.000268\n",
            "Iteration 00460    Loss -0.040196  0.000279\n",
            "Iteration 00461    Loss -0.040236  0.000275\n",
            "Iteration 00462    Loss -0.040297  0.000267\n",
            "Iteration 00463    Loss -0.040335  0.000286\n",
            "Iteration 00464    Loss -0.040358  0.000268\n",
            "Iteration 00465    Loss -0.040425  0.000278\n",
            "Iteration 00466    Loss -0.040452  0.000287\n",
            "Iteration 00467    Loss -0.040488  0.000274\n",
            "Iteration 00468    Loss -0.040535  0.000296\n",
            "Iteration 00469    Loss -0.040583  0.000271\n",
            "Iteration 00470    Loss -0.040617  0.000288\n",
            "Iteration 00471    Loss -0.040625  0.000286\n",
            "Iteration 00472    Loss -0.040667  0.000272\n",
            "Iteration 00473    Loss -0.040709  0.000295\n",
            "Iteration 00474    Loss -0.040749  0.000284\n",
            "Iteration 00475    Loss -0.040815  0.000275\n",
            "Iteration 00476    Loss -0.040842  0.000304\n",
            "Iteration 00477    Loss -0.040861  0.000266\n",
            "Iteration 00478    Loss -0.040899  0.000296\n",
            "Iteration 00479    Loss -0.040929  0.000296\n",
            "Iteration 00480    Loss -0.040986  0.000287\n",
            "Iteration 00481    Loss -0.041015  0.000287\n",
            "Iteration 00482    Loss -0.041041  0.000301\n",
            "Iteration 00483    Loss -0.041049  0.000262\n",
            "Iteration 00484    Loss -0.041091  0.000304\n",
            "Iteration 00485    Loss -0.041101  0.000281\n",
            "Iteration 00486    Loss -0.041112  0.000287\n",
            "Iteration 00487    Loss -0.041122  0.000278\n",
            "Iteration 00488    Loss -0.041187  0.000314\n",
            "Iteration 00489    Loss -0.041253  0.000246\n",
            "Iteration 00490    Loss -0.041326  0.000278\n",
            "Iteration 00491    Loss -0.041329  0.000303\n",
            "Iteration 00492    Loss -0.041352  0.000268\n",
            "Iteration 00493    Loss -0.041409  0.000280\n",
            "Iteration 00494    Loss -0.041451  0.000290\n",
            "Iteration 00495    Loss -0.041472  0.000281\n",
            "Iteration 00496    Loss -0.041516  0.000288\n",
            "Iteration 00497    Loss -0.041554  0.000292\n",
            "Iteration 00498    Loss -0.041601  0.000294\n",
            "Iteration 00499    Loss -0.041640  0.000295\n",
            "100% 500/500 [4:45:31<00:00, 34.26s/it]\n"
          ]
        }
      ],
      "source": [
        "!python RW_dehazing.py -input /content/SOTS/outdoor/hazy/ -output out/outdoor/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FdzQ0y5jTfAb",
        "outputId": "4b094732-07d2-46e3-f44a-e7d4c0872ed8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Iteration 00010    Loss 0.066340  0.000001\n",
            "Iteration 00011    Loss 0.060927  0.000001\n",
            "Iteration 00012    Loss 0.057587  0.000001\n",
            "Iteration 00013    Loss 0.054155  0.000002\n",
            "Iteration 00014    Loss 0.050854  0.000002\n",
            "Iteration 00015    Loss 0.048609  0.000002\n",
            "Iteration 00016    Loss 0.046255  0.000003\n",
            "Iteration 00017    Loss 0.044611  0.000003\n",
            "Iteration 00018    Loss 0.042822  0.000004\n",
            "Iteration 00019    Loss 0.040845  0.000004\n",
            "Iteration 00020    Loss 0.039369  0.000004\n",
            "Iteration 00021    Loss 0.038054  0.000005\n",
            "Iteration 00022    Loss 0.036617  0.000005\n",
            "Iteration 00023    Loss 0.035328  0.000006\n",
            "Iteration 00024    Loss 0.034022  0.000007\n",
            "Iteration 00025    Loss 0.032994  0.000008\n",
            "Iteration 00026    Loss 0.031837  0.000009\n",
            "Iteration 00027    Loss 0.030884  0.000010\n",
            "Iteration 00028    Loss 0.030050  0.000010\n",
            "Iteration 00029    Loss 0.028987  0.000010\n",
            "Iteration 00030    Loss 0.027963  0.000011\n",
            "Iteration 00031    Loss 0.026879  0.000012\n",
            "Iteration 00032    Loss 0.026022  0.000014\n",
            "Iteration 00033    Loss 0.025101  0.000016\n",
            "Iteration 00034    Loss 0.024035  0.000016\n",
            "Iteration 00035    Loss 0.022988  0.000016\n",
            "Iteration 00036    Loss 0.021996  0.000017\n",
            "Iteration 00037    Loss 0.021171  0.000019\n",
            "Iteration 00038    Loss 0.020422  0.000020\n",
            "Iteration 00039    Loss 0.019557  0.000021\n",
            "Iteration 00040    Loss 0.018746  0.000021\n",
            "Iteration 00041    Loss 0.017926  0.000023\n",
            "Iteration 00042    Loss 0.017160  0.000024\n",
            "Iteration 00043    Loss 0.016167  0.000024\n",
            "Iteration 00044    Loss 0.015505  0.000025\n",
            "Iteration 00045    Loss 0.014763  0.000028\n",
            "Iteration 00046    Loss 0.013921  0.000029\n",
            "Iteration 00047    Loss 0.013130  0.000029\n",
            "Iteration 00048    Loss 0.012479  0.000029\n",
            "Iteration 00049    Loss 0.011943  0.000031\n",
            "Iteration 00050    Loss 0.010932  0.000032\n",
            "Iteration 00051    Loss 0.010460  0.000033\n",
            "Iteration 00052    Loss 0.009762  0.000032\n",
            "Iteration 00053    Loss 0.009113  0.000034\n",
            "Iteration 00054    Loss 0.008483  0.000036\n",
            "Iteration 00055    Loss 0.007821  0.000039\n",
            "Iteration 00056    Loss 0.007187  0.000039\n",
            "Iteration 00057    Loss 0.006616  0.000038\n",
            "Iteration 00058    Loss 0.006055  0.000040\n",
            "Iteration 00059    Loss 0.005521  0.000042\n",
            "Iteration 00060    Loss 0.004912  0.000043\n",
            "Iteration 00061    Loss 0.004390  0.000041\n",
            "Iteration 00062    Loss 0.004036  0.000045\n",
            "Iteration 00063    Loss 0.003245  0.000050\n",
            "Iteration 00064    Loss 0.002711  0.000048\n",
            "Iteration 00065    Loss 0.002274  0.000047\n",
            "Iteration 00066    Loss 0.001752  0.000052\n",
            "Iteration 00067    Loss 0.001277  0.000056\n",
            "Iteration 00068    Loss 0.000794  0.000054\n",
            "Iteration 00069    Loss 0.000137  0.000050\n",
            "Iteration 00070    Loss -0.000334  0.000054\n",
            "Iteration 00071    Loss -0.000792  0.000062\n",
            "Iteration 00072    Loss -0.001237  0.000063\n",
            "Iteration 00073    Loss -0.001747  0.000058\n",
            "Iteration 00074    Loss -0.002146  0.000059\n",
            "Iteration 00075    Loss -0.002645  0.000065\n",
            "Iteration 00076    Loss -0.003064  0.000063\n",
            "Iteration 00077    Loss -0.003469  0.000065\n",
            "Iteration 00078    Loss -0.003940  0.000065\n",
            "Iteration 00079    Loss -0.004246  0.000067\n",
            "Iteration 00080    Loss -0.004812  0.000070\n",
            "Iteration 00081    Loss -0.005260  0.000073\n",
            "Iteration 00082    Loss -0.005627  0.000077\n",
            "Iteration 00083    Loss -0.005831  0.000077\n",
            "Iteration 00084    Loss -0.006402  0.000077\n",
            "Iteration 00085    Loss -0.006805  0.000083\n",
            "Iteration 00086    Loss -0.007198  0.000082\n",
            "Iteration 00087    Loss -0.007554  0.000085\n",
            "Iteration 00088    Loss -0.007937  0.000090\n",
            "Iteration 00089    Loss -0.008338  0.000092\n",
            "Iteration 00090    Loss -0.008670  0.000092\n",
            "Iteration 00091    Loss -0.008928  0.000100\n",
            "Iteration 00092    Loss -0.009280  0.000107\n",
            "Iteration 00093    Loss -0.009666  0.000096\n",
            "Iteration 00094    Loss -0.009736  0.000095\n",
            "Iteration 00095    Loss -0.010419  0.000109\n",
            "Iteration 00096    Loss -0.010746  0.000106\n",
            "Iteration 00097    Loss -0.011080  0.000105\n",
            "Iteration 00098    Loss -0.011404  0.000107\n",
            "Iteration 00099    Loss -0.011625  0.000104\n",
            "Iteration 00100    Loss -0.011945  0.000116\n",
            "Iteration 00101    Loss -0.012169  0.000111\n",
            "Iteration 00102    Loss -0.012623  0.000115\n",
            "Iteration 00103    Loss -0.012887  0.000118\n",
            "Iteration 00104    Loss -0.013332  0.000110\n",
            "Iteration 00105    Loss -0.013744  0.000119\n",
            "Iteration 00106    Loss -0.013923  0.000127\n",
            "Iteration 00107    Loss -0.014153  0.000123\n",
            "Iteration 00108    Loss -0.014517  0.000130\n",
            "Iteration 00109    Loss -0.014896  0.000134\n",
            "Iteration 00110    Loss -0.015104  0.000133\n",
            "Iteration 00111    Loss -0.015413  0.000136\n",
            "Iteration 00112    Loss -0.015788  0.000136\n",
            "Iteration 00113    Loss -0.016004  0.000135\n",
            "Iteration 00114    Loss -0.016278  0.000143\n",
            "Iteration 00115    Loss -0.016596  0.000144\n",
            "Iteration 00116    Loss -0.016876  0.000153\n",
            "Iteration 00117    Loss -0.017128  0.000159\n",
            "Iteration 00118    Loss -0.017245  0.000152\n",
            "Iteration 00119    Loss -0.017656  0.000156\n",
            "Iteration 00120    Loss -0.017905  0.000164\n",
            "Iteration 00121    Loss -0.018113  0.000166\n",
            "Iteration 00122    Loss -0.018489  0.000164\n",
            "Iteration 00123    Loss -0.018673  0.000160\n",
            "Iteration 00124    Loss -0.018880  0.000159\n",
            "Iteration 00125    Loss -0.019042  0.000171\n",
            "Iteration 00126    Loss -0.019183  0.000160\n",
            "Iteration 00127    Loss -0.019379  0.000158\n",
            "Iteration 00128    Loss -0.019615  0.000167\n",
            "Iteration 00129    Loss -0.020104  0.000171\n",
            "Iteration 00130    Loss -0.020210  0.000164\n",
            "Iteration 00131    Loss -0.020430  0.000156\n",
            "Iteration 00132    Loss -0.020670  0.000161\n",
            "Iteration 00133    Loss -0.020967  0.000175\n",
            "Iteration 00134    Loss -0.021267  0.000166\n",
            "Iteration 00135    Loss -0.021469  0.000161\n",
            "Iteration 00136    Loss -0.021596  0.000184\n",
            "Iteration 00137    Loss -0.021868  0.000195\n",
            "Iteration 00138    Loss -0.022038  0.000194\n",
            "Iteration 00139    Loss -0.022281  0.000193\n",
            "Iteration 00140    Loss -0.022463  0.000179\n",
            "Iteration 00141    Loss -0.022750  0.000185\n",
            "Iteration 00142    Loss -0.022906  0.000188\n",
            "Iteration 00143    Loss -0.023169  0.000189\n",
            "Iteration 00144    Loss -0.023400  0.000191\n",
            "Iteration 00145    Loss -0.023552  0.000201\n",
            "Iteration 00146    Loss -0.023759  0.000204\n",
            "Iteration 00147    Loss -0.023960  0.000195\n",
            "Iteration 00148    Loss -0.024193  0.000190\n",
            "Iteration 00149    Loss -0.024417  0.000200\n",
            "Iteration 00150    Loss -0.024555  0.000210\n",
            "Iteration 00151    Loss -0.024759  0.000204\n",
            "Iteration 00152    Loss -0.024961  0.000200\n",
            "Iteration 00153    Loss -0.025064  0.000209\n",
            "Iteration 00154    Loss -0.025277  0.000218\n",
            "Iteration 00155    Loss -0.025442  0.000213\n",
            "Iteration 00156    Loss -0.025531  0.000222\n",
            "Iteration 00157    Loss -0.025813  0.000220\n",
            "Iteration 00158    Loss -0.025986  0.000227\n",
            "Iteration 00159    Loss -0.026235  0.000220\n",
            "Iteration 00160    Loss -0.026426  0.000216\n",
            "Iteration 00161    Loss -0.026576  0.000218\n",
            "Iteration 00162    Loss -0.026676  0.000224\n",
            "Iteration 00163    Loss -0.026839  0.000222\n",
            "Iteration 00164    Loss -0.027015  0.000220\n",
            "Iteration 00165    Loss -0.027188  0.000237\n",
            "Iteration 00166    Loss -0.027291  0.000228\n",
            "Iteration 00167    Loss -0.027551  0.000228\n",
            "Iteration 00168    Loss -0.027699  0.000232\n",
            "Iteration 00169    Loss -0.027809  0.000234\n",
            "Iteration 00170    Loss -0.027858  0.000244\n",
            "Iteration 00171    Loss -0.028085  0.000214\n",
            "Iteration 00172    Loss -0.028169  0.000224\n",
            "Iteration 00173    Loss -0.028400  0.000228\n",
            "Iteration 00174    Loss -0.028554  0.000217\n",
            "Iteration 00175    Loss -0.028681  0.000229\n",
            "Iteration 00176    Loss -0.028969  0.000227\n",
            "Iteration 00177    Loss -0.029068  0.000234\n",
            "Iteration 00178    Loss -0.029186  0.000252\n",
            "Iteration 00179    Loss -0.029334  0.000228\n",
            "Iteration 00180    Loss -0.029450  0.000232\n",
            "Iteration 00181    Loss -0.029628  0.000248\n",
            "Iteration 00182    Loss -0.029775  0.000246\n",
            "Iteration 00183    Loss -0.029979  0.000249\n",
            "Iteration 00184    Loss -0.030030  0.000245\n",
            "Iteration 00185    Loss -0.030201  0.000245\n",
            "Iteration 00186    Loss -0.030306  0.000244\n",
            "Iteration 00187    Loss -0.030457  0.000236\n",
            "Iteration 00188    Loss -0.030651  0.000244\n",
            "Iteration 00189    Loss -0.030783  0.000261\n",
            "Iteration 00190    Loss -0.030887  0.000246\n",
            "Iteration 00191    Loss -0.031038  0.000247\n",
            "Iteration 00192    Loss -0.031116  0.000260\n",
            "Iteration 00193    Loss -0.031295  0.000252\n",
            "Iteration 00194    Loss -0.031401  0.000254\n",
            "Iteration 00195    Loss -0.031572  0.000255\n",
            "Iteration 00196    Loss -0.031635  0.000267\n",
            "Iteration 00197    Loss -0.031807  0.000254\n",
            "Iteration 00198    Loss -0.031916  0.000266\n",
            "Iteration 00199    Loss -0.032018  0.000265\n",
            "Iteration 00200    Loss -0.032166  0.000258\n",
            "Iteration 00201    Loss -0.032226  0.000254\n",
            "Iteration 00202    Loss -0.032313  0.000262\n",
            "Iteration 00203    Loss -0.032115  0.000270\n",
            "Iteration 00204    Loss -0.032637  0.000261\n",
            "Iteration 00205    Loss -0.032769  0.000259\n",
            "Iteration 00206    Loss -0.032870  0.000274\n",
            "Iteration 00207    Loss -0.032901  0.000281\n",
            "Iteration 00208    Loss -0.033092  0.000272\n",
            "Iteration 00209    Loss -0.033101  0.000256\n",
            "Iteration 00210    Loss -0.033105  0.000273\n",
            "Iteration 00211    Loss -0.033394  0.000277\n",
            "Iteration 00212    Loss -0.033344  0.000259\n",
            "Iteration 00213    Loss -0.033375  0.000277\n",
            "Iteration 00214    Loss -0.033639  0.000260\n",
            "Iteration 00215    Loss -0.033803  0.000254\n",
            "Iteration 00216    Loss -0.033925  0.000268\n",
            "Iteration 00217    Loss -0.033987  0.000263\n",
            "Iteration 00218    Loss -0.034113  0.000272\n",
            "Iteration 00219    Loss -0.034144  0.000272\n",
            "Iteration 00220    Loss -0.034242  0.000260\n",
            "Iteration 00221    Loss -0.034416  0.000287\n",
            "Iteration 00222    Loss -0.034383  0.000269\n",
            "Iteration 00223    Loss -0.034633  0.000271\n",
            "Iteration 00224    Loss -0.034476  0.000278\n",
            "Iteration 00225    Loss -0.034693  0.000271\n",
            "Iteration 00226    Loss -0.034881  0.000282\n",
            "Iteration 00227    Loss -0.035031  0.000277\n",
            "Iteration 00228    Loss -0.035067  0.000277\n",
            "Iteration 00229    Loss -0.035220  0.000295\n",
            "Iteration 00230    Loss -0.035322  0.000276\n",
            "Iteration 00231    Loss -0.035389  0.000266\n",
            "Iteration 00232    Loss -0.035381  0.000278\n",
            "Iteration 00233    Loss -0.035566  0.000268\n",
            "Iteration 00234    Loss -0.035678  0.000283\n",
            "Iteration 00235    Loss -0.035739  0.000293\n",
            "Iteration 00236    Loss -0.035716  0.000286\n",
            "Iteration 00237    Loss -0.035927  0.000287\n",
            "Iteration 00238    Loss -0.036012  0.000296\n",
            "Iteration 00239    Loss -0.036066  0.000285\n",
            "Iteration 00240    Loss -0.036234  0.000299\n",
            "Iteration 00241    Loss -0.036310  0.000300\n",
            "Iteration 00242    Loss -0.036379  0.000284\n",
            "Iteration 00243    Loss -0.036485  0.000294\n",
            "Iteration 00244    Loss -0.036489  0.000286\n",
            "Iteration 00245    Loss -0.036602  0.000288\n",
            "Iteration 00246    Loss -0.036752  0.000298\n",
            "Iteration 00247    Loss -0.036820  0.000283\n",
            "Iteration 00248    Loss -0.036886  0.000292\n",
            "Iteration 00249    Loss -0.036978  0.000296\n",
            "Iteration 00250    Loss -0.037025  0.000301\n",
            "Iteration 00251    Loss -0.037147  0.000298\n",
            "Iteration 00252    Loss -0.037195  0.000287\n",
            "Iteration 00253    Loss -0.037264  0.000312\n",
            "Iteration 00254    Loss -0.037184  0.000293\n",
            "Iteration 00255    Loss -0.037431  0.000296\n",
            "Iteration 00256    Loss -0.037494  0.000310\n",
            "Iteration 00257    Loss -0.037493  0.000296\n",
            "Iteration 00258    Loss -0.037582  0.000281\n",
            "Iteration 00259    Loss -0.037649  0.000304\n",
            "Iteration 00260    Loss -0.037719  0.000292\n",
            "Iteration 00261    Loss -0.037826  0.000278\n",
            "Iteration 00262    Loss -0.037921  0.000299\n",
            "Iteration 00263    Loss -0.038027  0.000295\n",
            "Iteration 00264    Loss -0.038060  0.000299\n",
            "Iteration 00265    Loss -0.038127  0.000300\n",
            "Iteration 00266    Loss -0.038193  0.000302\n",
            "Iteration 00267    Loss -0.038271  0.000277\n",
            "Iteration 00268    Loss -0.038359  0.000299\n",
            "Iteration 00269    Loss -0.038298  0.000311\n",
            "Iteration 00270    Loss -0.038448  0.000277\n",
            "Iteration 00271    Loss -0.038416  0.000308\n",
            "Iteration 00272    Loss -0.038528  0.000297\n",
            "Iteration 00273    Loss -0.038656  0.000297\n",
            "Iteration 00274    Loss -0.038747  0.000288\n",
            "Iteration 00275    Loss -0.038845  0.000293\n",
            "Iteration 00276    Loss -0.038868  0.000308\n",
            "Iteration 00277    Loss -0.038955  0.000290\n",
            "Iteration 00278    Loss -0.039029  0.000307\n",
            "Iteration 00279    Loss -0.039027  0.000310\n",
            "Iteration 00280    Loss -0.039115  0.000282\n",
            "Iteration 00281    Loss -0.039146  0.000325\n",
            "Iteration 00282    Loss -0.039099  0.000276\n",
            "Iteration 00283    Loss -0.039292  0.000290\n",
            "Iteration 00284    Loss -0.039318  0.000296\n",
            "Iteration 00285    Loss -0.039344  0.000283\n",
            "Iteration 00286    Loss -0.039468  0.000298\n",
            "Iteration 00287    Loss -0.039541  0.000290\n",
            "Iteration 00288    Loss -0.039459  0.000284\n",
            "Iteration 00289    Loss -0.039454  0.000290\n",
            "Iteration 00290    Loss -0.039612  0.000288\n",
            "Iteration 00291    Loss -0.039729  0.000282\n",
            "Iteration 00292    Loss -0.039794  0.000287\n",
            "Iteration 00293    Loss -0.039787  0.000276\n",
            "Iteration 00294    Loss -0.039843  0.000269\n",
            "Iteration 00295    Loss -0.039970  0.000304\n",
            "Iteration 00296    Loss -0.040044  0.000289\n",
            "Iteration 00297    Loss -0.040109  0.000288\n",
            "Iteration 00298    Loss -0.040168  0.000303\n",
            "Iteration 00299    Loss -0.040215  0.000292\n",
            "Iteration 00300    Loss -0.040293  0.000294\n",
            "Iteration 00301    Loss -0.040363  0.000294\n",
            "Iteration 00302    Loss -0.040413  0.000285\n",
            "Iteration 00303    Loss -0.040483  0.000302\n",
            "Iteration 00304    Loss -0.040524  0.000311\n",
            "Iteration 00305    Loss -0.040558  0.000303\n",
            "Iteration 00306    Loss -0.040611  0.000308\n",
            "Iteration 00307    Loss -0.040551  0.000301\n",
            "Iteration 00308    Loss -0.040708  0.000304\n",
            "Iteration 00309    Loss -0.040751  0.000327\n",
            "Iteration 00310    Loss -0.040731  0.000284\n",
            "Iteration 00311    Loss -0.040862  0.000296\n",
            "Iteration 00312    Loss -0.040900  0.000317\n",
            "Iteration 00313    Loss -0.040928  0.000291\n",
            "Iteration 00314    Loss -0.040994  0.000302\n",
            "Iteration 00315    Loss -0.041040  0.000317\n",
            "Iteration 00316    Loss -0.041085  0.000289\n",
            "Iteration 00317    Loss -0.041141  0.000302\n",
            "Iteration 00318    Loss -0.041187  0.000307\n",
            "Iteration 00319    Loss -0.041234  0.000305\n",
            "Iteration 00320    Loss -0.041284  0.000308\n",
            "Iteration 00321    Loss -0.041350  0.000315\n",
            "Iteration 00322    Loss -0.041361  0.000308\n",
            "Iteration 00323    Loss -0.041420  0.000316\n",
            "Iteration 00324    Loss -0.041456  0.000324\n",
            "Iteration 00325    Loss -0.041532  0.000308\n",
            "Iteration 00326    Loss -0.041566  0.000315\n",
            "Iteration 00327    Loss -0.041587  0.000320\n",
            "Iteration 00328    Loss -0.041634  0.000310\n",
            "Iteration 00329    Loss -0.041574  0.000333\n",
            "Iteration 00330    Loss -0.041583  0.000299\n",
            "Iteration 00331    Loss -0.041479  0.000323\n",
            "Iteration 00332    Loss -0.041527  0.000302\n",
            "Iteration 00333    Loss -0.041519  0.000272\n",
            "Iteration 00334    Loss -0.041687  0.000293\n",
            "Iteration 00335    Loss -0.041660  0.000269\n",
            "Iteration 00336    Loss -0.041758  0.000289\n",
            "Iteration 00337    Loss -0.041883  0.000294\n",
            "Iteration 00338    Loss -0.041901  0.000282\n",
            "Iteration 00339    Loss -0.041855  0.000280\n",
            "Iteration 00340    Loss -0.042018  0.000281\n",
            "Iteration 00341    Loss -0.042052  0.000284\n",
            "Iteration 00342    Loss -0.041941  0.000286\n",
            "Iteration 00343    Loss -0.042128  0.000291\n",
            "Iteration 00344    Loss -0.042229  0.000288\n",
            "Iteration 00345    Loss -0.042252  0.000304\n",
            "Iteration 00346    Loss -0.042283  0.000303\n",
            "Iteration 00347    Loss -0.042328  0.000297\n",
            "Iteration 00348    Loss -0.042382  0.000320\n",
            "Iteration 00349    Loss -0.042404  0.000311\n",
            "Iteration 00350    Loss -0.042453  0.000298\n",
            "Iteration 00351    Loss -0.042454  0.000312\n",
            "Iteration 00352    Loss -0.042550  0.000310\n",
            "Iteration 00353    Loss -0.042493  0.000324\n",
            "Iteration 00354    Loss -0.042591  0.000329\n",
            "Iteration 00355    Loss -0.042618  0.000297\n",
            "Iteration 00356    Loss -0.042638  0.000318\n",
            "Iteration 00357    Loss -0.042704  0.000319\n",
            "Iteration 00358    Loss -0.042731  0.000298\n",
            "Iteration 00359    Loss -0.042759  0.000308\n",
            "Iteration 00360    Loss -0.042835  0.000316\n",
            "Iteration 00361    Loss -0.042864  0.000314\n",
            "Iteration 00362    Loss -0.042881  0.000315\n",
            "Iteration 00363    Loss -0.042928  0.000314\n",
            "Iteration 00364    Loss -0.042981  0.000324\n",
            "Iteration 00365    Loss -0.042956  0.000324\n",
            "Iteration 00366    Loss -0.043024  0.000322\n",
            "Iteration 00367    Loss -0.043074  0.000312\n",
            "Iteration 00368    Loss -0.043056  0.000327\n",
            "Iteration 00369    Loss -0.043089  0.000322\n",
            "Iteration 00370    Loss -0.043120  0.000332\n",
            "Iteration 00371    Loss -0.043156  0.000310\n",
            "Iteration 00372    Loss -0.043219  0.000322\n",
            "Iteration 00373    Loss -0.043249  0.000327\n",
            "Iteration 00374    Loss -0.043212  0.000303\n",
            "Iteration 00375    Loss -0.043231  0.000332\n",
            "Iteration 00376    Loss -0.043241  0.000333\n",
            "Iteration 00377    Loss -0.043322  0.000307\n",
            "Iteration 00378    Loss -0.043384  0.000316\n",
            "Iteration 00379    Loss -0.043356  0.000333\n",
            "Iteration 00380    Loss -0.043401  0.000311\n",
            "Iteration 00381    Loss -0.043440  0.000314\n",
            "Iteration 00382    Loss -0.043480  0.000325\n",
            "Iteration 00383    Loss -0.043531  0.000313\n",
            "Iteration 00384    Loss -0.043553  0.000321\n",
            "Iteration 00385    Loss -0.043601  0.000336\n",
            "Iteration 00386    Loss -0.043619  0.000325\n",
            "Iteration 00387    Loss -0.043623  0.000324\n",
            "Iteration 00388    Loss -0.043692  0.000327\n",
            "Iteration 00389    Loss -0.043725  0.000336\n",
            "Iteration 00390    Loss -0.043686  0.000324\n",
            "Iteration 00391    Loss -0.043734  0.000335\n",
            "Iteration 00392    Loss -0.043812  0.000334\n",
            "Iteration 00393    Loss -0.043803  0.000326\n",
            "Iteration 00394    Loss -0.043830  0.000328\n",
            "Iteration 00395    Loss -0.043893  0.000325\n",
            "Iteration 00396    Loss -0.043917  0.000338\n",
            "Iteration 00397    Loss -0.043955  0.000319\n",
            "Iteration 00398    Loss -0.043977  0.000336\n",
            "Iteration 00399    Loss -0.043987  0.000334\n",
            "Iteration 00400    Loss -0.044005  0.000340\n",
            "Iteration 00401    Loss -0.044003  0.000353\n",
            "Iteration 00402    Loss -0.043878  0.000318\n",
            "Iteration 00403    Loss -0.043982  0.000322\n",
            "Iteration 00404    Loss -0.044006  0.000332\n",
            "Iteration 00405    Loss -0.044076  0.000311\n",
            "Iteration 00406    Loss -0.044090  0.000328\n",
            "Iteration 00407    Loss -0.044062  0.000313\n",
            "Iteration 00408    Loss -0.044098  0.000315\n",
            "Iteration 00409    Loss -0.044171  0.000292\n",
            "Iteration 00410    Loss -0.044227  0.000335\n",
            "Iteration 00411    Loss -0.044228  0.000324\n",
            "Iteration 00412    Loss -0.043980  0.000315\n",
            "Iteration 00413    Loss -0.044231  0.000313\n",
            "Iteration 00414    Loss -0.044254  0.000323\n",
            "Iteration 00415    Loss -0.044278  0.000323\n",
            "Iteration 00416    Loss -0.044159  0.000332\n",
            "Iteration 00417    Loss -0.044272  0.000320\n",
            "Iteration 00418    Loss -0.044334  0.000313\n",
            "Iteration 00419    Loss -0.044282  0.000314\n",
            "Iteration 00420    Loss -0.044354  0.000323\n",
            "Iteration 00421    Loss -0.044308  0.000316\n",
            "Iteration 00422    Loss -0.044435  0.000309\n",
            "Iteration 00423    Loss -0.044479  0.000333\n",
            "Iteration 00424    Loss -0.044527  0.000324\n",
            "Iteration 00425    Loss -0.044555  0.000316\n",
            "Iteration 00426    Loss -0.044477  0.000332\n",
            "Iteration 00427    Loss -0.044587  0.000316\n",
            "Iteration 00428    Loss -0.044627  0.000335\n",
            "Iteration 00429    Loss -0.044684  0.000322\n",
            "Iteration 00430    Loss -0.044503  0.000319\n",
            "Iteration 00431    Loss -0.044729  0.000321\n",
            "Iteration 00432    Loss -0.044731  0.000319\n",
            "Iteration 00433    Loss -0.044757  0.000332\n",
            "Iteration 00434    Loss -0.044789  0.000318\n",
            "Iteration 00435    Loss -0.044818  0.000313\n",
            "Iteration 00436    Loss -0.044861  0.000338\n",
            "Iteration 00437    Loss -0.044822  0.000322\n",
            "Iteration 00438    Loss -0.044882  0.000333\n",
            "Iteration 00439    Loss -0.044945  0.000326\n",
            "Iteration 00440    Loss -0.044923  0.000330\n",
            "Iteration 00441    Loss -0.044891  0.000328\n",
            "Iteration 00442    Loss -0.044969  0.000316\n",
            "Iteration 00443    Loss -0.045015  0.000351\n",
            "Iteration 00444    Loss -0.045026  0.000321\n",
            "Iteration 00445    Loss -0.045071  0.000322\n",
            "Iteration 00446    Loss -0.045092  0.000338\n",
            "Iteration 00447    Loss -0.045080  0.000319\n",
            "Iteration 00448    Loss -0.045134  0.000337\n",
            "Iteration 00449    Loss -0.045168  0.000344\n",
            "Iteration 00450    Loss -0.045188  0.000333\n",
            "Iteration 00451    Loss -0.045187  0.000348\n",
            "Iteration 00452    Loss -0.045225  0.000341\n",
            "Iteration 00453    Loss -0.045217  0.000328\n",
            "Iteration 00454    Loss -0.045244  0.000352\n",
            "Iteration 00455    Loss -0.045233  0.000338\n",
            "Iteration 00456    Loss -0.045282  0.000333\n",
            "Iteration 00457    Loss -0.045302  0.000338\n",
            "Iteration 00458    Loss -0.045350  0.000339\n",
            "Iteration 00459    Loss -0.045333  0.000346\n",
            "Iteration 00460    Loss -0.045395  0.000337\n",
            "Iteration 00461    Loss -0.045392  0.000358\n",
            "Iteration 00462    Loss -0.045415  0.000335\n",
            "Iteration 00463    Loss -0.045400  0.000351\n",
            "Iteration 00464    Loss -0.045426  0.000363\n",
            "Iteration 00465    Loss -0.045426  0.000335\n",
            "Iteration 00466    Loss -0.045424  0.000351\n",
            "Iteration 00467    Loss -0.045402  0.000336\n",
            "Iteration 00468    Loss -0.045417  0.000332\n",
            "Iteration 00469    Loss -0.045405  0.000352\n",
            "Iteration 00470    Loss -0.045353  0.000320\n",
            "Iteration 00471    Loss -0.045419  0.000331\n",
            "Iteration 00472    Loss -0.045462  0.000339\n",
            "Iteration 00473    Loss -0.045496  0.000337\n",
            "Iteration 00474    Loss -0.045477  0.000327\n",
            "Iteration 00475    Loss -0.045500  0.000336\n",
            "Iteration 00476    Loss -0.045616  0.000330\n",
            "Iteration 00477    Loss -0.045597  0.000345\n",
            "Iteration 00478    Loss -0.045599  0.000315\n",
            "Iteration 00479    Loss -0.045646  0.000339\n",
            "Iteration 00480    Loss -0.045688  0.000344\n",
            "Iteration 00481    Loss -0.045692  0.000330\n",
            "Iteration 00482    Loss -0.045685  0.000344\n",
            "Iteration 00483    Loss -0.045670  0.000342\n",
            "Iteration 00484    Loss -0.045729  0.000339\n",
            "Iteration 00485    Loss -0.045758  0.000344\n",
            "Iteration 00486    Loss -0.045768  0.000353\n",
            "Iteration 00487    Loss -0.045744  0.000349\n",
            "Iteration 00488    Loss -0.045766  0.000355\n",
            "Iteration 00489    Loss -0.045746  0.000330\n",
            "Iteration 00490    Loss -0.045699  0.000358\n",
            "Iteration 00491    Loss -0.045666  0.000314\n",
            "Iteration 00492    Loss -0.045749  0.000330\n",
            "Iteration 00493    Loss -0.045629  0.000367\n",
            "Iteration 00494    Loss -0.045696  0.000309\n",
            "Iteration 00495    Loss -0.045798  0.000326\n",
            "Iteration 00496    Loss -0.045854  0.000343\n",
            "Iteration 00497    Loss -0.045713  0.000320\n",
            "Iteration 00498    Loss -0.045869  0.000312\n",
            "Iteration 00499    Loss -0.045884  0.000348\n",
            " 98% 491/500 [4:49:32<05:19, 35.51s/it]/content/SOTS/indoor/hazy/1423_6.png\n",
            "Iteration 00000    Loss 0.378759  0.000038\n",
            "Iteration 00001    Loss 24.171659  0.000010\n",
            "Iteration 00002    Loss 0.504409  0.000005\n",
            "Iteration 00003    Loss 0.281939  0.000003\n",
            "Iteration 00004    Loss 5.749301  0.000003\n",
            "Iteration 00005    Loss 0.292259  0.000002\n",
            "Iteration 00006    Loss 0.495901  0.000002\n",
            "Iteration 00007    Loss 0.327953  0.000001\n",
            "Iteration 00008    Loss 0.201116  0.000001\n",
            "Iteration 00009    Loss 0.185660  0.000002\n",
            "Iteration 00010    Loss 0.177721  0.000001\n",
            "Iteration 00011    Loss 0.172128  0.000001\n",
            "Iteration 00012    Loss 0.167594  0.000001\n",
            "Iteration 00013    Loss 0.163462  0.000001\n",
            "Iteration 00014    Loss 0.160203  0.000002\n",
            "Iteration 00015    Loss 0.157211  0.000002\n",
            "Iteration 00016    Loss 0.154348  0.000002\n",
            "Iteration 00017    Loss 0.151669  0.000002\n",
            "Iteration 00018    Loss 0.149114  0.000002\n",
            "Iteration 00019    Loss 0.146476  0.000002\n",
            "Iteration 00020    Loss 0.151532  0.000002\n",
            "Iteration 00021    Loss 0.142160  0.000002\n",
            "Iteration 00022    Loss 0.141033  0.000002\n",
            "Iteration 00023    Loss 0.139345  0.000002\n",
            "Iteration 00024    Loss 0.138033  0.000002\n",
            "Iteration 00025    Loss 0.136446  0.000003\n",
            "Iteration 00026    Loss 0.135029  0.000003\n",
            "Iteration 00027    Loss 0.133062  0.000003\n",
            "Iteration 00028    Loss 0.131339  0.000004\n",
            "Iteration 00029    Loss 0.129541  0.000004\n",
            "Iteration 00030    Loss 0.127805  0.000004\n",
            "Iteration 00031    Loss 0.125961  0.000005\n",
            "Iteration 00032    Loss 0.124139  0.000005\n",
            "Iteration 00033    Loss 0.122510  0.000005\n",
            "Iteration 00034    Loss 0.120776  0.000006\n",
            "Iteration 00035    Loss 0.119139  0.000006\n",
            "Iteration 00036    Loss 0.117596  0.000007\n",
            "Iteration 00037    Loss 0.115763  0.000008\n",
            "Iteration 00038    Loss 0.114410  0.000009\n",
            "Iteration 00039    Loss 0.112983  0.000009\n",
            "Iteration 00040    Loss 0.110926  0.000009\n",
            "Iteration 00041    Loss 0.109481  0.000010\n",
            "Iteration 00042    Loss 0.107836  0.000010\n",
            "Iteration 00043    Loss 0.106308  0.000010\n",
            "Iteration 00044    Loss 0.104667  0.000011\n",
            "Iteration 00045    Loss 0.103312  0.000011\n",
            "Iteration 00046    Loss 0.101570  0.000012\n",
            "Iteration 00047    Loss 0.100201  0.000013\n",
            "Iteration 00048    Loss 0.098808  0.000014\n",
            "Iteration 00049    Loss 0.097523  0.000015\n",
            "Iteration 00050    Loss 0.095832  0.000016\n",
            "Iteration 00051    Loss 0.094411  0.000017\n",
            "Iteration 00052    Loss 0.093116  0.000019\n",
            "Iteration 00053    Loss 0.091878  0.000020\n",
            "Iteration 00054    Loss 0.090459  0.000020\n",
            "Iteration 00055    Loss 0.089227  0.000021\n",
            "Iteration 00056    Loss 0.087877  0.000022\n",
            "Iteration 00057    Loss 0.087085  0.000021\n",
            "Iteration 00058    Loss 0.085478  0.000020\n",
            "Iteration 00059    Loss 0.084505  0.000021\n",
            "Iteration 00060    Loss 0.083548  0.000022\n",
            "Iteration 00061    Loss 0.082800  0.000023\n",
            "Iteration 00062    Loss 0.080892  0.000025\n",
            "Iteration 00063    Loss 0.079673  0.000027\n",
            "Iteration 00064    Loss 0.078570  0.000028\n",
            "Iteration 00065    Loss 0.077402  0.000029\n",
            "Iteration 00066    Loss 0.076139  0.000030\n",
            "Iteration 00067    Loss 0.075143  0.000030\n",
            "Iteration 00068    Loss 0.073774  0.000030\n",
            "Iteration 00069    Loss 0.072626  0.000029\n",
            "Iteration 00070    Loss 0.071335  0.000030\n",
            "Iteration 00071    Loss 0.070295  0.000034\n",
            "Iteration 00072    Loss 0.068983  0.000037\n",
            "Iteration 00073    Loss 0.068040  0.000038\n",
            "Iteration 00074    Loss 0.066968  0.000037\n",
            "Iteration 00075    Loss 0.065822  0.000036\n",
            "Iteration 00076    Loss 0.064750  0.000036\n",
            "Iteration 00077    Loss 0.063657  0.000037\n",
            "Iteration 00078    Loss 0.062685  0.000037\n",
            "Iteration 00079    Loss 0.061516  0.000039\n",
            "Iteration 00080    Loss 0.060529  0.000042\n",
            "Iteration 00081    Loss 0.059745  0.000043\n",
            "Iteration 00082    Loss 0.059385  0.000044\n",
            "Iteration 00083    Loss 0.057813  0.000047\n",
            "Iteration 00084    Loss 0.057329  0.000044\n",
            "Iteration 00085    Loss 0.055764  0.000041\n",
            "Iteration 00086    Loss 0.054950  0.000046\n",
            "Iteration 00087    Loss 0.054032  0.000049\n",
            "Iteration 00088    Loss 0.053194  0.000049\n",
            "Iteration 00089    Loss 0.052152  0.000050\n",
            "Iteration 00090    Loss 0.051305  0.000052\n",
            "Iteration 00091    Loss 0.050589  0.000053\n",
            "Iteration 00092    Loss 0.049323  0.000051\n",
            "Iteration 00093    Loss 0.048374  0.000051\n",
            "Iteration 00094    Loss 0.047425  0.000054\n",
            "Iteration 00095    Loss 0.046659  0.000059\n",
            "Iteration 00096    Loss 0.045583  0.000060\n",
            "Iteration 00097    Loss 0.044734  0.000055\n",
            "Iteration 00098    Loss 0.044082  0.000051\n",
            "Iteration 00099    Loss 0.043195  0.000053\n",
            "Iteration 00100    Loss 0.042297  0.000057\n",
            "Iteration 00101    Loss 0.041547  0.000057\n",
            "Iteration 00102    Loss 0.040646  0.000057\n",
            "Iteration 00103    Loss 0.039701  0.000057\n",
            "Iteration 00104    Loss 0.038946  0.000058\n",
            "Iteration 00105    Loss 0.038075  0.000062\n",
            "Iteration 00106    Loss 0.037349  0.000060\n",
            "Iteration 00107    Loss 0.036548  0.000062\n",
            "Iteration 00108    Loss 0.035645  0.000066\n",
            "Iteration 00109    Loss 0.034933  0.000066\n",
            "Iteration 00110    Loss 0.034329  0.000064\n",
            "Iteration 00111    Loss 0.033523  0.000062\n",
            "Iteration 00112    Loss 0.032753  0.000063\n",
            "Iteration 00113    Loss 0.031991  0.000071\n",
            "Iteration 00114    Loss 0.031206  0.000072\n",
            "Iteration 00115    Loss 0.030518  0.000070\n",
            "Iteration 00116    Loss 0.029954  0.000071\n",
            "Iteration 00117    Loss 0.029133  0.000069\n",
            "Iteration 00118    Loss 0.028518  0.000069\n",
            "Iteration 00119    Loss 0.027828  0.000077\n",
            "Iteration 00120    Loss 0.027125  0.000070\n",
            "Iteration 00121    Loss 0.026558  0.000069\n",
            "Iteration 00122    Loss 0.025865  0.000075\n",
            "Iteration 00123    Loss 0.025158  0.000073\n",
            "Iteration 00124    Loss 0.024602  0.000077\n",
            "Iteration 00125    Loss 0.023966  0.000082\n",
            "Iteration 00126    Loss 0.023219  0.000078\n",
            "Iteration 00127    Loss 0.022848  0.000080\n",
            "Iteration 00128    Loss 0.022138  0.000078\n",
            "Iteration 00129    Loss 0.021483  0.000080\n",
            "Iteration 00130    Loss 0.021011  0.000090\n",
            "Iteration 00131    Loss 0.020293  0.000093\n",
            "Iteration 00132    Loss 0.019683  0.000090\n",
            "Iteration 00133    Loss 0.019299  0.000091\n",
            "Iteration 00134    Loss 0.018794  0.000082\n",
            "Iteration 00135    Loss 0.017939  0.000086\n",
            "Iteration 00136    Loss 0.017436  0.000090\n",
            "Iteration 00137    Loss 0.016900  0.000088\n",
            "Iteration 00138    Loss 0.016369  0.000089\n",
            "Iteration 00139    Loss 0.016041  0.000082\n",
            "Iteration 00140    Loss 0.015288  0.000089\n",
            "Iteration 00141    Loss 0.014931  0.000092\n",
            "Iteration 00142    Loss 0.014262  0.000093\n",
            "Iteration 00143    Loss 0.013912  0.000099\n",
            "Iteration 00144    Loss 0.013413  0.000094\n",
            "Iteration 00145    Loss 0.012974  0.000085\n",
            "Iteration 00146    Loss 0.012409  0.000090\n",
            "Iteration 00147    Loss 0.011951  0.000097\n",
            "Iteration 00148    Loss 0.011453  0.000095\n",
            "Iteration 00149    Loss 0.011301  0.000092\n",
            "Iteration 00150    Loss 0.010399  0.000098\n",
            "Iteration 00151    Loss 0.009987  0.000101\n",
            "Iteration 00152    Loss 0.009710  0.000104\n",
            "Iteration 00153    Loss 0.009116  0.000101\n",
            "Iteration 00154    Loss 0.008618  0.000087\n",
            "Iteration 00155    Loss 0.008271  0.000090\n",
            "Iteration 00156    Loss 0.008292  0.000103\n",
            "Iteration 00157    Loss 0.007225  0.000109\n",
            "Iteration 00158    Loss 0.006864  0.000106\n",
            "Iteration 00159    Loss 0.006693  0.000100\n",
            "Iteration 00160    Loss 0.006482  0.000101\n",
            "Iteration 00161    Loss 0.005568  0.000100\n",
            "Iteration 00162    Loss 0.005155  0.000097\n",
            "Iteration 00163    Loss 0.004913  0.000102\n",
            "Iteration 00164    Loss 0.004353  0.000099\n",
            "Iteration 00165    Loss 0.004062  0.000095\n",
            "Iteration 00166    Loss 0.003756  0.000102\n",
            "Iteration 00167    Loss 0.003089  0.000104\n",
            "Iteration 00168    Loss 0.002773  0.000106\n",
            "Iteration 00169    Loss 0.002353  0.000111\n",
            "Iteration 00170    Loss 0.001948  0.000102\n",
            "Iteration 00171    Loss 0.001420  0.000105\n",
            "Iteration 00172    Loss 0.001105  0.000107\n",
            "Iteration 00173    Loss 0.000830  0.000109\n",
            "Iteration 00174    Loss 0.000536  0.000109\n",
            "Iteration 00175    Loss -0.000012  0.000103\n",
            "Iteration 00176    Loss -0.000297  0.000098\n",
            "Iteration 00177    Loss -0.000556  0.000106\n",
            "Iteration 00178    Loss -0.000903  0.000110\n",
            "Iteration 00179    Loss -0.001305  0.000109\n",
            "Iteration 00180    Loss -0.001641  0.000111\n",
            "Iteration 00181    Loss -0.002280  0.000111\n",
            "Iteration 00182    Loss -0.002579  0.000112\n",
            "Iteration 00183    Loss -0.002854  0.000118\n",
            "Iteration 00184    Loss -0.003087  0.000115\n",
            "Iteration 00185    Loss -0.003402  0.000114\n",
            "Iteration 00186    Loss -0.003945  0.000120\n",
            "Iteration 00187    Loss -0.004108  0.000120\n",
            "Iteration 00188    Loss -0.004575  0.000115\n",
            "Iteration 00189    Loss -0.004822  0.000111\n",
            "Iteration 00190    Loss -0.005212  0.000117\n",
            "Iteration 00191    Loss -0.005555  0.000121\n",
            "Iteration 00192    Loss -0.005640  0.000114\n",
            "Iteration 00193    Loss -0.006016  0.000111\n",
            "Iteration 00194    Loss -0.006492  0.000112\n",
            "Iteration 00195    Loss -0.006804  0.000121\n",
            "Iteration 00196    Loss -0.007149  0.000118\n",
            "Iteration 00197    Loss -0.007483  0.000113\n",
            "Iteration 00198    Loss -0.007865  0.000121\n",
            "Iteration 00199    Loss -0.007950  0.000116\n",
            "Iteration 00200    Loss -0.008457  0.000111\n",
            "Iteration 00201    Loss -0.008808  0.000116\n",
            "Iteration 00202    Loss -0.008990  0.000117\n",
            "Iteration 00203    Loss -0.009395  0.000118\n",
            "Iteration 00204    Loss -0.009663  0.000119\n",
            "Iteration 00205    Loss -0.009615  0.000124\n",
            "Iteration 00206    Loss -0.009870  0.000123\n",
            "Iteration 00207    Loss -0.010395  0.000128\n",
            "Iteration 00208    Loss -0.010597  0.000114\n",
            "Iteration 00209    Loss -0.010552  0.000115\n",
            "Iteration 00210    Loss -0.010867  0.000123\n",
            "Iteration 00211    Loss -0.011328  0.000120\n",
            "Iteration 00212    Loss -0.011755  0.000113\n",
            "Iteration 00213    Loss -0.011619  0.000109\n",
            "Iteration 00214    Loss -0.012212  0.000123\n",
            "Iteration 00215    Loss -0.012371  0.000121\n",
            "Iteration 00216    Loss -0.012814  0.000111\n",
            "Iteration 00217    Loss -0.012738  0.000113\n",
            "Iteration 00218    Loss -0.013104  0.000127\n",
            "Iteration 00219    Loss -0.013304  0.000121\n",
            "Iteration 00220    Loss -0.013754  0.000109\n",
            "Iteration 00221    Loss -0.013955  0.000105\n",
            "Iteration 00222    Loss -0.014378  0.000116\n",
            "Iteration 00223    Loss -0.014641  0.000124\n",
            "Iteration 00224    Loss -0.014783  0.000116\n",
            "Iteration 00225    Loss -0.014967  0.000110\n",
            "Iteration 00226    Loss -0.015054  0.000120\n",
            "Iteration 00227    Loss -0.015285  0.000126\n",
            "Iteration 00228    Loss -0.015681  0.000121\n",
            "Iteration 00229    Loss -0.015959  0.000116\n",
            "Iteration 00230    Loss -0.016118  0.000122\n",
            "Iteration 00231    Loss -0.016308  0.000124\n",
            "Iteration 00232    Loss -0.016446  0.000116\n",
            "Iteration 00233    Loss -0.016989  0.000107\n",
            "Iteration 00234    Loss -0.017126  0.000115\n",
            "Iteration 00235    Loss -0.016944  0.000127\n",
            "Iteration 00236    Loss -0.017447  0.000129\n",
            "Iteration 00237    Loss -0.017892  0.000117\n",
            "Iteration 00238    Loss -0.017326  0.000104\n",
            "Iteration 00239    Loss -0.018276  0.000109\n",
            "Iteration 00240    Loss -0.018067  0.000124\n",
            "Iteration 00241    Loss -0.018616  0.000128\n",
            "Iteration 00242    Loss -0.018679  0.000121\n",
            "Iteration 00243    Loss -0.019098  0.000116\n",
            "Iteration 00244    Loss -0.018951  0.000122\n",
            "Iteration 00245    Loss -0.019377  0.000131\n",
            "Iteration 00246    Loss -0.019730  0.000132\n",
            "Iteration 00247    Loss -0.019593  0.000131\n",
            "Iteration 00248    Loss -0.019994  0.000132\n",
            "Iteration 00249    Loss -0.020203  0.000126\n",
            "Iteration 00250    Loss -0.020406  0.000129\n",
            "Iteration 00251    Loss -0.020416  0.000126\n",
            "Iteration 00252    Loss -0.020695  0.000121\n",
            "Iteration 00253    Loss -0.020196  0.000125\n",
            "Iteration 00254    Loss -0.020873  0.000133\n",
            "Iteration 00255    Loss -0.021317  0.000118\n",
            "Iteration 00256    Loss -0.021580  0.000113\n",
            "Iteration 00257    Loss -0.021261  0.000122\n",
            "Iteration 00258    Loss -0.021711  0.000135\n",
            "Iteration 00259    Loss -0.022158  0.000141\n",
            "Iteration 00260    Loss -0.022014  0.000129\n",
            "Iteration 00261    Loss -0.022216  0.000117\n",
            "Iteration 00262    Loss -0.022442  0.000121\n",
            "Iteration 00263    Loss -0.022364  0.000128\n",
            "Iteration 00264    Loss -0.022574  0.000129\n",
            "Iteration 00265    Loss -0.022751  0.000125\n",
            "Iteration 00266    Loss -0.022295  0.000125\n",
            "Iteration 00267    Loss -0.023152  0.000131\n",
            "Iteration 00268    Loss -0.023341  0.000136\n",
            "Iteration 00269    Loss -0.023127  0.000134\n",
            "Iteration 00270    Loss -0.023363  0.000131\n",
            "Iteration 00271    Loss -0.023450  0.000131\n",
            "Iteration 00272    Loss -0.023742  0.000131\n",
            "Iteration 00273    Loss -0.023905  0.000133\n",
            "Iteration 00274    Loss -0.023313  0.000141\n",
            "Iteration 00275    Loss -0.024410  0.000143\n",
            "Iteration 00276    Loss -0.024114  0.000135\n",
            "Iteration 00277    Loss -0.024604  0.000132\n",
            "Iteration 00278    Loss -0.025073  0.000135\n",
            "Iteration 00279    Loss -0.024324  0.000135\n",
            "Iteration 00280    Loss -0.024563  0.000139\n",
            "Iteration 00281    Loss -0.025411  0.000144\n",
            "Iteration 00282    Loss -0.025321  0.000132\n",
            "Iteration 00283    Loss -0.025600  0.000128\n",
            "Iteration 00284    Loss -0.025633  0.000139\n",
            "Iteration 00285    Loss -0.025845  0.000148\n",
            "Iteration 00286    Loss -0.025912  0.000139\n",
            "Iteration 00287    Loss -0.026218  0.000128\n",
            "Iteration 00288    Loss -0.025856  0.000138\n",
            "Iteration 00289    Loss -0.025617  0.000147\n",
            "Iteration 00290    Loss -0.025880  0.000139\n",
            "Iteration 00291    Loss -0.026794  0.000136\n",
            "Iteration 00292    Loss -0.027067  0.000144\n",
            "Iteration 00293    Loss -0.026769  0.000146\n",
            "Iteration 00294    Loss -0.026792  0.000136\n",
            "Iteration 00295    Loss -0.027063  0.000137\n",
            "Iteration 00296    Loss -0.027007  0.000132\n",
            "Iteration 00297    Loss -0.027438  0.000131\n",
            "Iteration 00298    Loss -0.027341  0.000146\n",
            "Iteration 00299    Loss -0.027942  0.000157\n",
            "Iteration 00300    Loss -0.027291  0.000145\n",
            "Iteration 00301    Loss -0.027479  0.000149\n",
            "Iteration 00302    Loss -0.027567  0.000150\n",
            "Iteration 00303    Loss -0.028115  0.000147\n",
            "Iteration 00304    Loss -0.028377  0.000143\n",
            "Iteration 00305    Loss -0.027946  0.000141\n",
            "Iteration 00306    Loss -0.028550  0.000143\n",
            "Iteration 00307    Loss -0.028475  0.000148\n",
            "Iteration 00308    Loss -0.028635  0.000134\n",
            "Iteration 00309    Loss -0.028942  0.000142\n",
            "Iteration 00310    Loss -0.029047  0.000160\n",
            "Iteration 00311    Loss -0.028823  0.000154\n",
            "Iteration 00312    Loss -0.028698  0.000144\n",
            "Iteration 00313    Loss -0.029289  0.000144\n",
            "Iteration 00314    Loss -0.029429  0.000142\n",
            "Iteration 00315    Loss -0.029578  0.000125\n",
            "Iteration 00316    Loss -0.029806  0.000121\n",
            "Iteration 00317    Loss -0.029686  0.000138\n",
            "Iteration 00318    Loss -0.029429  0.000164\n",
            "Iteration 00319    Loss -0.029865  0.000155\n",
            "Iteration 00320    Loss -0.030135  0.000132\n",
            "Iteration 00321    Loss -0.030414  0.000129\n",
            "Iteration 00322    Loss -0.029699  0.000136\n",
            "Iteration 00323    Loss -0.030420  0.000147\n",
            "Iteration 00324    Loss -0.029600  0.000149\n",
            "Iteration 00325    Loss -0.029746  0.000141\n",
            "Iteration 00326    Loss -0.030813  0.000143\n",
            "Iteration 00327    Loss -0.030648  0.000139\n",
            "Iteration 00328    Loss -0.031023  0.000141\n",
            "Iteration 00329    Loss -0.031067  0.000146\n",
            "Iteration 00330    Loss -0.031070  0.000150\n",
            "Iteration 00331    Loss -0.031342  0.000144\n",
            "Iteration 00332    Loss -0.031071  0.000140\n",
            "Iteration 00333    Loss -0.031348  0.000143\n",
            "Iteration 00334    Loss -0.031091  0.000143\n",
            "Iteration 00335    Loss -0.031394  0.000149\n",
            "Iteration 00336    Loss -0.031627  0.000155\n",
            "Iteration 00337    Loss -0.031809  0.000148\n",
            "Iteration 00338    Loss -0.031572  0.000145\n",
            "Iteration 00339    Loss -0.030758  0.000147\n",
            "Iteration 00340    Loss -0.031882  0.000157\n",
            "Iteration 00341    Loss -0.031902  0.000139\n",
            "Iteration 00342    Loss -0.031751  0.000139\n",
            "Iteration 00343    Loss -0.032283  0.000147\n",
            "Iteration 00344    Loss -0.032083  0.000145\n",
            "Iteration 00345    Loss -0.032424  0.000135\n",
            "Iteration 00346    Loss -0.032009  0.000126\n",
            "Iteration 00347    Loss -0.032549  0.000135\n",
            "Iteration 00348    Loss -0.032684  0.000145\n",
            "Iteration 00349    Loss -0.032596  0.000142\n",
            "Iteration 00350    Loss -0.032660  0.000143\n",
            "Iteration 00351    Loss -0.033020  0.000145\n",
            "Iteration 00352    Loss -0.032921  0.000135\n",
            "Iteration 00353    Loss -0.032878  0.000137\n",
            "Iteration 00354    Loss -0.033115  0.000143\n",
            "Iteration 00355    Loss -0.033268  0.000137\n",
            "Iteration 00356    Loss -0.033431  0.000134\n",
            "Iteration 00357    Loss -0.033487  0.000140\n",
            "Iteration 00358    Loss -0.033540  0.000145\n",
            "Iteration 00359    Loss -0.033626  0.000140\n",
            "Iteration 00360    Loss -0.033270  0.000136\n",
            "Iteration 00361    Loss -0.033839  0.000141\n",
            "Iteration 00362    Loss -0.033279  0.000141\n",
            "Iteration 00363    Loss -0.033371  0.000140\n",
            "Iteration 00364    Loss -0.033889  0.000137\n",
            "Iteration 00365    Loss -0.034110  0.000137\n",
            "Iteration 00366    Loss -0.034019  0.000140\n",
            "Iteration 00367    Loss -0.033775  0.000145\n",
            "Iteration 00368    Loss -0.034114  0.000148\n",
            "Iteration 00369    Loss -0.034039  0.000139\n",
            "Iteration 00370    Loss -0.034111  0.000145\n",
            "Iteration 00371    Loss -0.034402  0.000149\n",
            "Iteration 00372    Loss -0.034520  0.000135\n",
            "Iteration 00373    Loss -0.034361  0.000134\n",
            "Iteration 00374    Loss -0.034552  0.000145\n",
            "Iteration 00375    Loss -0.034512  0.000150\n",
            "Iteration 00376    Loss -0.034797  0.000143\n",
            "Iteration 00377    Loss -0.034870  0.000132\n",
            "Iteration 00378    Loss -0.034920  0.000129\n",
            "Iteration 00379    Loss -0.034984  0.000138\n",
            "Iteration 00380    Loss -0.035095  0.000144\n",
            "Iteration 00381    Loss -0.035203  0.000144\n",
            "Iteration 00382    Loss -0.034792  0.000140\n",
            "Iteration 00383    Loss -0.034716  0.000137\n",
            "Iteration 00384    Loss -0.035331  0.000134\n",
            "Iteration 00385    Loss -0.035251  0.000141\n",
            "Iteration 00386    Loss -0.035403  0.000148\n",
            "Iteration 00387    Loss -0.035401  0.000147\n",
            "Iteration 00388    Loss -0.035532  0.000146\n",
            "Iteration 00389    Loss -0.035233  0.000145\n",
            "Iteration 00390    Loss -0.035554  0.000147\n",
            "Iteration 00391    Loss -0.035476  0.000140\n",
            "Iteration 00392    Loss -0.035772  0.000135\n",
            "Iteration 00393    Loss -0.035930  0.000143\n",
            "Iteration 00394    Loss -0.035864  0.000150\n",
            "Iteration 00395    Loss -0.035898  0.000147\n",
            "Iteration 00396    Loss -0.036123  0.000140\n",
            "Iteration 00397    Loss -0.036107  0.000140\n",
            "Iteration 00398    Loss -0.036178  0.000148\n",
            "Iteration 00399    Loss -0.036277  0.000153\n",
            "Iteration 00400    Loss -0.036202  0.000150\n",
            "Iteration 00401    Loss -0.036309  0.000144\n",
            "Iteration 00402    Loss -0.035975  0.000144\n",
            "Iteration 00403    Loss -0.036463  0.000145\n",
            "Iteration 00404    Loss -0.036606  0.000140\n",
            "Iteration 00405    Loss -0.036444  0.000140\n",
            "Iteration 00406    Loss -0.036621  0.000146\n",
            "Iteration 00407    Loss -0.036709  0.000150\n",
            "Iteration 00408    Loss -0.036780  0.000147\n",
            "Iteration 00409    Loss -0.036805  0.000141\n",
            "Iteration 00410    Loss -0.036934  0.000138\n",
            "Iteration 00411    Loss -0.036672  0.000150\n",
            "Iteration 00412    Loss -0.036920  0.000155\n",
            "Iteration 00413    Loss -0.037014  0.000154\n",
            "Iteration 00414    Loss -0.036649  0.000143\n",
            "Iteration 00415    Loss -0.036910  0.000140\n",
            "Iteration 00416    Loss -0.037212  0.000142\n",
            "Iteration 00417    Loss -0.036976  0.000141\n",
            "Iteration 00418    Loss -0.037061  0.000151\n",
            "Iteration 00419    Loss -0.037145  0.000152\n",
            "Iteration 00420    Loss -0.037160  0.000150\n",
            "Iteration 00421    Loss -0.036937  0.000151\n",
            "Iteration 00422    Loss -0.037298  0.000139\n",
            "Iteration 00423    Loss -0.037306  0.000130\n",
            "Iteration 00424    Loss -0.037491  0.000141\n",
            "Iteration 00425    Loss -0.037607  0.000154\n",
            "Iteration 00426    Loss -0.037477  0.000151\n",
            "Iteration 00427    Loss -0.037677  0.000144\n",
            "Iteration 00428    Loss -0.037720  0.000139\n",
            "Iteration 00429    Loss -0.037842  0.000145\n",
            "Iteration 00430    Loss -0.037625  0.000151\n",
            "Iteration 00431    Loss -0.037970  0.000148\n",
            "Iteration 00432    Loss -0.037765  0.000142\n",
            "Iteration 00433    Loss -0.037876  0.000149\n",
            "Iteration 00434    Loss -0.038076  0.000153\n",
            "Iteration 00435    Loss -0.038085  0.000150\n",
            "Iteration 00436    Loss -0.038056  0.000148\n",
            "Iteration 00437    Loss -0.038176  0.000147\n",
            "Iteration 00438    Loss -0.038096  0.000143\n",
            "Iteration 00439    Loss -0.038283  0.000149\n",
            "Iteration 00440    Loss -0.038313  0.000146\n",
            "Iteration 00441    Loss -0.038380  0.000142\n",
            "Iteration 00442    Loss -0.038371  0.000141\n",
            "Iteration 00443    Loss -0.038380  0.000147\n",
            "Iteration 00444    Loss -0.038546  0.000153\n",
            "Iteration 00445    Loss -0.038509  0.000150\n",
            "Iteration 00446    Loss -0.038518  0.000140\n",
            "Iteration 00447    Loss -0.038416  0.000138\n",
            "Iteration 00448    Loss -0.038677  0.000144\n",
            "Iteration 00449    Loss -0.038583  0.000145\n",
            "Iteration 00450    Loss -0.038818  0.000149\n",
            "Iteration 00451    Loss -0.038790  0.000147\n",
            "Iteration 00452    Loss -0.038750  0.000146\n",
            "Iteration 00453    Loss -0.038794  0.000153\n",
            "Iteration 00454    Loss -0.038785  0.000140\n",
            "Iteration 00455    Loss -0.038850  0.000140\n",
            "Iteration 00456    Loss -0.038807  0.000151\n",
            "Iteration 00457    Loss -0.038989  0.000148\n",
            "Iteration 00458    Loss -0.039105  0.000145\n",
            "Iteration 00459    Loss -0.039048  0.000145\n",
            "Iteration 00460    Loss -0.039077  0.000145\n",
            "Iteration 00461    Loss -0.039136  0.000144\n",
            "Iteration 00462    Loss -0.039186  0.000141\n",
            "Iteration 00463    Loss -0.039230  0.000131\n",
            "Iteration 00464    Loss -0.039248  0.000141\n",
            "Iteration 00465    Loss -0.039037  0.000148\n",
            "Iteration 00466    Loss -0.039348  0.000143\n",
            "Iteration 00467    Loss -0.039321  0.000134\n",
            "Iteration 00468    Loss -0.039432  0.000139\n",
            "Iteration 00469    Loss -0.039099  0.000150\n",
            "Iteration 00470    Loss -0.039538  0.000156\n",
            "Iteration 00471    Loss -0.039574  0.000149\n",
            "Iteration 00472    Loss -0.039601  0.000139\n",
            "Iteration 00473    Loss -0.039628  0.000140\n",
            "Iteration 00474    Loss -0.039551  0.000152\n",
            "Iteration 00475    Loss -0.039725  0.000151\n",
            "Iteration 00476    Loss -0.039749  0.000146\n",
            "Iteration 00477    Loss -0.039685  0.000140\n",
            "Iteration 00478    Loss -0.039558  0.000142\n",
            "Iteration 00479    Loss -0.039614  0.000144\n",
            "Iteration 00480    Loss -0.039731  0.000152\n",
            "Iteration 00481    Loss -0.039918  0.000149\n",
            "Iteration 00482    Loss -0.039710  0.000144\n",
            "Iteration 00483    Loss -0.039884  0.000145\n",
            "Iteration 00484    Loss -0.039920  0.000141\n",
            "Iteration 00485    Loss -0.039862  0.000138\n",
            "Iteration 00486    Loss -0.039957  0.000142\n",
            "Iteration 00487    Loss -0.040045  0.000138\n",
            "Iteration 00488    Loss -0.039998  0.000134\n",
            "Iteration 00489    Loss -0.040125  0.000139\n",
            "Iteration 00490    Loss -0.040157  0.000144\n",
            "Iteration 00491    Loss -0.040248  0.000135\n",
            "Iteration 00492    Loss -0.040309  0.000131\n",
            "Iteration 00493    Loss -0.040300  0.000139\n",
            "Iteration 00494    Loss -0.040179  0.000148\n",
            "Iteration 00495    Loss -0.040408  0.000140\n",
            "Iteration 00496    Loss -0.040450  0.000130\n",
            "Iteration 00497    Loss -0.040435  0.000133\n",
            "Iteration 00498    Loss -0.040455  0.000146\n",
            "Iteration 00499    Loss -0.040567  0.000147\n",
            " 98% 492/500 [4:50:08<04:44, 35.53s/it]/content/SOTS/indoor/hazy/1440_5.png\n",
            "Iteration 00000    Loss 0.534371  0.000030\n",
            "Iteration 00001    Loss 20.873167  0.000014\n",
            "Iteration 00002    Loss 0.421566  0.000005\n",
            "Iteration 00003    Loss 0.425155  0.000004\n",
            "Iteration 00004    Loss 0.323541  0.000002\n",
            "Iteration 00005    Loss 0.284263  0.000002\n",
            "Iteration 00006    Loss 0.250631  0.000002\n",
            "Iteration 00007    Loss 0.223294  0.000002\n",
            "Iteration 00008    Loss 0.198501  0.000002\n",
            "Iteration 00009    Loss 0.180103  0.000003\n",
            "Iteration 00010    Loss 0.162266  0.000002\n",
            "Iteration 00011    Loss 0.147631  0.000003\n",
            "Iteration 00012    Loss 0.136640  0.000004\n",
            "Iteration 00013    Loss 0.126905  0.000006\n",
            "Iteration 00014    Loss 0.119206  0.000007\n",
            "Iteration 00015    Loss 0.114046  0.000008\n",
            "Iteration 00016    Loss 0.108198  0.000009\n",
            "Iteration 00017    Loss 0.103831  0.000009\n",
            "Iteration 00018    Loss 0.100130  0.000010\n",
            "Iteration 00019    Loss 0.096995  0.000012\n",
            "Iteration 00020    Loss 0.093993  0.000012\n",
            "Iteration 00021    Loss 0.091436  0.000013\n",
            "Iteration 00022    Loss 0.088593  0.000014\n",
            "Iteration 00023    Loss 0.086549  0.000015\n",
            "Iteration 00024    Loss 0.084569  0.000015\n",
            "Iteration 00025    Loss 0.082700  0.000017\n",
            "Iteration 00026    Loss 0.080729  0.000019\n",
            "Iteration 00027    Loss 0.079212  0.000022\n",
            "Iteration 00028    Loss 0.077365  0.000022\n",
            "Iteration 00029    Loss 0.075909  0.000024\n",
            "Iteration 00030    Loss 0.074300  0.000025\n",
            "Iteration 00031    Loss 0.072991  0.000027\n",
            "Iteration 00032    Loss 0.071426  0.000029\n",
            "Iteration 00033    Loss 0.070139  0.000030\n",
            "Iteration 00034    Loss 0.068875  0.000033\n",
            "Iteration 00035    Loss 0.067673  0.000034\n",
            "Iteration 00036    Loss 0.066425  0.000034\n",
            "Iteration 00037    Loss 0.065459  0.000034\n",
            "Iteration 00038    Loss 0.064146  0.000037\n",
            "Iteration 00039    Loss 0.063184  0.000040\n",
            "Iteration 00040    Loss 0.062058  0.000041\n",
            "Iteration 00041    Loss 0.060982  0.000043\n",
            "Iteration 00042    Loss 0.059963  0.000045\n",
            "Iteration 00043    Loss 0.058909  0.000049\n",
            "Iteration 00044    Loss 0.058007  0.000051\n",
            "Iteration 00045    Loss 0.057130  0.000054\n",
            "Iteration 00046    Loss 0.056442  0.000057\n",
            "Iteration 00047    Loss 0.055530  0.000060\n",
            "Iteration 00048    Loss 0.054472  0.000059\n",
            "Iteration 00049    Loss 0.053472  0.000063\n",
            "Iteration 00050    Loss 0.052526  0.000067\n",
            "Iteration 00051    Loss 0.051641  0.000070\n",
            "Iteration 00052    Loss 0.050781  0.000073\n",
            "Iteration 00053    Loss 0.049914  0.000074\n",
            "Iteration 00054    Loss 0.048997  0.000079\n",
            "Iteration 00055    Loss 0.048120  0.000078\n",
            "Iteration 00056    Loss 0.047314  0.000079\n",
            "Iteration 00057    Loss 0.046499  0.000089\n",
            "Iteration 00058    Loss 0.045601  0.000090\n",
            "Iteration 00059    Loss 0.044831  0.000087\n",
            "Iteration 00060    Loss 0.043990  0.000092\n",
            "Iteration 00061    Loss 0.043167  0.000097\n",
            "Iteration 00062    Loss 0.042356  0.000098\n",
            "Iteration 00063    Loss 0.041865  0.000102\n",
            "Iteration 00064    Loss 0.041161  0.000105\n",
            "Iteration 00065    Loss 0.040526  0.000109\n",
            "Iteration 00066    Loss 0.039701  0.000112\n",
            "Iteration 00067    Loss 0.038943  0.000114\n",
            "Iteration 00068    Loss 0.038150  0.000119\n",
            "Iteration 00069    Loss 0.037438  0.000124\n",
            "Iteration 00070    Loss 0.036716  0.000128\n",
            "Iteration 00071    Loss 0.036053  0.000133\n",
            "Iteration 00072    Loss 0.035396  0.000133\n",
            "Iteration 00073    Loss 0.034681  0.000134\n",
            "Iteration 00074    Loss 0.033984  0.000140\n",
            "Iteration 00075    Loss 0.033268  0.000142\n",
            "Iteration 00076    Loss 0.032597  0.000145\n",
            "Iteration 00077    Loss 0.031870  0.000149\n",
            "Iteration 00078    Loss 0.031255  0.000151\n",
            "Iteration 00079    Loss 0.030636  0.000153\n",
            "Iteration 00080    Loss 0.029992  0.000152\n",
            "Iteration 00081    Loss 0.029400  0.000152\n",
            "Iteration 00082    Loss 0.028834  0.000156\n",
            "Iteration 00083    Loss 0.028134  0.000159\n",
            "Iteration 00084    Loss 0.027664  0.000159\n",
            "Iteration 00085    Loss 0.027186  0.000162\n",
            "Iteration 00086    Loss 0.026663  0.000149\n",
            "Iteration 00087    Loss 0.025830  0.000161\n",
            "Iteration 00088    Loss 0.025284  0.000148\n",
            "Iteration 00089    Loss 0.024737  0.000141\n",
            "Iteration 00090    Loss 0.024034  0.000142\n",
            "Iteration 00091    Loss 0.023536  0.000145\n",
            "Iteration 00092    Loss 0.022911  0.000155\n",
            "Iteration 00093    Loss 0.022367  0.000155\n",
            "Iteration 00094    Loss 0.021766  0.000157\n",
            "Iteration 00095    Loss 0.021223  0.000165\n",
            "Iteration 00096    Loss 0.020823  0.000171\n",
            "Iteration 00097    Loss 0.020265  0.000163\n",
            "Iteration 00098    Loss 0.019718  0.000159\n",
            "Iteration 00099    Loss 0.019169  0.000164\n",
            "Iteration 00100    Loss 0.018643  0.000169\n",
            "Iteration 00101    Loss 0.018120  0.000171\n",
            "Iteration 00102    Loss 0.017664  0.000172\n",
            "Iteration 00103    Loss 0.017011  0.000176\n",
            "Iteration 00104    Loss 0.016565  0.000181\n",
            "Iteration 00105    Loss 0.016024  0.000183\n",
            "Iteration 00106    Loss 0.015513  0.000182\n",
            "Iteration 00107    Loss 0.015053  0.000185\n",
            "Iteration 00108    Loss 0.014545  0.000191\n",
            "Iteration 00109    Loss 0.014066  0.000191\n",
            "Iteration 00110    Loss 0.013550  0.000191\n",
            "Iteration 00111    Loss 0.013084  0.000193\n",
            "Iteration 00112    Loss 0.012702  0.000199\n",
            "Iteration 00113    Loss 0.012164  0.000200\n",
            "Iteration 00114    Loss 0.011725  0.000196\n",
            "Iteration 00115    Loss 0.011244  0.000198\n",
            "Iteration 00116    Loss 0.010788  0.000199\n",
            "Iteration 00117    Loss 0.010385  0.000205\n",
            "Iteration 00118    Loss 0.009869  0.000210\n",
            "Iteration 00119    Loss 0.009481  0.000209\n",
            "Iteration 00120    Loss 0.009041  0.000210\n",
            "Iteration 00121    Loss 0.008619  0.000219\n",
            "Iteration 00122    Loss 0.008183  0.000218\n",
            "Iteration 00123    Loss 0.007852  0.000230\n",
            "Iteration 00124    Loss 0.007425  0.000217\n",
            "Iteration 00125    Loss 0.007098  0.000222\n",
            "Iteration 00126    Loss 0.006769  0.000209\n",
            "Iteration 00127    Loss 0.006317  0.000203\n",
            "Iteration 00128    Loss 0.005862  0.000198\n",
            "Iteration 00129    Loss 0.005524  0.000195\n",
            "Iteration 00130    Loss 0.005054  0.000207\n",
            "Iteration 00131    Loss 0.004671  0.000210\n",
            "Iteration 00132    Loss 0.004306  0.000214\n",
            "Iteration 00133    Loss 0.003902  0.000222\n",
            "Iteration 00134    Loss 0.003584  0.000215\n",
            "Iteration 00135    Loss 0.003204  0.000212\n",
            "Iteration 00136    Loss 0.002798  0.000221\n",
            "Iteration 00137    Loss 0.002418  0.000219\n",
            "Iteration 00138    Loss 0.002061  0.000216\n",
            "Iteration 00139    Loss 0.001706  0.000222\n",
            "Iteration 00140    Loss 0.001419  0.000228\n",
            "Iteration 00141    Loss 0.001104  0.000235\n",
            "Iteration 00142    Loss 0.000692  0.000235\n",
            "Iteration 00143    Loss 0.000400  0.000236\n",
            "Iteration 00144    Loss 0.000038  0.000240\n",
            "Iteration 00145    Loss -0.000309  0.000236\n",
            "Iteration 00146    Loss -0.000668  0.000236\n",
            "Iteration 00147    Loss -0.000971  0.000237\n",
            "Iteration 00148    Loss -0.001183  0.000241\n",
            "Iteration 00149    Loss -0.001539  0.000245\n",
            "Iteration 00150    Loss -0.001781  0.000241\n",
            "Iteration 00151    Loss -0.002066  0.000244\n",
            "Iteration 00152    Loss -0.002425  0.000243\n",
            "Iteration 00153    Loss -0.002752  0.000248\n",
            "Iteration 00154    Loss -0.003036  0.000246\n",
            "Iteration 00155    Loss -0.003421  0.000243\n",
            "Iteration 00156    Loss -0.003705  0.000245\n",
            "Iteration 00157    Loss -0.004005  0.000248\n",
            "Iteration 00158    Loss -0.004273  0.000245\n",
            "Iteration 00159    Loss -0.004550  0.000240\n",
            "Iteration 00160    Loss -0.004732  0.000253\n",
            "Iteration 00161    Loss -0.005111  0.000236\n",
            "Iteration 00162    Loss -0.005417  0.000248\n",
            "Iteration 00163    Loss -0.005726  0.000251\n",
            "Iteration 00164    Loss -0.005991  0.000252\n",
            "Iteration 00165    Loss -0.006241  0.000253\n",
            "Iteration 00166    Loss -0.006541  0.000247\n",
            "Iteration 00167    Loss -0.006863  0.000246\n",
            "Iteration 00168    Loss -0.007180  0.000246\n",
            "Iteration 00169    Loss -0.007461  0.000247\n",
            "Iteration 00170    Loss -0.007692  0.000251\n",
            "Iteration 00171    Loss -0.007980  0.000252\n",
            "Iteration 00172    Loss -0.008246  0.000257\n",
            "Iteration 00173    Loss -0.008531  0.000255\n",
            "Iteration 00174    Loss -0.008772  0.000244\n",
            "Iteration 00175    Loss -0.009032  0.000249\n",
            "Iteration 00176    Loss -0.009288  0.000248\n",
            "Iteration 00177    Loss -0.009546  0.000248\n",
            "Iteration 00178    Loss -0.009766  0.000255\n",
            "Iteration 00179    Loss -0.009981  0.000258\n",
            "Iteration 00180    Loss -0.010248  0.000252\n",
            "Iteration 00181    Loss -0.010544  0.000258\n",
            "Iteration 00182    Loss -0.010778  0.000253\n",
            "Iteration 00183    Loss -0.011032  0.000249\n",
            "Iteration 00184    Loss -0.011263  0.000254\n",
            "Iteration 00185    Loss -0.011489  0.000252\n",
            "Iteration 00186    Loss -0.011748  0.000244\n",
            "Iteration 00187    Loss -0.011955  0.000251\n",
            "Iteration 00188    Loss -0.012176  0.000265\n",
            "Iteration 00189    Loss -0.012414  0.000270\n",
            "Iteration 00190    Loss -0.012611  0.000269\n",
            "Iteration 00191    Loss -0.012832  0.000269\n",
            "Iteration 00192    Loss -0.013048  0.000264\n",
            "Iteration 00193    Loss -0.013215  0.000267\n",
            "Iteration 00194    Loss -0.013436  0.000269\n",
            "Iteration 00195    Loss -0.013571  0.000262\n",
            "Iteration 00196    Loss -0.013621  0.000249\n",
            "Iteration 00197    Loss -0.013915  0.000264\n",
            "Iteration 00198    Loss -0.014185  0.000240\n",
            "Iteration 00199    Loss -0.014484  0.000243\n",
            "Iteration 00200    Loss -0.014646  0.000254\n",
            "Iteration 00201    Loss -0.014847  0.000248\n",
            "Iteration 00202    Loss -0.015058  0.000240\n",
            "Iteration 00203    Loss -0.015294  0.000239\n",
            "Iteration 00204    Loss -0.015476  0.000255\n",
            "Iteration 00205    Loss -0.015688  0.000261\n",
            "Iteration 00206    Loss -0.015867  0.000256\n",
            "Iteration 00207    Loss -0.016038  0.000253\n",
            "Iteration 00208    Loss -0.016211  0.000258\n",
            "Iteration 00209    Loss -0.016425  0.000263\n",
            "Iteration 00210    Loss -0.016601  0.000258\n",
            "Iteration 00211    Loss -0.016781  0.000255\n",
            "Iteration 00212    Loss -0.017016  0.000260\n",
            "Iteration 00213    Loss -0.017159  0.000271\n",
            "Iteration 00214    Loss -0.017337  0.000272\n",
            "Iteration 00215    Loss -0.017538  0.000261\n",
            "Iteration 00216    Loss -0.017653  0.000260\n",
            "Iteration 00217    Loss -0.017837  0.000270\n",
            "Iteration 00218    Loss -0.018035  0.000271\n",
            "Iteration 00219    Loss -0.018227  0.000266\n",
            "Iteration 00220    Loss -0.018384  0.000266\n",
            "Iteration 00221    Loss -0.018539  0.000267\n",
            "Iteration 00222    Loss -0.018722  0.000272\n",
            "Iteration 00223    Loss -0.018885  0.000274\n",
            "Iteration 00224    Loss -0.019052  0.000269\n",
            "Iteration 00225    Loss -0.019246  0.000268\n",
            "Iteration 00226    Loss -0.019413  0.000274\n",
            "Iteration 00227    Loss -0.019540  0.000270\n",
            "Iteration 00228    Loss -0.019734  0.000267\n",
            "Iteration 00229    Loss -0.019909  0.000270\n",
            "Iteration 00230    Loss -0.020041  0.000273\n",
            "Iteration 00231    Loss -0.020181  0.000266\n",
            "Iteration 00232    Loss -0.020326  0.000262\n",
            "Iteration 00233    Loss -0.020485  0.000270\n",
            "Iteration 00234    Loss -0.020679  0.000270\n",
            "Iteration 00235    Loss -0.020807  0.000268\n",
            "Iteration 00236    Loss -0.020960  0.000265\n",
            "Iteration 00237    Loss -0.021129  0.000267\n",
            "Iteration 00238    Loss -0.021255  0.000267\n",
            "Iteration 00239    Loss -0.021441  0.000264\n",
            "Iteration 00240    Loss -0.021586  0.000267\n",
            "Iteration 00241    Loss -0.021689  0.000272\n",
            "Iteration 00242    Loss -0.021864  0.000266\n",
            "Iteration 00243    Loss -0.022025  0.000266\n",
            "Iteration 00244    Loss -0.022133  0.000275\n",
            "Iteration 00245    Loss -0.022315  0.000274\n",
            "Iteration 00246    Loss -0.022443  0.000268\n",
            "Iteration 00247    Loss -0.022605  0.000271\n",
            "Iteration 00248    Loss -0.022751  0.000274\n",
            "Iteration 00249    Loss -0.022893  0.000264\n",
            "Iteration 00250    Loss -0.023052  0.000266\n",
            "Iteration 00251    Loss -0.023146  0.000275\n",
            "Iteration 00252    Loss -0.023275  0.000275\n",
            "Iteration 00253    Loss -0.023386  0.000268\n",
            "Iteration 00254    Loss -0.023486  0.000278\n",
            "Iteration 00255    Loss -0.023634  0.000267\n",
            "Iteration 00256    Loss -0.023799  0.000260\n",
            "Iteration 00257    Loss -0.023940  0.000268\n",
            "Iteration 00258    Loss -0.024012  0.000267\n",
            "Iteration 00259    Loss -0.024188  0.000268\n",
            "Iteration 00260    Loss -0.024343  0.000269\n",
            "Iteration 00261    Loss -0.024468  0.000270\n",
            "Iteration 00262    Loss -0.024567  0.000273\n",
            "Iteration 00263    Loss -0.024732  0.000265\n",
            "Iteration 00264    Loss -0.024850  0.000270\n",
            "Iteration 00265    Loss -0.024966  0.000282\n",
            "Iteration 00266    Loss -0.025108  0.000280\n",
            "Iteration 00267    Loss -0.025253  0.000270\n",
            "Iteration 00268    Loss -0.025343  0.000272\n",
            "Iteration 00269    Loss -0.025476  0.000283\n",
            "Iteration 00270    Loss -0.025613  0.000277\n",
            "Iteration 00271    Loss -0.025695  0.000268\n",
            "Iteration 00272    Loss -0.025813  0.000273\n",
            "Iteration 00273    Loss -0.025915  0.000278\n",
            "Iteration 00274    Loss -0.026075  0.000270\n",
            "Iteration 00275    Loss -0.026184  0.000276\n",
            "Iteration 00276    Loss -0.026251  0.000280\n",
            "Iteration 00277    Loss -0.026360  0.000274\n",
            "Iteration 00278    Loss -0.026521  0.000273\n",
            "Iteration 00279    Loss -0.026643  0.000279\n",
            "Iteration 00280    Loss -0.026706  0.000282\n",
            "Iteration 00281    Loss -0.026879  0.000280\n",
            "Iteration 00282    Loss -0.026956  0.000275\n",
            "Iteration 00283    Loss -0.027073  0.000276\n",
            "Iteration 00284    Loss -0.027199  0.000285\n",
            "Iteration 00285    Loss -0.027305  0.000277\n",
            "Iteration 00286    Loss -0.027419  0.000276\n",
            "Iteration 00287    Loss -0.027511  0.000281\n",
            "Iteration 00288    Loss -0.027642  0.000279\n",
            "Iteration 00289    Loss -0.027747  0.000279\n",
            "Iteration 00290    Loss -0.027866  0.000279\n",
            "Iteration 00291    Loss -0.027966  0.000284\n",
            "Iteration 00292    Loss -0.028059  0.000285\n",
            "Iteration 00293    Loss -0.028147  0.000282\n",
            "Iteration 00294    Loss -0.028272  0.000275\n",
            "Iteration 00295    Loss -0.028365  0.000274\n",
            "Iteration 00296    Loss -0.028431  0.000273\n",
            "Iteration 00297    Loss -0.028523  0.000282\n",
            "Iteration 00298    Loss -0.028530  0.000281\n",
            "Iteration 00299    Loss -0.028693  0.000273\n",
            "Iteration 00300    Loss -0.028799  0.000259\n",
            "Iteration 00301    Loss -0.028901  0.000255\n",
            "Iteration 00302    Loss -0.029013  0.000265\n",
            "Iteration 00303    Loss -0.029138  0.000268\n",
            "Iteration 00304    Loss -0.029204  0.000256\n",
            "Iteration 00305    Loss -0.029320  0.000257\n",
            "Iteration 00306    Loss -0.029409  0.000263\n",
            "Iteration 00307    Loss -0.029473  0.000264\n",
            "Iteration 00308    Loss -0.029576  0.000261\n",
            "Iteration 00309    Loss -0.029654  0.000269\n",
            "Iteration 00310    Loss -0.029786  0.000268\n",
            "Iteration 00311    Loss -0.029893  0.000252\n",
            "Iteration 00312    Loss -0.029970  0.000252\n",
            "Iteration 00313    Loss -0.030066  0.000267\n",
            "Iteration 00314    Loss -0.030176  0.000275\n",
            "Iteration 00315    Loss -0.030246  0.000272\n",
            "Iteration 00316    Loss -0.030365  0.000272\n",
            "Iteration 00317    Loss -0.030465  0.000262\n",
            "Iteration 00318    Loss -0.030554  0.000264\n",
            "Iteration 00319    Loss -0.030648  0.000274\n",
            "Iteration 00320    Loss -0.030737  0.000274\n",
            "Iteration 00321    Loss -0.030811  0.000272\n",
            "Iteration 00322    Loss -0.030908  0.000271\n",
            "Iteration 00323    Loss -0.030986  0.000270\n",
            "Iteration 00324    Loss -0.031084  0.000276\n",
            "Iteration 00325    Loss -0.031162  0.000277\n",
            "Iteration 00326    Loss -0.031244  0.000274\n",
            "Iteration 00327    Loss -0.031340  0.000279\n",
            "Iteration 00328    Loss -0.031415  0.000278\n",
            "Iteration 00329    Loss -0.031487  0.000270\n",
            "Iteration 00330    Loss -0.031579  0.000276\n",
            "Iteration 00331    Loss -0.031668  0.000274\n",
            "Iteration 00332    Loss -0.031761  0.000277\n",
            "Iteration 00333    Loss -0.031826  0.000279\n",
            "Iteration 00334    Loss -0.031884  0.000273\n",
            "Iteration 00335    Loss -0.031950  0.000273\n",
            "Iteration 00336    Loss -0.032010  0.000277\n",
            "Iteration 00337    Loss -0.032091  0.000270\n",
            "Iteration 00338    Loss -0.032197  0.000276\n",
            "Iteration 00339    Loss -0.032235  0.000272\n",
            "Iteration 00340    Loss -0.032349  0.000266\n",
            "Iteration 00341    Loss -0.032445  0.000269\n",
            "Iteration 00342    Loss -0.032484  0.000266\n",
            "Iteration 00343    Loss -0.032583  0.000273\n",
            "Iteration 00344    Loss -0.032635  0.000273\n",
            "Iteration 00345    Loss -0.032717  0.000264\n",
            "Iteration 00346    Loss -0.032790  0.000260\n",
            "Iteration 00347    Loss -0.032875  0.000275\n",
            "Iteration 00348    Loss -0.032938  0.000271\n",
            "Iteration 00349    Loss -0.033029  0.000263\n",
            "Iteration 00350    Loss -0.033110  0.000265\n",
            "Iteration 00351    Loss -0.033181  0.000265\n",
            "Iteration 00352    Loss -0.033251  0.000268\n",
            "Iteration 00353    Loss -0.033314  0.000263\n",
            "Iteration 00354    Loss -0.033413  0.000270\n",
            "Iteration 00355    Loss -0.033457  0.000274\n",
            "Iteration 00356    Loss -0.033551  0.000267\n",
            "Iteration 00357    Loss -0.033587  0.000269\n",
            "Iteration 00358    Loss -0.033672  0.000274\n",
            "Iteration 00359    Loss -0.033662  0.000277\n",
            "Iteration 00360    Loss -0.033681  0.000266\n",
            "Iteration 00361    Loss -0.033782  0.000256\n",
            "Iteration 00362    Loss -0.033928  0.000257\n",
            "Iteration 00363    Loss -0.033904  0.000261\n",
            "Iteration 00364    Loss -0.033989  0.000250\n",
            "Iteration 00365    Loss -0.034108  0.000252\n",
            "Iteration 00366    Loss -0.034119  0.000264\n",
            "Iteration 00367    Loss -0.034237  0.000257\n",
            "Iteration 00368    Loss -0.034302  0.000242\n",
            "Iteration 00369    Loss -0.034383  0.000247\n",
            "Iteration 00370    Loss -0.034457  0.000260\n",
            "Iteration 00371    Loss -0.034542  0.000255\n",
            "Iteration 00372    Loss -0.034611  0.000252\n",
            "Iteration 00373    Loss -0.034671  0.000255\n",
            "Iteration 00374    Loss -0.034747  0.000262\n",
            "Iteration 00375    Loss -0.034817  0.000262\n",
            "Iteration 00376    Loss -0.034844  0.000263\n",
            "Iteration 00377    Loss -0.034915  0.000261\n",
            "Iteration 00378    Loss -0.035004  0.000260\n",
            "Iteration 00379    Loss -0.035064  0.000263\n",
            "Iteration 00380    Loss -0.035105  0.000266\n",
            "Iteration 00381    Loss -0.035180  0.000272\n",
            "Iteration 00382    Loss -0.035269  0.000268\n",
            "Iteration 00383    Loss -0.035311  0.000267\n",
            "Iteration 00384    Loss -0.035376  0.000265\n",
            "Iteration 00385    Loss -0.035420  0.000268\n",
            "Iteration 00386    Loss -0.035494  0.000274\n",
            "Iteration 00387    Loss -0.035560  0.000266\n",
            "Iteration 00388    Loss -0.035629  0.000268\n",
            "Iteration 00389    Loss -0.035656  0.000277\n",
            "Iteration 00390    Loss -0.035681  0.000263\n",
            "Iteration 00391    Loss -0.035706  0.000269\n",
            "Iteration 00392    Loss -0.035838  0.000252\n",
            "Iteration 00393    Loss -0.035896  0.000255\n",
            "Iteration 00394    Loss -0.035933  0.000268\n",
            "Iteration 00395    Loss -0.036009  0.000263\n",
            "Iteration 00396    Loss -0.036073  0.000261\n",
            "Iteration 00397    Loss -0.036136  0.000259\n",
            "Iteration 00398    Loss -0.036171  0.000268\n",
            "Iteration 00399    Loss -0.036257  0.000260\n",
            "Iteration 00400    Loss -0.036313  0.000257\n",
            "Iteration 00401    Loss -0.036343  0.000265\n",
            "Iteration 00402    Loss -0.036447  0.000254\n",
            "Iteration 00403    Loss -0.036472  0.000253\n",
            "Iteration 00404    Loss -0.036540  0.000265\n",
            "Iteration 00405    Loss -0.036597  0.000266\n",
            "Iteration 00406    Loss -0.036676  0.000247\n",
            "Iteration 00407    Loss -0.036718  0.000254\n",
            "Iteration 00408    Loss -0.036759  0.000274\n",
            "Iteration 00409    Loss -0.036822  0.000270\n",
            "Iteration 00410    Loss -0.036886  0.000254\n",
            "Iteration 00411    Loss -0.036926  0.000263\n",
            "Iteration 00412    Loss -0.036979  0.000273\n",
            "Iteration 00413    Loss -0.037034  0.000269\n",
            "Iteration 00414    Loss -0.037056  0.000254\n",
            "Iteration 00415    Loss -0.037132  0.000275\n",
            "Iteration 00416    Loss -0.037205  0.000265\n",
            "Iteration 00417    Loss -0.037239  0.000256\n",
            "Iteration 00418    Loss -0.037304  0.000262\n",
            "Iteration 00419    Loss -0.037349  0.000266\n",
            "Iteration 00420    Loss -0.037390  0.000269\n",
            "Iteration 00421    Loss -0.037442  0.000267\n",
            "Iteration 00422    Loss -0.037507  0.000266\n",
            "Iteration 00423    Loss -0.037567  0.000265\n",
            "Iteration 00424    Loss -0.037610  0.000265\n",
            "Iteration 00425    Loss -0.037656  0.000262\n",
            "Iteration 00426    Loss -0.037710  0.000265\n",
            "Iteration 00427    Loss -0.037756  0.000268\n",
            "Iteration 00428    Loss -0.037831  0.000266\n",
            "Iteration 00429    Loss -0.037863  0.000271\n",
            "Iteration 00430    Loss -0.037897  0.000261\n",
            "Iteration 00431    Loss -0.037880  0.000273\n",
            "Iteration 00432    Loss -0.037885  0.000255\n",
            "Iteration 00433    Loss -0.037958  0.000268\n",
            "Iteration 00434    Loss -0.038074  0.000254\n",
            "Iteration 00435    Loss -0.038118  0.000248\n",
            "Iteration 00436    Loss -0.038113  0.000259\n",
            "Iteration 00437    Loss -0.038158  0.000257\n",
            "Iteration 00438    Loss -0.038209  0.000253\n",
            "Iteration 00439    Loss -0.038267  0.000245\n",
            "Iteration 00440    Loss -0.038324  0.000249\n",
            "Iteration 00441    Loss -0.038397  0.000253\n",
            "Iteration 00442    Loss -0.038449  0.000248\n",
            "Iteration 00443    Loss -0.038480  0.000242\n",
            "Iteration 00444    Loss -0.038545  0.000250\n",
            "Iteration 00445    Loss -0.038587  0.000257\n",
            "Iteration 00446    Loss -0.038641  0.000253\n",
            "Iteration 00447    Loss -0.038677  0.000245\n",
            "Iteration 00448    Loss -0.038718  0.000260\n",
            "Iteration 00449    Loss -0.038784  0.000265\n",
            "Iteration 00450    Loss -0.038840  0.000253\n",
            "Iteration 00451    Loss -0.038884  0.000253\n",
            "Iteration 00452    Loss -0.038920  0.000257\n",
            "Iteration 00453    Loss -0.038972  0.000264\n",
            "Iteration 00454    Loss -0.039000  0.000253\n",
            "Iteration 00455    Loss -0.039066  0.000255\n",
            "Iteration 00456    Loss -0.039106  0.000255\n",
            "Iteration 00457    Loss -0.039146  0.000258\n",
            "Iteration 00458    Loss -0.039180  0.000266\n",
            "Iteration 00459    Loss -0.039238  0.000258\n",
            "Iteration 00460    Loss -0.039290  0.000263\n",
            "Iteration 00461    Loss -0.039312  0.000258\n",
            "Iteration 00462    Loss -0.039369  0.000255\n",
            "Iteration 00463    Loss -0.039423  0.000264\n",
            "Iteration 00464    Loss -0.039472  0.000266\n",
            "Iteration 00465    Loss -0.039492  0.000261\n",
            "Iteration 00466    Loss -0.039533  0.000258\n",
            "Iteration 00467    Loss -0.039543  0.000271\n",
            "Iteration 00468    Loss -0.039576  0.000254\n",
            "Iteration 00469    Loss -0.039617  0.000259\n",
            "Iteration 00470    Loss -0.039654  0.000255\n",
            "Iteration 00471    Loss -0.039713  0.000260\n",
            "Iteration 00472    Loss -0.039774  0.000252\n",
            "Iteration 00473    Loss -0.039812  0.000254\n",
            "Iteration 00474    Loss -0.039846  0.000268\n",
            "Iteration 00475    Loss -0.039862  0.000253\n",
            "Iteration 00476    Loss -0.039907  0.000259\n",
            "Iteration 00477    Loss -0.039936  0.000258\n",
            "Iteration 00478    Loss -0.039986  0.000254\n",
            "Iteration 00479    Loss -0.040025  0.000265\n",
            "Iteration 00480    Loss -0.040077  0.000251\n",
            "Iteration 00481    Loss -0.040106  0.000262\n",
            "Iteration 00482    Loss -0.040154  0.000266\n",
            "Iteration 00483    Loss -0.040159  0.000245\n",
            "Iteration 00484    Loss -0.040228  0.000259\n",
            "Iteration 00485    Loss -0.040278  0.000263\n",
            "Iteration 00486    Loss -0.040294  0.000251\n",
            "Iteration 00487    Loss -0.040351  0.000260\n",
            "Iteration 00488    Loss -0.040357  0.000258\n",
            "Iteration 00489    Loss -0.040402  0.000257\n",
            "Iteration 00490    Loss -0.040449  0.000269\n",
            "Iteration 00491    Loss -0.040511  0.000262\n",
            "Iteration 00492    Loss -0.040503  0.000254\n",
            "Iteration 00493    Loss -0.040553  0.000257\n",
            "Iteration 00494    Loss -0.040569  0.000264\n",
            "Iteration 00495    Loss -0.040615  0.000256\n",
            "Iteration 00496    Loss -0.040641  0.000267\n",
            "Iteration 00497    Loss -0.040653  0.000261\n",
            "Iteration 00498    Loss -0.040620  0.000254\n",
            "Iteration 00499    Loss -0.040635  0.000241\n",
            " 99% 493/500 [4:50:43<04:08, 35.53s/it]/content/SOTS/indoor/hazy/1424_9.png\n",
            "Iteration 00000    Loss 0.423840  0.000034\n",
            "Iteration 00001    Loss 12.446032  0.000007\n",
            "Iteration 00002    Loss 0.344369  0.000003\n",
            "Iteration 00003    Loss 0.297234  0.000003\n",
            "Iteration 00004    Loss 0.261411  0.000001\n",
            "Iteration 00005    Loss 0.204624  0.000001\n",
            "Iteration 00006    Loss 0.161240  0.000001\n",
            "Iteration 00007    Loss 0.138409  0.000001\n",
            "Iteration 00008    Loss 0.122456  0.000001\n",
            "Iteration 00009    Loss 0.109226  0.000000\n",
            "Iteration 00010    Loss 0.098037  0.000001\n",
            "Iteration 00011    Loss 0.090670  0.000001\n",
            "Iteration 00012    Loss 0.083321  0.000001\n",
            "Iteration 00013    Loss 0.078616  0.000001\n",
            "Iteration 00014    Loss 0.074355  0.000001\n",
            "Iteration 00015    Loss 0.070564  0.000001\n",
            "Iteration 00016    Loss 0.067559  0.000001\n",
            "Iteration 00017    Loss 0.065232  0.000001\n",
            "Iteration 00018    Loss 0.062864  0.000001\n",
            "Iteration 00019    Loss 0.061018  0.000001\n",
            "Iteration 00020    Loss 0.058989  0.000001\n",
            "Iteration 00021    Loss 0.057471  0.000001\n",
            "Iteration 00022    Loss 0.055651  0.000001\n",
            "Iteration 00023    Loss 0.054678  0.000002\n",
            "Iteration 00024    Loss 0.053223  0.000002\n",
            "Iteration 00025    Loss 0.051946  0.000002\n",
            "Iteration 00026    Loss 0.050827  0.000002\n",
            "Iteration 00027    Loss 0.049827  0.000003\n",
            "Iteration 00028    Loss 0.048763  0.000003\n",
            "Iteration 00029    Loss 0.047741  0.000003\n",
            "Iteration 00030    Loss 0.046771  0.000004\n",
            "Iteration 00031    Loss 0.045726  0.000004\n",
            "Iteration 00032    Loss 0.044791  0.000005\n",
            "Iteration 00033    Loss 0.043872  0.000005\n",
            "Iteration 00034    Loss 0.042920  0.000005\n",
            "Iteration 00035    Loss 0.042046  0.000006\n",
            "Iteration 00036    Loss 0.041173  0.000007\n",
            "Iteration 00037    Loss 0.040177  0.000009\n",
            "Iteration 00038    Loss 0.039273  0.000010\n",
            "Iteration 00039    Loss 0.038520  0.000011\n",
            "Iteration 00040    Loss 0.037425  0.000012\n",
            "Iteration 00041    Loss 0.036448  0.000011\n",
            "Iteration 00042    Loss 0.035724  0.000011\n",
            "Iteration 00043    Loss 0.034842  0.000013\n",
            "Iteration 00044    Loss 0.033995  0.000013\n",
            "Iteration 00045    Loss 0.033274  0.000013\n",
            "Iteration 00046    Loss 0.032513  0.000014\n",
            "Iteration 00047    Loss 0.031720  0.000015\n",
            "Iteration 00048    Loss 0.031072  0.000016\n",
            "Iteration 00049    Loss 0.030381  0.000018\n",
            "Iteration 00050    Loss 0.029784  0.000017\n",
            "Iteration 00051    Loss 0.029754  0.000017\n",
            "Iteration 00052    Loss 0.028624  0.000017\n",
            "Iteration 00053    Loss 0.028043  0.000018\n",
            "Iteration 00054    Loss 0.027246  0.000019\n",
            "Iteration 00055    Loss 0.026436  0.000020\n",
            "Iteration 00056    Loss 0.025871  0.000021\n",
            "Iteration 00057    Loss 0.025138  0.000020\n",
            "Iteration 00058    Loss 0.024621  0.000021\n",
            "Iteration 00059    Loss 0.023758  0.000023\n",
            "Iteration 00060    Loss 0.023675  0.000025\n",
            "Iteration 00061    Loss 0.022812  0.000026\n",
            "Iteration 00062    Loss 0.022389  0.000025\n",
            "Iteration 00063    Loss 0.021595  0.000025\n",
            "Iteration 00064    Loss 0.020898  0.000026\n",
            "Iteration 00065    Loss 0.020495  0.000027\n",
            "Iteration 00066    Loss 0.019965  0.000028\n",
            "Iteration 00067    Loss 0.019476  0.000031\n",
            "Iteration 00068    Loss 0.018855  0.000034\n",
            "Iteration 00069    Loss 0.018153  0.000031\n",
            "Iteration 00070    Loss 0.017966  0.000030\n",
            "Iteration 00071    Loss 0.016951  0.000032\n",
            "Iteration 00072    Loss 0.016658  0.000035\n",
            "Iteration 00073    Loss 0.015944  0.000037\n",
            "Iteration 00074    Loss 0.015485  0.000037\n",
            "Iteration 00075    Loss 0.014921  0.000037\n",
            "Iteration 00076    Loss 0.014361  0.000039\n",
            "Iteration 00077    Loss 0.013854  0.000040\n",
            "Iteration 00078    Loss 0.013428  0.000041\n",
            "Iteration 00079    Loss 0.012859  0.000042\n",
            "Iteration 00080    Loss 0.012448  0.000043\n",
            "Iteration 00081    Loss 0.011970  0.000043\n",
            "Iteration 00082    Loss 0.011523  0.000044\n",
            "Iteration 00083    Loss 0.011248  0.000044\n",
            "Iteration 00084    Loss 0.010570  0.000045\n",
            "Iteration 00085    Loss 0.010047  0.000047\n",
            "Iteration 00086    Loss 0.009845  0.000049\n",
            "Iteration 00087    Loss 0.009330  0.000051\n",
            "Iteration 00088    Loss 0.009033  0.000052\n",
            "Iteration 00089    Loss 0.008498  0.000049\n",
            "Iteration 00090    Loss 0.007976  0.000051\n",
            "Iteration 00091    Loss 0.007745  0.000053\n",
            "Iteration 00092    Loss 0.007029  0.000053\n",
            "Iteration 00093    Loss 0.006713  0.000055\n",
            "Iteration 00094    Loss 0.006263  0.000055\n",
            "Iteration 00095    Loss 0.005740  0.000057\n",
            "Iteration 00096    Loss 0.005395  0.000058\n",
            "Iteration 00097    Loss 0.004940  0.000059\n",
            "Iteration 00098    Loss 0.004519  0.000063\n",
            "Iteration 00099    Loss 0.004161  0.000062\n",
            "Iteration 00100    Loss 0.003760  0.000063\n",
            "Iteration 00101    Loss 0.003331  0.000064\n",
            "Iteration 00102    Loss 0.002962  0.000064\n",
            "Iteration 00103    Loss 0.002644  0.000066\n",
            "Iteration 00104    Loss 0.002177  0.000064\n",
            "Iteration 00105    Loss 0.001811  0.000067\n",
            "Iteration 00106    Loss 0.001412  0.000070\n",
            "Iteration 00107    Loss 0.001082  0.000067\n",
            "Iteration 00108    Loss 0.000755  0.000069\n",
            "Iteration 00109    Loss 0.000329  0.000075\n",
            "Iteration 00110    Loss -0.000037  0.000073\n",
            "Iteration 00111    Loss -0.000361  0.000071\n",
            "Iteration 00112    Loss -0.000718  0.000068\n",
            "Iteration 00113    Loss -0.001101  0.000073\n",
            "Iteration 00114    Loss -0.001356  0.000072\n",
            "Iteration 00115    Loss -0.001801  0.000073\n",
            "Iteration 00116    Loss -0.002176  0.000079\n",
            "Iteration 00117    Loss -0.002409  0.000077\n",
            "Iteration 00118    Loss -0.002784  0.000078\n",
            "Iteration 00119    Loss -0.003105  0.000085\n",
            "Iteration 00120    Loss -0.003374  0.000081\n",
            "Iteration 00121    Loss -0.003687  0.000082\n",
            "Iteration 00122    Loss -0.004043  0.000085\n",
            "Iteration 00123    Loss -0.004365  0.000082\n",
            "Iteration 00124    Loss -0.004560  0.000082\n",
            "Iteration 00125    Loss -0.004890  0.000079\n",
            "Iteration 00126    Loss -0.005174  0.000083\n",
            "Iteration 00127    Loss -0.005487  0.000086\n",
            "Iteration 00128    Loss -0.005888  0.000082\n",
            "Iteration 00129    Loss -0.006134  0.000084\n",
            "Iteration 00130    Loss -0.006483  0.000085\n",
            "Iteration 00131    Loss -0.006787  0.000085\n",
            "Iteration 00132    Loss -0.007008  0.000085\n",
            "Iteration 00133    Loss -0.007345  0.000092\n",
            "Iteration 00134    Loss -0.007584  0.000087\n",
            "Iteration 00135    Loss -0.007917  0.000088\n",
            "Iteration 00136    Loss -0.008140  0.000091\n",
            "Iteration 00137    Loss -0.008397  0.000087\n",
            "Iteration 00138    Loss -0.008613  0.000088\n",
            "Iteration 00139    Loss -0.008824  0.000089\n",
            "Iteration 00140    Loss -0.009145  0.000090\n",
            "Iteration 00141    Loss -0.009367  0.000095\n",
            "Iteration 00142    Loss -0.009713  0.000095\n",
            "Iteration 00143    Loss -0.009996  0.000095\n",
            "Iteration 00144    Loss -0.010256  0.000095\n",
            "Iteration 00145    Loss -0.010473  0.000097\n",
            "Iteration 00146    Loss -0.010774  0.000101\n",
            "Iteration 00147    Loss -0.010982  0.000099\n",
            "Iteration 00148    Loss -0.011208  0.000098\n",
            "Iteration 00149    Loss -0.011500  0.000099\n",
            "Iteration 00150    Loss -0.011677  0.000090\n",
            "Iteration 00151    Loss -0.011996  0.000097\n",
            "Iteration 00152    Loss -0.012284  0.000098\n",
            "Iteration 00153    Loss -0.012389  0.000095\n",
            "Iteration 00154    Loss -0.012647  0.000094\n",
            "Iteration 00155    Loss -0.012857  0.000101\n",
            "Iteration 00156    Loss -0.013018  0.000103\n",
            "Iteration 00157    Loss -0.013148  0.000092\n",
            "Iteration 00158    Loss -0.013535  0.000097\n",
            "Iteration 00159    Loss -0.013767  0.000100\n",
            "Iteration 00160    Loss -0.013971  0.000096\n",
            "Iteration 00161    Loss -0.014197  0.000104\n",
            "Iteration 00162    Loss -0.014463  0.000104\n",
            "Iteration 00163    Loss -0.014688  0.000099\n",
            "Iteration 00164    Loss -0.014910  0.000100\n",
            "Iteration 00165    Loss -0.015086  0.000101\n",
            "Iteration 00166    Loss -0.015195  0.000103\n",
            "Iteration 00167    Loss -0.015452  0.000103\n",
            "Iteration 00168    Loss -0.015799  0.000103\n",
            "Iteration 00169    Loss -0.015952  0.000103\n",
            "Iteration 00170    Loss -0.016112  0.000100\n",
            "Iteration 00171    Loss -0.016303  0.000102\n",
            "Iteration 00172    Loss -0.016568  0.000103\n",
            "Iteration 00173    Loss -0.016740  0.000103\n",
            "Iteration 00174    Loss -0.016900  0.000105\n",
            "Iteration 00175    Loss -0.017116  0.000105\n",
            "Iteration 00176    Loss -0.017297  0.000105\n",
            "Iteration 00177    Loss -0.017596  0.000101\n",
            "Iteration 00178    Loss -0.017732  0.000106\n",
            "Iteration 00179    Loss -0.018031  0.000106\n",
            "Iteration 00180    Loss -0.018064  0.000103\n",
            "Iteration 00181    Loss -0.018154  0.000109\n",
            "Iteration 00182    Loss -0.018492  0.000104\n",
            "Iteration 00183    Loss -0.018689  0.000106\n",
            "Iteration 00184    Loss -0.018731  0.000105\n",
            "Iteration 00185    Loss -0.019008  0.000103\n",
            "Iteration 00186    Loss -0.019193  0.000107\n",
            "Iteration 00187    Loss -0.019360  0.000105\n",
            "Iteration 00188    Loss -0.019535  0.000106\n",
            "Iteration 00189    Loss -0.019795  0.000107\n",
            "Iteration 00190    Loss -0.019944  0.000102\n",
            "Iteration 00191    Loss -0.019999  0.000111\n",
            "Iteration 00192    Loss -0.020202  0.000103\n",
            "Iteration 00193    Loss -0.020350  0.000103\n",
            "Iteration 00194    Loss -0.020562  0.000114\n",
            "Iteration 00195    Loss -0.020666  0.000110\n",
            "Iteration 00196    Loss -0.020929  0.000103\n",
            "Iteration 00197    Loss -0.020982  0.000111\n",
            "Iteration 00198    Loss -0.021250  0.000108\n",
            "Iteration 00199    Loss -0.021275  0.000101\n",
            "Iteration 00200    Loss -0.021506  0.000098\n",
            "Iteration 00201    Loss -0.021703  0.000105\n",
            "Iteration 00202    Loss -0.021781  0.000104\n",
            "Iteration 00203    Loss -0.021946  0.000097\n",
            "Iteration 00204    Loss -0.022078  0.000103\n",
            "Iteration 00205    Loss -0.022275  0.000113\n",
            "Iteration 00206    Loss -0.022509  0.000100\n",
            "Iteration 00207    Loss -0.022568  0.000098\n",
            "Iteration 00208    Loss -0.022737  0.000107\n",
            "Iteration 00209    Loss -0.022951  0.000107\n",
            "Iteration 00210    Loss -0.022954  0.000110\n",
            "Iteration 00211    Loss -0.023121  0.000114\n",
            "Iteration 00212    Loss -0.023317  0.000107\n",
            "Iteration 00213    Loss -0.023493  0.000101\n",
            "Iteration 00214    Loss -0.023670  0.000100\n",
            "Iteration 00215    Loss -0.023758  0.000105\n",
            "Iteration 00216    Loss -0.023631  0.000110\n",
            "Iteration 00217    Loss -0.024038  0.000109\n",
            "Iteration 00218    Loss -0.024129  0.000109\n",
            "Iteration 00219    Loss -0.024344  0.000107\n",
            "Iteration 00220    Loss -0.024415  0.000111\n",
            "Iteration 00221    Loss -0.024609  0.000112\n",
            "Iteration 00222    Loss -0.024664  0.000108\n",
            "Iteration 00223    Loss -0.024960  0.000111\n",
            "Iteration 00224    Loss -0.025018  0.000109\n",
            "Iteration 00225    Loss -0.025163  0.000101\n",
            "Iteration 00226    Loss -0.025193  0.000107\n",
            "Iteration 00227    Loss -0.025405  0.000107\n",
            "Iteration 00228    Loss -0.025535  0.000104\n",
            "Iteration 00229    Loss -0.025718  0.000114\n",
            "Iteration 00230    Loss -0.025900  0.000110\n",
            "Iteration 00231    Loss -0.025907  0.000108\n",
            "Iteration 00232    Loss -0.026207  0.000115\n",
            "Iteration 00233    Loss -0.026057  0.000114\n",
            "Iteration 00234    Loss -0.026454  0.000112\n",
            "Iteration 00235    Loss -0.026401  0.000108\n",
            "Iteration 00236    Loss -0.026542  0.000111\n",
            "Iteration 00237    Loss -0.026735  0.000117\n",
            "Iteration 00238    Loss -0.026887  0.000119\n",
            "Iteration 00239    Loss -0.026961  0.000108\n",
            "Iteration 00240    Loss -0.026951  0.000112\n",
            "Iteration 00241    Loss -0.027181  0.000120\n",
            "Iteration 00242    Loss -0.027382  0.000115\n",
            "Iteration 00243    Loss -0.027551  0.000113\n",
            "Iteration 00244    Loss -0.027690  0.000112\n",
            "Iteration 00245    Loss -0.027530  0.000112\n",
            "Iteration 00246    Loss -0.027568  0.000118\n",
            "Iteration 00247    Loss -0.028073  0.000109\n",
            "Iteration 00248    Loss -0.028214  0.000116\n",
            "Iteration 00249    Loss -0.028174  0.000119\n",
            "Iteration 00250    Loss -0.028209  0.000115\n",
            "Iteration 00251    Loss -0.028172  0.000117\n",
            "Iteration 00252    Loss -0.028309  0.000111\n",
            "Iteration 00253    Loss -0.028352  0.000115\n",
            "Iteration 00254    Loss -0.028635  0.000112\n",
            "Iteration 00255    Loss -0.028673  0.000109\n",
            "Iteration 00256    Loss -0.028872  0.000109\n",
            "Iteration 00257    Loss -0.029070  0.000113\n",
            "Iteration 00258    Loss -0.029180  0.000118\n",
            "Iteration 00259    Loss -0.029407  0.000103\n",
            "Iteration 00260    Loss -0.029434  0.000107\n",
            "Iteration 00261    Loss -0.029425  0.000116\n",
            "Iteration 00262    Loss -0.029679  0.000108\n",
            "Iteration 00263    Loss -0.029843  0.000112\n",
            "Iteration 00264    Loss -0.030014  0.000116\n",
            "Iteration 00265    Loss -0.030034  0.000113\n",
            "Iteration 00266    Loss -0.030182  0.000111\n",
            "Iteration 00267    Loss -0.030241  0.000108\n",
            "Iteration 00268    Loss -0.030341  0.000117\n",
            "Iteration 00269    Loss -0.030491  0.000111\n",
            "Iteration 00270    Loss -0.030469  0.000105\n",
            "Iteration 00271    Loss -0.030346  0.000117\n",
            "Iteration 00272    Loss -0.030783  0.000106\n",
            "Iteration 00273    Loss -0.031064  0.000108\n",
            "Iteration 00274    Loss -0.030822  0.000117\n",
            "Iteration 00275    Loss -0.030981  0.000114\n",
            "Iteration 00276    Loss -0.031012  0.000106\n",
            "Iteration 00277    Loss -0.031031  0.000103\n",
            "Iteration 00278    Loss -0.031131  0.000116\n",
            "Iteration 00279    Loss -0.031586  0.000118\n",
            "Iteration 00280    Loss -0.031198  0.000099\n",
            "Iteration 00281    Loss -0.031751  0.000106\n",
            "Iteration 00282    Loss -0.031791  0.000122\n",
            "Iteration 00283    Loss -0.031946  0.000115\n",
            "Iteration 00284    Loss -0.031954  0.000105\n",
            "Iteration 00285    Loss -0.031983  0.000110\n",
            "Iteration 00286    Loss -0.032068  0.000116\n",
            "Iteration 00287    Loss -0.032060  0.000116\n",
            "Iteration 00288    Loss -0.032408  0.000108\n",
            "Iteration 00289    Loss -0.032306  0.000106\n",
            "Iteration 00290    Loss -0.032351  0.000112\n",
            "Iteration 00291    Loss -0.032357  0.000115\n",
            "Iteration 00292    Loss -0.032691  0.000117\n",
            "Iteration 00293    Loss -0.032197  0.000109\n",
            "Iteration 00294    Loss -0.032512  0.000115\n",
            "Iteration 00295    Loss -0.032788  0.000119\n",
            "Iteration 00296    Loss -0.032988  0.000109\n",
            "Iteration 00297    Loss -0.032874  0.000107\n",
            "Iteration 00298    Loss -0.033219  0.000113\n",
            "Iteration 00299    Loss -0.032988  0.000108\n",
            "Iteration 00300    Loss -0.032674  0.000101\n",
            "Iteration 00301    Loss -0.033130  0.000100\n",
            "Iteration 00302    Loss -0.033414  0.000105\n",
            "Iteration 00303    Loss -0.033305  0.000115\n",
            "Iteration 00304    Loss -0.033690  0.000101\n",
            "Iteration 00305    Loss -0.033455  0.000095\n",
            "Iteration 00306    Loss -0.033351  0.000102\n",
            "Iteration 00307    Loss -0.033624  0.000104\n",
            "Iteration 00308    Loss -0.033356  0.000103\n",
            "Iteration 00309    Loss -0.033823  0.000106\n",
            "Iteration 00310    Loss -0.034151  0.000109\n",
            "Iteration 00311    Loss -0.034172  0.000109\n",
            "Iteration 00312    Loss -0.034070  0.000107\n",
            "Iteration 00313    Loss -0.034438  0.000111\n",
            "Iteration 00314    Loss -0.034466  0.000111\n",
            "Iteration 00315    Loss -0.034593  0.000105\n",
            "Iteration 00316    Loss -0.034530  0.000107\n",
            "Iteration 00317    Loss -0.034826  0.000110\n",
            "Iteration 00318    Loss -0.034587  0.000110\n",
            "Iteration 00319    Loss -0.034502  0.000109\n",
            "Iteration 00320    Loss -0.034901  0.000113\n",
            "Iteration 00321    Loss -0.034972  0.000112\n",
            "Iteration 00322    Loss -0.035038  0.000111\n",
            "Iteration 00323    Loss -0.035159  0.000119\n",
            "Iteration 00324    Loss -0.035071  0.000122\n",
            "Iteration 00325    Loss -0.035131  0.000116\n",
            "Iteration 00326    Loss -0.035335  0.000118\n",
            "Iteration 00327    Loss -0.035436  0.000119\n",
            "Iteration 00328    Loss -0.035552  0.000121\n",
            "Iteration 00329    Loss -0.035585  0.000111\n",
            "Iteration 00330    Loss -0.035526  0.000111\n",
            "Iteration 00331    Loss -0.035406  0.000112\n",
            "Iteration 00332    Loss -0.035840  0.000115\n",
            "Iteration 00333    Loss -0.035581  0.000121\n",
            "Iteration 00334    Loss -0.035700  0.000116\n",
            "Iteration 00335    Loss -0.036012  0.000126\n",
            "Iteration 00336    Loss -0.036032  0.000105\n",
            "Iteration 00337    Loss -0.036089  0.000103\n",
            "Iteration 00338    Loss -0.036153  0.000120\n",
            "Iteration 00339    Loss -0.036032  0.000115\n",
            "Iteration 00340    Loss -0.036168  0.000109\n",
            "Iteration 00341    Loss -0.036208  0.000112\n",
            "Iteration 00342    Loss -0.036294  0.000118\n",
            "Iteration 00343    Loss -0.036089  0.000115\n",
            "Iteration 00344    Loss -0.036140  0.000111\n",
            "Iteration 00345    Loss -0.036566  0.000114\n",
            "Iteration 00346    Loss -0.036618  0.000121\n",
            "Iteration 00347    Loss -0.036668  0.000113\n",
            "Iteration 00348    Loss -0.036509  0.000116\n",
            "Iteration 00349    Loss -0.036366  0.000122\n",
            "Iteration 00350    Loss -0.036949  0.000112\n",
            "Iteration 00351    Loss -0.036907  0.000108\n",
            "Iteration 00352    Loss -0.037017  0.000114\n",
            "Iteration 00353    Loss -0.036667  0.000122\n",
            "Iteration 00354    Loss -0.037021  0.000113\n",
            "Iteration 00355    Loss -0.037192  0.000112\n",
            "Iteration 00356    Loss -0.036932  0.000111\n",
            "Iteration 00357    Loss -0.037094  0.000118\n",
            "Iteration 00358    Loss -0.037219  0.000110\n",
            "Iteration 00359    Loss -0.037330  0.000103\n",
            "Iteration 00360    Loss -0.037008  0.000118\n",
            "Iteration 00361    Loss -0.037574  0.000118\n",
            "Iteration 00362    Loss -0.037378  0.000110\n",
            "Iteration 00363    Loss -0.037431  0.000109\n",
            "Iteration 00364    Loss -0.037432  0.000111\n",
            "Iteration 00365    Loss -0.037727  0.000114\n",
            "Iteration 00366    Loss -0.037731  0.000118\n",
            "Iteration 00367    Loss -0.037780  0.000111\n",
            "Iteration 00368    Loss -0.037835  0.000107\n",
            "Iteration 00369    Loss -0.037604  0.000111\n",
            "Iteration 00370    Loss -0.037876  0.000116\n",
            "Iteration 00371    Loss -0.037959  0.000122\n",
            "Iteration 00372    Loss -0.038174  0.000111\n",
            "Iteration 00373    Loss -0.038169  0.000111\n",
            "Iteration 00374    Loss -0.038107  0.000113\n",
            "Iteration 00375    Loss -0.038128  0.000117\n",
            "Iteration 00376    Loss -0.038126  0.000112\n",
            "Iteration 00377    Loss -0.038161  0.000110\n",
            "Iteration 00378    Loss -0.038217  0.000123\n",
            "Iteration 00379    Loss -0.038161  0.000122\n",
            "Iteration 00380    Loss -0.038342  0.000111\n",
            "Iteration 00381    Loss -0.038141  0.000117\n",
            "Iteration 00382    Loss -0.038508  0.000122\n",
            "Iteration 00383    Loss -0.038503  0.000118\n",
            "Iteration 00384    Loss -0.038725  0.000109\n",
            "Iteration 00385    Loss -0.038710  0.000105\n",
            "Iteration 00386    Loss -0.038435  0.000117\n",
            "Iteration 00387    Loss -0.038870  0.000116\n",
            "Iteration 00388    Loss -0.038842  0.000107\n",
            "Iteration 00389    Loss -0.038980  0.000111\n",
            "Iteration 00390    Loss -0.038799  0.000113\n",
            "Iteration 00391    Loss -0.039021  0.000114\n",
            "Iteration 00392    Loss -0.039139  0.000113\n",
            "Iteration 00393    Loss -0.039140  0.000107\n",
            "Iteration 00394    Loss -0.039257  0.000117\n",
            "Iteration 00395    Loss -0.039260  0.000121\n",
            "Iteration 00396    Loss -0.039069  0.000110\n",
            "Iteration 00397    Loss -0.039428  0.000112\n",
            "Iteration 00398    Loss -0.039336  0.000120\n",
            "Iteration 00399    Loss -0.039312  0.000120\n",
            "Iteration 00400    Loss -0.039505  0.000121\n",
            "Iteration 00401    Loss -0.039459  0.000112\n",
            "Iteration 00402    Loss -0.039678  0.000118\n",
            "Iteration 00403    Loss -0.039725  0.000121\n",
            "Iteration 00404    Loss -0.039456  0.000111\n",
            "Iteration 00405    Loss -0.039714  0.000123\n",
            "Iteration 00406    Loss -0.039590  0.000118\n",
            "Iteration 00407    Loss -0.039908  0.000114\n",
            "Iteration 00408    Loss -0.039695  0.000118\n",
            "Iteration 00409    Loss -0.039591  0.000112\n",
            "Iteration 00410    Loss -0.040058  0.000121\n",
            "Iteration 00411    Loss -0.039996  0.000125\n",
            "Iteration 00412    Loss -0.040065  0.000115\n",
            "Iteration 00413    Loss -0.040034  0.000121\n",
            "Iteration 00414    Loss -0.040098  0.000123\n",
            "Iteration 00415    Loss -0.040263  0.000117\n",
            "Iteration 00416    Loss -0.040362  0.000120\n",
            "Iteration 00417    Loss -0.040266  0.000122\n",
            "Iteration 00418    Loss -0.040335  0.000119\n",
            "Iteration 00419    Loss -0.040417  0.000117\n",
            "Iteration 00420    Loss -0.040329  0.000121\n",
            "Iteration 00421    Loss -0.040422  0.000116\n",
            "Iteration 00422    Loss -0.040554  0.000119\n",
            "Iteration 00423    Loss -0.040543  0.000117\n",
            "Iteration 00424    Loss -0.040665  0.000120\n",
            "Iteration 00425    Loss -0.040742  0.000122\n",
            "Iteration 00426    Loss -0.040658  0.000119\n",
            "Iteration 00427    Loss -0.040773  0.000118\n",
            "Iteration 00428    Loss -0.040864  0.000124\n",
            "Iteration 00429    Loss -0.040818  0.000118\n",
            "Iteration 00430    Loss -0.040856  0.000122\n",
            "Iteration 00431    Loss -0.040583  0.000120\n",
            "Iteration 00432    Loss -0.040987  0.000120\n",
            "Iteration 00433    Loss -0.040965  0.000124\n",
            "Iteration 00434    Loss -0.040861  0.000128\n",
            "Iteration 00435    Loss -0.041084  0.000118\n",
            "Iteration 00436    Loss -0.041106  0.000122\n",
            "Iteration 00437    Loss -0.040884  0.000112\n",
            "Iteration 00438    Loss -0.041179  0.000123\n",
            "Iteration 00439    Loss -0.041188  0.000124\n",
            "Iteration 00440    Loss -0.041203  0.000115\n",
            "Iteration 00441    Loss -0.041303  0.000118\n",
            "Iteration 00442    Loss -0.041168  0.000117\n",
            "Iteration 00443    Loss -0.041354  0.000118\n",
            "Iteration 00444    Loss -0.041452  0.000129\n",
            "Iteration 00445    Loss -0.041111  0.000118\n",
            "Iteration 00446    Loss -0.041349  0.000115\n",
            "Iteration 00447    Loss -0.041525  0.000129\n",
            "Iteration 00448    Loss -0.041579  0.000122\n",
            "Iteration 00449    Loss -0.041320  0.000120\n",
            "Iteration 00450    Loss -0.041628  0.000127\n",
            "Iteration 00451    Loss -0.041721  0.000123\n",
            "Iteration 00452    Loss -0.041748  0.000118\n",
            "Iteration 00453    Loss -0.041746  0.000126\n",
            "Iteration 00454    Loss -0.041641  0.000128\n",
            "Iteration 00455    Loss -0.041765  0.000125\n",
            "Iteration 00456    Loss -0.041866  0.000123\n",
            "Iteration 00457    Loss -0.041910  0.000124\n",
            "Iteration 00458    Loss -0.041820  0.000125\n",
            "Iteration 00459    Loss -0.042026  0.000126\n",
            "Iteration 00460    Loss -0.041976  0.000131\n",
            "Iteration 00461    Loss -0.042029  0.000124\n",
            "Iteration 00462    Loss -0.042104  0.000124\n",
            "Iteration 00463    Loss -0.042167  0.000127\n",
            "Iteration 00464    Loss -0.042142  0.000125\n",
            "Iteration 00465    Loss -0.041891  0.000129\n",
            "Iteration 00466    Loss -0.042164  0.000134\n",
            "Iteration 00467    Loss -0.042153  0.000125\n",
            "Iteration 00468    Loss -0.042171  0.000119\n",
            "Iteration 00469    Loss -0.042134  0.000130\n",
            "Iteration 00470    Loss -0.042348  0.000126\n",
            "Iteration 00471    Loss -0.042271  0.000120\n",
            "Iteration 00472    Loss -0.042289  0.000132\n",
            "Iteration 00473    Loss -0.042323  0.000125\n",
            "Iteration 00474    Loss -0.042307  0.000124\n",
            "Iteration 00475    Loss -0.042312  0.000102\n",
            "Iteration 00476    Loss -0.042429  0.000113\n",
            "Iteration 00477    Loss -0.042397  0.000120\n",
            "Iteration 00478    Loss -0.042407  0.000112\n",
            "Iteration 00479    Loss -0.042567  0.000105\n",
            "Iteration 00480    Loss -0.042510  0.000112\n",
            "Iteration 00481    Loss -0.042560  0.000117\n",
            "Iteration 00482    Loss -0.042601  0.000113\n",
            "Iteration 00483    Loss -0.042701  0.000111\n",
            "Iteration 00484    Loss -0.042448  0.000108\n",
            "Iteration 00485    Loss -0.042695  0.000118\n",
            "Iteration 00486    Loss -0.042778  0.000118\n",
            "Iteration 00487    Loss -0.042832  0.000111\n",
            "Iteration 00488    Loss -0.042817  0.000113\n",
            "Iteration 00489    Loss -0.042833  0.000122\n",
            "Iteration 00490    Loss -0.042841  0.000114\n",
            "Iteration 00491    Loss -0.042940  0.000113\n",
            "Iteration 00492    Loss -0.042935  0.000122\n",
            "Iteration 00493    Loss -0.042957  0.000118\n",
            "Iteration 00494    Loss -0.043077  0.000117\n",
            "Iteration 00495    Loss -0.042896  0.000113\n",
            "Iteration 00496    Loss -0.043118  0.000108\n",
            "Iteration 00497    Loss -0.043124  0.000118\n",
            "Iteration 00498    Loss -0.043148  0.000129\n",
            "Iteration 00499    Loss -0.043194  0.000118\n",
            " 99% 494/500 [4:51:19<03:33, 35.57s/it]/content/SOTS/indoor/hazy/1410_10.png\n",
            "Iteration 00000    Loss 0.527656  0.000019\n",
            "Iteration 00001    Loss 24.153864  0.000006\n",
            "Iteration 00002    Loss 0.430041  0.000002\n",
            "Iteration 00003    Loss 1.281115  0.000001\n",
            "Iteration 00004    Loss 2.170280  0.000001\n",
            "Iteration 00005    Loss 0.853206  0.000001\n",
            "Iteration 00006    Loss 0.198100  0.000001\n",
            "Iteration 00007    Loss 0.176991  0.000001\n",
            "Iteration 00008    Loss 0.156974  0.000001\n",
            "Iteration 00009    Loss 0.140131  0.000001\n",
            "Iteration 00010    Loss 0.130650  0.000000\n",
            "Iteration 00011    Loss 0.120037  0.000000\n",
            "Iteration 00012    Loss 0.110368  0.000000\n",
            "Iteration 00013    Loss 0.102798  0.000000\n",
            "Iteration 00014    Loss 0.099003  0.000000\n",
            "Iteration 00015    Loss 0.095670  0.000001\n",
            "Iteration 00016    Loss 0.090197  0.000001\n",
            "Iteration 00017    Loss 0.086755  0.000001\n",
            "Iteration 00018    Loss 0.083629  0.000001\n",
            "Iteration 00019    Loss 0.082140  0.000001\n",
            "Iteration 00020    Loss 0.085754  0.000001\n",
            "Iteration 00021    Loss 0.090363  0.000001\n",
            "Iteration 00022    Loss 0.088485  0.000001\n",
            "Iteration 00023    Loss 0.082277  0.000001\n",
            "Iteration 00024    Loss 0.073921  0.000001\n",
            "Iteration 00025    Loss 0.071147  0.000002\n",
            "Iteration 00026    Loss 0.069981  0.000002\n",
            "Iteration 00027    Loss 0.068908  0.000002\n",
            "Iteration 00028    Loss 0.067857  0.000002\n",
            "Iteration 00029    Loss 0.066730  0.000002\n",
            "Iteration 00030    Loss 0.065716  0.000003\n",
            "Iteration 00031    Loss 0.064754  0.000003\n",
            "Iteration 00032    Loss 0.063739  0.000004\n",
            "Iteration 00033    Loss 0.062587  0.000004\n",
            "Iteration 00034    Loss 0.061490  0.000005\n",
            "Iteration 00035    Loss 0.060666  0.000005\n",
            "Iteration 00036    Loss 0.059571  0.000005\n",
            "Iteration 00037    Loss 0.058561  0.000005\n",
            "Iteration 00038    Loss 0.057631  0.000005\n",
            "Iteration 00039    Loss 0.056764  0.000005\n",
            "Iteration 00040    Loss 0.055835  0.000005\n",
            "Iteration 00041    Loss 0.054951  0.000005\n",
            "Iteration 00042    Loss 0.054077  0.000005\n",
            "Iteration 00043    Loss 0.053324  0.000005\n",
            "Iteration 00044    Loss 0.052439  0.000005\n",
            "Iteration 00045    Loss 0.051641  0.000006\n",
            "Iteration 00046    Loss 0.050771  0.000007\n",
            "Iteration 00047    Loss 0.050046  0.000006\n",
            "Iteration 00048    Loss 0.049266  0.000006\n",
            "Iteration 00049    Loss 0.048504  0.000007\n",
            "Iteration 00050    Loss 0.047716  0.000007\n",
            "Iteration 00051    Loss 0.046969  0.000007\n",
            "Iteration 00052    Loss 0.046230  0.000007\n",
            "Iteration 00053    Loss 0.045484  0.000007\n",
            "Iteration 00054    Loss 0.044826  0.000008\n",
            "Iteration 00055    Loss 0.044095  0.000008\n",
            "Iteration 00056    Loss 0.043356  0.000008\n",
            "Iteration 00057    Loss 0.042637  0.000009\n",
            "Iteration 00058    Loss 0.041998  0.000009\n",
            "Iteration 00059    Loss 0.041277  0.000009\n",
            "Iteration 00060    Loss 0.040595  0.000009\n",
            "Iteration 00061    Loss 0.039919  0.000010\n",
            "Iteration 00062    Loss 0.039294  0.000010\n",
            "Iteration 00063    Loss 0.038616  0.000011\n",
            "Iteration 00064    Loss 0.037999  0.000011\n",
            "Iteration 00065    Loss 0.037325  0.000011\n",
            "Iteration 00066    Loss 0.036686  0.000012\n",
            "Iteration 00067    Loss 0.036098  0.000012\n",
            "Iteration 00068    Loss 0.035503  0.000012\n",
            "Iteration 00069    Loss 0.034846  0.000013\n",
            "Iteration 00070    Loss 0.034235  0.000014\n",
            "Iteration 00071    Loss 0.033670  0.000014\n",
            "Iteration 00072    Loss 0.033017  0.000015\n",
            "Iteration 00073    Loss 0.032472  0.000015\n",
            "Iteration 00074    Loss 0.031945  0.000015\n",
            "Iteration 00075    Loss 0.031302  0.000016\n",
            "Iteration 00076    Loss 0.030747  0.000016\n",
            "Iteration 00077    Loss 0.030208  0.000016\n",
            "Iteration 00078    Loss 0.029640  0.000017\n",
            "Iteration 00079    Loss 0.029137  0.000017\n",
            "Iteration 00080    Loss 0.028547  0.000018\n",
            "Iteration 00081    Loss 0.028058  0.000017\n",
            "Iteration 00082    Loss 0.027544  0.000020\n",
            "Iteration 00083    Loss 0.027050  0.000019\n",
            "Iteration 00084    Loss 0.026458  0.000021\n",
            "Iteration 00085    Loss 0.025942  0.000023\n",
            "Iteration 00086    Loss 0.025546  0.000022\n",
            "Iteration 00087    Loss 0.024945  0.000024\n",
            "Iteration 00088    Loss 0.024538  0.000025\n",
            "Iteration 00089    Loss 0.024119  0.000022\n",
            "Iteration 00090    Loss 0.023540  0.000026\n",
            "Iteration 00091    Loss 0.023123  0.000027\n",
            "Iteration 00092    Loss 0.022677  0.000025\n",
            "Iteration 00093    Loss 0.022193  0.000026\n",
            "Iteration 00094    Loss 0.021685  0.000027\n",
            "Iteration 00095    Loss 0.021254  0.000030\n",
            "Iteration 00096    Loss 0.020851  0.000027\n",
            "Iteration 00097    Loss 0.020385  0.000029\n",
            "Iteration 00098    Loss 0.019949  0.000031\n",
            "Iteration 00099    Loss 0.019446  0.000029\n",
            "Iteration 00100    Loss 0.019063  0.000030\n",
            "Iteration 00101    Loss 0.018641  0.000036\n",
            "Iteration 00102    Loss 0.018243  0.000032\n",
            "Iteration 00103    Loss 0.017805  0.000035\n",
            "Iteration 00104    Loss 0.017274  0.000035\n",
            "Iteration 00105    Loss 0.017008  0.000035\n",
            "Iteration 00106    Loss 0.016520  0.000036\n",
            "Iteration 00107    Loss 0.016235  0.000036\n",
            "Iteration 00108    Loss 0.015799  0.000038\n",
            "Iteration 00109    Loss 0.015396  0.000041\n",
            "Iteration 00110    Loss 0.014959  0.000039\n",
            "Iteration 00111    Loss 0.014563  0.000041\n",
            "Iteration 00112    Loss 0.014245  0.000045\n",
            "Iteration 00113    Loss 0.013788  0.000044\n",
            "Iteration 00114    Loss 0.013463  0.000044\n",
            "Iteration 00115    Loss 0.013086  0.000047\n",
            "Iteration 00116    Loss 0.012745  0.000049\n",
            "Iteration 00117    Loss 0.012362  0.000050\n",
            "Iteration 00118    Loss 0.011945  0.000050\n",
            "Iteration 00119    Loss 0.011566  0.000050\n",
            "Iteration 00120    Loss 0.011241  0.000055\n",
            "Iteration 00121    Loss 0.010843  0.000053\n",
            "Iteration 00122    Loss 0.010476  0.000057\n",
            "Iteration 00123    Loss 0.010107  0.000057\n",
            "Iteration 00124    Loss 0.009796  0.000059\n",
            "Iteration 00125    Loss 0.009426  0.000060\n",
            "Iteration 00126    Loss 0.009146  0.000060\n",
            "Iteration 00127    Loss 0.008757  0.000066\n",
            "Iteration 00128    Loss 0.008325  0.000064\n",
            "Iteration 00129    Loss 0.008061  0.000071\n",
            "Iteration 00130    Loss 0.007668  0.000066\n",
            "Iteration 00131    Loss 0.007492  0.000074\n",
            "Iteration 00132    Loss 0.007027  0.000064\n",
            "Iteration 00133    Loss 0.006734  0.000079\n",
            "Iteration 00134    Loss 0.006529  0.000059\n",
            "Iteration 00135    Loss 0.006082  0.000077\n",
            "Iteration 00136    Loss 0.005790  0.000079\n",
            "Iteration 00137    Loss 0.005514  0.000068\n",
            "Iteration 00138    Loss 0.005118  0.000077\n",
            "Iteration 00139    Loss 0.004824  0.000083\n",
            "Iteration 00140    Loss 0.004577  0.000066\n",
            "Iteration 00141    Loss 0.004163  0.000082\n",
            "Iteration 00142    Loss 0.003876  0.000094\n",
            "Iteration 00143    Loss 0.003546  0.000079\n",
            "Iteration 00144    Loss 0.003227  0.000082\n",
            "Iteration 00145    Loss 0.002903  0.000090\n",
            "Iteration 00146    Loss 0.002670  0.000093\n",
            "Iteration 00147    Loss 0.002326  0.000092\n",
            "Iteration 00148    Loss 0.002060  0.000089\n",
            "Iteration 00149    Loss 0.001738  0.000091\n",
            "Iteration 00150    Loss 0.001442  0.000096\n",
            "Iteration 00151    Loss 0.001196  0.000095\n",
            "Iteration 00152    Loss 0.001048  0.000100\n",
            "Iteration 00153    Loss 0.000721  0.000097\n",
            "Iteration 00154    Loss 0.000393  0.000100\n",
            "Iteration 00155    Loss 0.000070  0.000106\n",
            "Iteration 00156    Loss -0.000231  0.000097\n",
            "Iteration 00157    Loss -0.000432  0.000103\n",
            "Iteration 00158    Loss -0.000673  0.000111\n",
            "Iteration 00159    Loss -0.001064  0.000103\n",
            "Iteration 00160    Loss -0.001261  0.000101\n",
            "Iteration 00161    Loss -0.001533  0.000104\n",
            "Iteration 00162    Loss -0.001851  0.000116\n",
            "Iteration 00163    Loss -0.002010  0.000099\n",
            "Iteration 00164    Loss -0.002324  0.000111\n",
            "Iteration 00165    Loss -0.002581  0.000108\n",
            "Iteration 00166    Loss -0.002891  0.000111\n",
            "Iteration 00167    Loss -0.003211  0.000113\n",
            "Iteration 00168    Loss -0.003387  0.000109\n",
            "Iteration 00169    Loss -0.003631  0.000114\n",
            "Iteration 00170    Loss -0.003890  0.000109\n",
            "Iteration 00171    Loss -0.004256  0.000123\n",
            "Iteration 00172    Loss -0.004447  0.000120\n",
            "Iteration 00173    Loss -0.004764  0.000109\n",
            "Iteration 00174    Loss -0.004966  0.000121\n",
            "Iteration 00175    Loss -0.005195  0.000120\n",
            "Iteration 00176    Loss -0.005404  0.000123\n",
            "Iteration 00177    Loss -0.005752  0.000120\n",
            "Iteration 00178    Loss -0.005879  0.000124\n",
            "Iteration 00179    Loss -0.006159  0.000119\n",
            "Iteration 00180    Loss -0.006508  0.000115\n",
            "Iteration 00181    Loss -0.006700  0.000135\n",
            "Iteration 00182    Loss -0.006908  0.000129\n",
            "Iteration 00183    Loss -0.007166  0.000129\n",
            "Iteration 00184    Loss -0.007390  0.000130\n",
            "Iteration 00185    Loss -0.007669  0.000134\n",
            "Iteration 00186    Loss -0.007839  0.000138\n",
            "Iteration 00187    Loss -0.008120  0.000114\n",
            "Iteration 00188    Loss -0.008143  0.000142\n",
            "Iteration 00189    Loss -0.008490  0.000110\n",
            "Iteration 00190    Loss -0.008816  0.000133\n",
            "Iteration 00191    Loss -0.008901  0.000127\n",
            "Iteration 00192    Loss -0.009072  0.000110\n",
            "Iteration 00193    Loss -0.009489  0.000128\n",
            "Iteration 00194    Loss -0.009658  0.000135\n",
            "Iteration 00195    Loss -0.009844  0.000129\n",
            "Iteration 00196    Loss -0.010090  0.000130\n",
            "Iteration 00197    Loss -0.010210  0.000133\n",
            "Iteration 00198    Loss -0.010438  0.000134\n",
            "Iteration 00199    Loss -0.010654  0.000124\n",
            "Iteration 00200    Loss -0.010777  0.000138\n",
            "Iteration 00201    Loss -0.011097  0.000140\n",
            "Iteration 00202    Loss -0.011214  0.000124\n",
            "Iteration 00203    Loss -0.011400  0.000126\n",
            "Iteration 00204    Loss -0.011555  0.000135\n",
            "Iteration 00205    Loss -0.011863  0.000125\n",
            "Iteration 00206    Loss -0.012004  0.000138\n",
            "Iteration 00207    Loss -0.012126  0.000136\n",
            "Iteration 00208    Loss -0.012338  0.000146\n",
            "Iteration 00209    Loss -0.012539  0.000125\n",
            "Iteration 00210    Loss -0.012807  0.000138\n",
            "Iteration 00211    Loss -0.012938  0.000142\n",
            "Iteration 00212    Loss -0.013031  0.000134\n",
            "Iteration 00213    Loss -0.013297  0.000146\n",
            "Iteration 00214    Loss -0.013492  0.000143\n",
            "Iteration 00215    Loss -0.013680  0.000137\n",
            "Iteration 00216    Loss -0.013800  0.000135\n",
            "Iteration 00217    Loss -0.014056  0.000143\n",
            "Iteration 00218    Loss -0.014157  0.000147\n",
            "Iteration 00219    Loss -0.014500  0.000141\n",
            "Iteration 00220    Loss -0.014510  0.000144\n",
            "Iteration 00221    Loss -0.014678  0.000142\n",
            "Iteration 00222    Loss -0.014925  0.000151\n",
            "Iteration 00223    Loss -0.014959  0.000134\n",
            "Iteration 00224    Loss -0.015094  0.000157\n",
            "Iteration 00225    Loss -0.015228  0.000119\n",
            "Iteration 00226    Loss -0.015449  0.000145\n",
            "Iteration 00227    Loss -0.015648  0.000155\n",
            "Iteration 00228    Loss -0.015819  0.000128\n",
            "Iteration 00229    Loss -0.016039  0.000123\n",
            "Iteration 00230    Loss -0.016190  0.000139\n",
            "Iteration 00231    Loss -0.016303  0.000138\n",
            "Iteration 00232    Loss -0.016441  0.000135\n",
            "Iteration 00233    Loss -0.016551  0.000137\n",
            "Iteration 00234    Loss -0.016757  0.000133\n",
            "Iteration 00235    Loss -0.017017  0.000146\n",
            "Iteration 00236    Loss -0.017147  0.000149\n",
            "Iteration 00237    Loss -0.017281  0.000141\n",
            "Iteration 00238    Loss -0.017530  0.000151\n",
            "Iteration 00239    Loss -0.017613  0.000153\n",
            "Iteration 00240    Loss -0.017800  0.000133\n",
            "Iteration 00241    Loss -0.017885  0.000141\n",
            "Iteration 00242    Loss -0.018067  0.000150\n",
            "Iteration 00243    Loss -0.018190  0.000138\n",
            "Iteration 00244    Loss -0.018436  0.000149\n",
            "Iteration 00245    Loss -0.018448  0.000144\n",
            "Iteration 00246    Loss -0.018704  0.000140\n",
            "Iteration 00247    Loss -0.018840  0.000144\n",
            "Iteration 00248    Loss -0.019030  0.000151\n",
            "Iteration 00249    Loss -0.019099  0.000144\n",
            "Iteration 00250    Loss -0.019263  0.000150\n",
            "Iteration 00251    Loss -0.019290  0.000162\n",
            "Iteration 00252    Loss -0.019303  0.000145\n",
            "Iteration 00253    Loss -0.019647  0.000153\n",
            "Iteration 00254    Loss -0.019708  0.000127\n",
            "Iteration 00255    Loss -0.020044  0.000145\n",
            "Iteration 00256    Loss -0.020135  0.000151\n",
            "Iteration 00257    Loss -0.020133  0.000145\n",
            "Iteration 00258    Loss -0.020403  0.000155\n",
            "Iteration 00259    Loss -0.020444  0.000141\n",
            "Iteration 00260    Loss -0.020640  0.000150\n",
            "Iteration 00261    Loss -0.020736  0.000154\n",
            "Iteration 00262    Loss -0.021087  0.000145\n",
            "Iteration 00263    Loss -0.021070  0.000154\n",
            "Iteration 00264    Loss -0.020984  0.000151\n",
            "Iteration 00265    Loss -0.021265  0.000158\n",
            "Iteration 00266    Loss -0.021534  0.000151\n",
            "Iteration 00267    Loss -0.021536  0.000135\n",
            "Iteration 00268    Loss -0.021703  0.000156\n",
            "Iteration 00269    Loss -0.021709  0.000130\n",
            "Iteration 00270    Loss -0.021804  0.000151\n",
            "Iteration 00271    Loss -0.022280  0.000163\n",
            "Iteration 00272    Loss -0.022137  0.000135\n",
            "Iteration 00273    Loss -0.022375  0.000148\n",
            "Iteration 00274    Loss -0.022568  0.000148\n",
            "Iteration 00275    Loss -0.022269  0.000138\n",
            "Iteration 00276    Loss -0.022683  0.000146\n",
            "Iteration 00277    Loss -0.022780  0.000156\n",
            "Iteration 00278    Loss -0.022934  0.000143\n",
            "Iteration 00279    Loss -0.022738  0.000141\n",
            "Iteration 00280    Loss -0.023306  0.000159\n",
            "Iteration 00281    Loss -0.023245  0.000138\n",
            "Iteration 00282    Loss -0.023533  0.000148\n",
            "Iteration 00283    Loss -0.023521  0.000150\n",
            "Iteration 00284    Loss -0.023610  0.000162\n",
            "Iteration 00285    Loss -0.023840  0.000131\n",
            "Iteration 00286    Loss -0.023966  0.000156\n",
            "Iteration 00287    Loss -0.024310  0.000151\n",
            "Iteration 00288    Loss -0.024298  0.000133\n",
            "Iteration 00289    Loss -0.024262  0.000144\n",
            "Iteration 00290    Loss -0.024695  0.000151\n",
            "Iteration 00291    Loss -0.024553  0.000145\n",
            "Iteration 00292    Loss -0.024935  0.000132\n",
            "Iteration 00293    Loss -0.024804  0.000142\n",
            "Iteration 00294    Loss -0.024952  0.000156\n",
            "Iteration 00295    Loss -0.024674  0.000138\n",
            "Iteration 00296    Loss -0.025443  0.000143\n",
            "Iteration 00297    Loss -0.025366  0.000148\n",
            "Iteration 00298    Loss -0.025228  0.000143\n",
            "Iteration 00299    Loss -0.025553  0.000141\n",
            "Iteration 00300    Loss -0.025681  0.000147\n",
            "Iteration 00301    Loss -0.025997  0.000148\n",
            "Iteration 00302    Loss -0.025233  0.000140\n",
            "Iteration 00303    Loss -0.025851  0.000129\n",
            "Iteration 00304    Loss -0.025652  0.000145\n",
            "Iteration 00305    Loss -0.025540  0.000136\n",
            "Iteration 00306    Loss -0.025868  0.000140\n",
            "Iteration 00307    Loss -0.026205  0.000148\n",
            "Iteration 00308    Loss -0.026470  0.000138\n",
            "Iteration 00309    Loss -0.026125  0.000143\n",
            "Iteration 00310    Loss -0.026546  0.000138\n",
            "Iteration 00311    Loss -0.026776  0.000140\n",
            "Iteration 00312    Loss -0.026683  0.000143\n",
            "Iteration 00313    Loss -0.027121  0.000138\n",
            "Iteration 00314    Loss -0.026972  0.000140\n",
            "Iteration 00315    Loss -0.027305  0.000147\n",
            "Iteration 00316    Loss -0.027426  0.000149\n",
            "Iteration 00317    Loss -0.027605  0.000141\n",
            "Iteration 00318    Loss -0.027660  0.000149\n",
            "Iteration 00319    Loss -0.027602  0.000153\n",
            "Iteration 00320    Loss -0.027648  0.000147\n",
            "Iteration 00321    Loss -0.027854  0.000145\n",
            "Iteration 00322    Loss -0.027962  0.000136\n",
            "Iteration 00323    Loss -0.028313  0.000137\n",
            "Iteration 00324    Loss -0.028351  0.000131\n",
            "Iteration 00325    Loss -0.028342  0.000132\n",
            "Iteration 00326    Loss -0.028393  0.000143\n",
            "Iteration 00327    Loss -0.028083  0.000135\n",
            "Iteration 00328    Loss -0.028514  0.000132\n",
            "Iteration 00329    Loss -0.028692  0.000148\n",
            "Iteration 00330    Loss -0.028996  0.000142\n",
            "Iteration 00331    Loss -0.028927  0.000140\n",
            "Iteration 00332    Loss -0.028972  0.000155\n",
            "Iteration 00333    Loss -0.028905  0.000149\n",
            "Iteration 00334    Loss -0.029349  0.000136\n",
            "Iteration 00335    Loss -0.029056  0.000147\n",
            "Iteration 00336    Loss -0.029566  0.000147\n",
            "Iteration 00337    Loss -0.029494  0.000140\n",
            "Iteration 00338    Loss -0.029668  0.000146\n",
            "Iteration 00339    Loss -0.029841  0.000140\n",
            "Iteration 00340    Loss -0.030018  0.000146\n",
            "Iteration 00341    Loss -0.030039  0.000158\n",
            "Iteration 00342    Loss -0.029610  0.000148\n",
            "Iteration 00343    Loss -0.030109  0.000141\n",
            "Iteration 00344    Loss -0.030410  0.000150\n",
            "Iteration 00345    Loss -0.030443  0.000135\n",
            "Iteration 00346    Loss -0.030607  0.000134\n",
            "Iteration 00347    Loss -0.030467  0.000147\n",
            "Iteration 00348    Loss -0.030582  0.000151\n",
            "Iteration 00349    Loss -0.030545  0.000145\n",
            "Iteration 00350    Loss -0.030916  0.000139\n",
            "Iteration 00351    Loss -0.030809  0.000155\n",
            "Iteration 00352    Loss -0.030996  0.000138\n",
            "Iteration 00353    Loss -0.030858  0.000145\n",
            "Iteration 00354    Loss -0.030904  0.000142\n",
            "Iteration 00355    Loss -0.031120  0.000141\n",
            "Iteration 00356    Loss -0.031446  0.000142\n",
            "Iteration 00357    Loss -0.031539  0.000140\n",
            "Iteration 00358    Loss -0.031529  0.000148\n",
            "Iteration 00359    Loss -0.031275  0.000142\n",
            "Iteration 00360    Loss -0.031776  0.000144\n",
            "Iteration 00361    Loss -0.031830  0.000144\n",
            "Iteration 00362    Loss -0.031833  0.000143\n",
            "Iteration 00363    Loss -0.032010  0.000139\n",
            "Iteration 00364    Loss -0.031813  0.000143\n",
            "Iteration 00365    Loss -0.031979  0.000140\n",
            "Iteration 00366    Loss -0.032175  0.000147\n",
            "Iteration 00367    Loss -0.032312  0.000135\n",
            "Iteration 00368    Loss -0.032482  0.000142\n",
            "Iteration 00369    Loss -0.032424  0.000144\n",
            "Iteration 00370    Loss -0.032655  0.000143\n",
            "Iteration 00371    Loss -0.032718  0.000140\n",
            "Iteration 00372    Loss -0.032729  0.000145\n",
            "Iteration 00373    Loss -0.032768  0.000137\n",
            "Iteration 00374    Loss -0.032931  0.000143\n",
            "Iteration 00375    Loss -0.032988  0.000130\n",
            "Iteration 00376    Loss -0.032854  0.000142\n",
            "Iteration 00377    Loss -0.033165  0.000130\n",
            "Iteration 00378    Loss -0.033125  0.000135\n",
            "Iteration 00379    Loss -0.033375  0.000151\n",
            "Iteration 00380    Loss -0.033347  0.000139\n",
            "Iteration 00381    Loss -0.033406  0.000137\n",
            "Iteration 00382    Loss -0.033302  0.000146\n",
            "Iteration 00383    Loss -0.033571  0.000138\n",
            "Iteration 00384    Loss -0.033690  0.000137\n",
            "Iteration 00385    Loss -0.033634  0.000138\n",
            "Iteration 00386    Loss -0.033831  0.000138\n",
            "Iteration 00387    Loss -0.033698  0.000137\n",
            "Iteration 00388    Loss -0.033923  0.000136\n",
            "Iteration 00389    Loss -0.034084  0.000146\n",
            "Iteration 00390    Loss -0.034039  0.000133\n",
            "Iteration 00391    Loss -0.033973  0.000138\n",
            "Iteration 00392    Loss -0.034059  0.000155\n",
            "Iteration 00393    Loss -0.034222  0.000129\n",
            "Iteration 00394    Loss -0.034145  0.000142\n",
            "Iteration 00395    Loss -0.034407  0.000150\n",
            "Iteration 00396    Loss -0.034133  0.000126\n",
            "Iteration 00397    Loss -0.034585  0.000139\n",
            "Iteration 00398    Loss -0.034377  0.000142\n",
            "Iteration 00399    Loss -0.034720  0.000132\n",
            "Iteration 00400    Loss -0.034712  0.000142\n",
            "Iteration 00401    Loss -0.034798  0.000138\n",
            "Iteration 00402    Loss -0.034827  0.000145\n",
            "Iteration 00403    Loss -0.034994  0.000132\n",
            "Iteration 00404    Loss -0.034549  0.000134\n",
            "Iteration 00405    Loss -0.035091  0.000143\n",
            "Iteration 00406    Loss -0.034488  0.000133\n",
            "Iteration 00407    Loss -0.035141  0.000129\n",
            "Iteration 00408    Loss -0.035353  0.000135\n",
            "Iteration 00409    Loss -0.035273  0.000140\n",
            "Iteration 00410    Loss -0.035326  0.000137\n",
            "Iteration 00411    Loss -0.035364  0.000134\n",
            "Iteration 00412    Loss -0.035589  0.000143\n",
            "Iteration 00413    Loss -0.035698  0.000146\n",
            "Iteration 00414    Loss -0.035633  0.000128\n",
            "Iteration 00415    Loss -0.035623  0.000132\n",
            "Iteration 00416    Loss -0.035386  0.000136\n",
            "Iteration 00417    Loss -0.035461  0.000139\n",
            "Iteration 00418    Loss -0.035817  0.000135\n",
            "Iteration 00419    Loss -0.035829  0.000140\n",
            "Iteration 00420    Loss -0.036088  0.000145\n",
            "Iteration 00421    Loss -0.036086  0.000128\n",
            "Iteration 00422    Loss -0.036090  0.000126\n",
            "Iteration 00423    Loss -0.036220  0.000152\n",
            "Iteration 00424    Loss -0.036285  0.000123\n",
            "Iteration 00425    Loss -0.036372  0.000132\n",
            "Iteration 00426    Loss -0.036413  0.000141\n",
            "Iteration 00427    Loss -0.036469  0.000135\n",
            "Iteration 00428    Loss -0.036452  0.000126\n",
            "Iteration 00429    Loss -0.036603  0.000138\n",
            "Iteration 00430    Loss -0.036663  0.000140\n",
            "Iteration 00431    Loss -0.036691  0.000131\n",
            "Iteration 00432    Loss -0.036684  0.000133\n",
            "Iteration 00433    Loss -0.036790  0.000137\n",
            "Iteration 00434    Loss -0.036744  0.000137\n",
            "Iteration 00435    Loss -0.036985  0.000118\n",
            "Iteration 00436    Loss -0.036779  0.000137\n",
            "Iteration 00437    Loss -0.036982  0.000141\n",
            "Iteration 00438    Loss -0.037017  0.000122\n",
            "Iteration 00439    Loss -0.037156  0.000128\n",
            "Iteration 00440    Loss -0.037206  0.000129\n",
            "Iteration 00441    Loss -0.037186  0.000135\n",
            "Iteration 00442    Loss -0.037431  0.000138\n",
            "Iteration 00443    Loss -0.037470  0.000138\n",
            "Iteration 00444    Loss -0.037522  0.000141\n",
            "Iteration 00445    Loss -0.037283  0.000135\n",
            "Iteration 00446    Loss -0.037618  0.000129\n",
            "Iteration 00447    Loss -0.037601  0.000146\n",
            "Iteration 00448    Loss -0.037760  0.000125\n",
            "Iteration 00449    Loss -0.037819  0.000136\n",
            "Iteration 00450    Loss -0.037624  0.000151\n",
            "Iteration 00451    Loss -0.037814  0.000124\n",
            "Iteration 00452    Loss -0.037943  0.000132\n",
            "Iteration 00453    Loss -0.037522  0.000142\n",
            "Iteration 00454    Loss -0.037748  0.000128\n",
            "Iteration 00455    Loss -0.038050  0.000134\n",
            "Iteration 00456    Loss -0.038137  0.000142\n",
            "Iteration 00457    Loss -0.038193  0.000148\n",
            "Iteration 00458    Loss -0.038227  0.000137\n",
            "Iteration 00459    Loss -0.038366  0.000131\n",
            "Iteration 00460    Loss -0.038267  0.000138\n",
            "Iteration 00461    Loss -0.038424  0.000144\n",
            "Iteration 00462    Loss -0.038361  0.000136\n",
            "Iteration 00463    Loss -0.037788  0.000132\n",
            "Iteration 00464    Loss -0.038626  0.000143\n",
            "Iteration 00465    Loss -0.038382  0.000150\n",
            "Iteration 00466    Loss -0.038261  0.000127\n",
            "Iteration 00467    Loss -0.038744  0.000137\n",
            "Iteration 00468    Loss -0.038775  0.000144\n",
            "Iteration 00469    Loss -0.038725  0.000141\n",
            "Iteration 00470    Loss -0.038672  0.000143\n",
            "Iteration 00471    Loss -0.038872  0.000134\n",
            "Iteration 00472    Loss -0.039001  0.000137\n",
            "Iteration 00473    Loss -0.039022  0.000138\n",
            "Iteration 00474    Loss -0.039025  0.000138\n",
            "Iteration 00475    Loss -0.039171  0.000143\n",
            "Iteration 00476    Loss -0.039177  0.000140\n",
            "Iteration 00477    Loss -0.039113  0.000139\n",
            "Iteration 00478    Loss -0.039263  0.000143\n",
            "Iteration 00479    Loss -0.039268  0.000141\n",
            "Iteration 00480    Loss -0.039136  0.000141\n",
            "Iteration 00481    Loss -0.039351  0.000136\n",
            "Iteration 00482    Loss -0.039302  0.000138\n",
            "Iteration 00483    Loss -0.039524  0.000147\n",
            "Iteration 00484    Loss -0.039511  0.000150\n",
            "Iteration 00485    Loss -0.039332  0.000130\n",
            "Iteration 00486    Loss -0.039657  0.000144\n",
            "Iteration 00487    Loss -0.039556  0.000145\n",
            "Iteration 00488    Loss -0.039642  0.000140\n",
            "Iteration 00489    Loss -0.039817  0.000142\n",
            "Iteration 00490    Loss -0.039808  0.000142\n",
            "Iteration 00491    Loss -0.039771  0.000141\n",
            "Iteration 00492    Loss -0.039791  0.000139\n",
            "Iteration 00493    Loss -0.039839  0.000141\n",
            "Iteration 00494    Loss -0.039983  0.000147\n",
            "Iteration 00495    Loss -0.039126  0.000131\n",
            "Iteration 00496    Loss -0.039904  0.000127\n",
            "Iteration 00497    Loss -0.039408  0.000134\n",
            "Iteration 00498    Loss -0.040007  0.000131\n",
            "Iteration 00499    Loss -0.039951  0.000111\n",
            " 99% 495/500 [4:51:54<02:57, 35.54s/it]/content/SOTS/indoor/hazy/1438_10.png\n",
            "Iteration 00000    Loss 0.457294  0.000022\n",
            "Iteration 00001    Loss 31.712614  0.000007\n",
            "Iteration 00002    Loss 0.409947  0.000004\n",
            "Iteration 00003    Loss 0.368286  0.000002\n",
            "Iteration 00004    Loss 0.898166  0.000001\n",
            "Iteration 00005    Loss 0.224020  0.000001\n",
            "Iteration 00006    Loss 0.204932  0.000001\n",
            "Iteration 00007    Loss 0.176802  0.000001\n",
            "Iteration 00008    Loss 0.161542  0.000001\n",
            "Iteration 00009    Loss 0.146441  0.000001\n",
            "Iteration 00010    Loss 0.134996  0.000001\n",
            "Iteration 00011    Loss 0.123950  0.000001\n",
            "Iteration 00012    Loss 0.117655  0.000001\n",
            "Iteration 00013    Loss 0.114549  0.000002\n",
            "Iteration 00014    Loss 0.109105  0.000003\n",
            "Iteration 00015    Loss 0.104055  0.000002\n",
            "Iteration 00016    Loss 0.099663  0.000002\n",
            "Iteration 00017    Loss 0.098260  0.000002\n",
            "Iteration 00018    Loss 0.093677  0.000003\n",
            "Iteration 00019    Loss 0.091414  0.000003\n",
            "Iteration 00020    Loss 0.090042  0.000004\n",
            "Iteration 00021    Loss 0.087318  0.000005\n",
            "Iteration 00022    Loss 0.084699  0.000006\n",
            "Iteration 00023    Loss 0.082672  0.000008\n",
            "Iteration 00024    Loss 0.081260  0.000009\n",
            "Iteration 00025    Loss 0.079639  0.000009\n",
            "Iteration 00026    Loss 0.078477  0.000009\n",
            "Iteration 00027    Loss 0.076495  0.000010\n",
            "Iteration 00028    Loss 0.075084  0.000012\n",
            "Iteration 00029    Loss 0.073560  0.000013\n",
            "Iteration 00030    Loss 0.072437  0.000013\n",
            "Iteration 00031    Loss 0.070915  0.000014\n",
            "Iteration 00032    Loss 0.069708  0.000014\n",
            "Iteration 00033    Loss 0.068414  0.000013\n",
            "Iteration 00034    Loss 0.067230  0.000014\n",
            "Iteration 00035    Loss 0.065962  0.000016\n",
            "Iteration 00036    Loss 0.064643  0.000018\n",
            "Iteration 00037    Loss 0.063344  0.000018\n",
            "Iteration 00038    Loss 0.062482  0.000018\n",
            "Iteration 00039    Loss 0.061198  0.000020\n",
            "Iteration 00040    Loss 0.059817  0.000022\n",
            "Iteration 00041    Loss 0.058834  0.000024\n",
            "Iteration 00042    Loss 0.058211  0.000025\n",
            "Iteration 00043    Loss 0.056814  0.000029\n",
            "Iteration 00044    Loss 0.056213  0.000029\n",
            "Iteration 00045    Loss 0.054746  0.000031\n",
            "Iteration 00046    Loss 0.053535  0.000032\n",
            "Iteration 00047    Loss 0.052613  0.000034\n",
            "Iteration 00048    Loss 0.051634  0.000036\n",
            "Iteration 00049    Loss 0.050649  0.000034\n",
            "Iteration 00050    Loss 0.049298  0.000038\n",
            "Iteration 00051    Loss 0.048448  0.000040\n",
            "Iteration 00052    Loss 0.047162  0.000041\n",
            "Iteration 00053    Loss 0.046381  0.000045\n",
            "Iteration 00054    Loss 0.045429  0.000047\n",
            "Iteration 00055    Loss 0.044425  0.000047\n",
            "Iteration 00056    Loss 0.043477  0.000050\n",
            "Iteration 00057    Loss 0.042701  0.000052\n",
            "Iteration 00058    Loss 0.041325  0.000054\n",
            "Iteration 00059    Loss 0.041320  0.000055\n",
            "Iteration 00060    Loss 0.040059  0.000054\n",
            "Iteration 00061    Loss 0.039116  0.000057\n",
            "Iteration 00062    Loss 0.038110  0.000059\n",
            "Iteration 00063    Loss 0.037245  0.000056\n",
            "Iteration 00064    Loss 0.036892  0.000059\n",
            "Iteration 00065    Loss 0.035461  0.000061\n",
            "Iteration 00066    Loss 0.034889  0.000069\n",
            "Iteration 00067    Loss 0.034553  0.000062\n",
            "Iteration 00068    Loss 0.033517  0.000063\n",
            "Iteration 00069    Loss 0.032766  0.000071\n",
            "Iteration 00070    Loss 0.032151  0.000072\n",
            "Iteration 00071    Loss 0.031189  0.000069\n",
            "Iteration 00072    Loss 0.030486  0.000066\n",
            "Iteration 00073    Loss 0.029598  0.000067\n",
            "Iteration 00074    Loss 0.028926  0.000078\n",
            "Iteration 00075    Loss 0.028401  0.000079\n",
            "Iteration 00076    Loss 0.027498  0.000079\n",
            "Iteration 00077    Loss 0.026971  0.000081\n",
            "Iteration 00078    Loss 0.026061  0.000085\n",
            "Iteration 00079    Loss 0.025720  0.000086\n",
            "Iteration 00080    Loss 0.024893  0.000086\n",
            "Iteration 00081    Loss 0.024313  0.000085\n",
            "Iteration 00082    Loss 0.023531  0.000092\n",
            "Iteration 00083    Loss 0.023162  0.000093\n",
            "Iteration 00084    Loss 0.022475  0.000100\n",
            "Iteration 00085    Loss 0.021885  0.000086\n",
            "Iteration 00086    Loss 0.021347  0.000097\n",
            "Iteration 00087    Loss 0.020530  0.000102\n",
            "Iteration 00088    Loss 0.020276  0.000097\n",
            "Iteration 00089    Loss 0.019768  0.000102\n",
            "Iteration 00090    Loss 0.018780  0.000102\n",
            "Iteration 00091    Loss 0.018698  0.000100\n",
            "Iteration 00092    Loss 0.017962  0.000107\n",
            "Iteration 00093    Loss 0.017211  0.000108\n",
            "Iteration 00094    Loss 0.016842  0.000101\n",
            "Iteration 00095    Loss 0.016033  0.000112\n",
            "Iteration 00096    Loss 0.015684  0.000117\n",
            "Iteration 00097    Loss 0.014972  0.000107\n",
            "Iteration 00098    Loss 0.014552  0.000111\n",
            "Iteration 00099    Loss 0.014020  0.000119\n",
            "Iteration 00100    Loss 0.013511  0.000116\n",
            "Iteration 00101    Loss 0.012975  0.000114\n",
            "Iteration 00102    Loss 0.012334  0.000118\n",
            "Iteration 00103    Loss 0.011887  0.000116\n",
            "Iteration 00104    Loss 0.011326  0.000117\n",
            "Iteration 00105    Loss 0.010909  0.000123\n",
            "Iteration 00106    Loss 0.010407  0.000126\n",
            "Iteration 00107    Loss 0.009829  0.000124\n",
            "Iteration 00108    Loss 0.009471  0.000130\n",
            "Iteration 00109    Loss 0.008963  0.000129\n",
            "Iteration 00110    Loss 0.008474  0.000126\n",
            "Iteration 00111    Loss 0.007946  0.000129\n",
            "Iteration 00112    Loss 0.007555  0.000136\n",
            "Iteration 00113    Loss 0.007147  0.000134\n",
            "Iteration 00114    Loss 0.006642  0.000133\n",
            "Iteration 00115    Loss 0.006211  0.000137\n",
            "Iteration 00116    Loss 0.005713  0.000143\n",
            "Iteration 00117    Loss 0.005442  0.000143\n",
            "Iteration 00118    Loss 0.004987  0.000143\n",
            "Iteration 00119    Loss 0.004585  0.000139\n",
            "Iteration 00120    Loss 0.004116  0.000144\n",
            "Iteration 00121    Loss 0.003632  0.000146\n",
            "Iteration 00122    Loss 0.003356  0.000153\n",
            "Iteration 00123    Loss 0.002788  0.000146\n",
            "Iteration 00124    Loss 0.002338  0.000145\n",
            "Iteration 00125    Loss 0.002033  0.000155\n",
            "Iteration 00126    Loss 0.001662  0.000150\n",
            "Iteration 00127    Loss 0.001200  0.000153\n",
            "Iteration 00128    Loss 0.000922  0.000154\n",
            "Iteration 00129    Loss 0.000489  0.000155\n",
            "Iteration 00130    Loss 0.000095  0.000157\n",
            "Iteration 00131    Loss -0.000277  0.000158\n",
            "Iteration 00132    Loss -0.000560  0.000167\n",
            "Iteration 00133    Loss -0.001121  0.000163\n",
            "Iteration 00134    Loss -0.001535  0.000159\n",
            "Iteration 00135    Loss -0.001796  0.000168\n",
            "Iteration 00136    Loss -0.002062  0.000161\n",
            "Iteration 00137    Loss -0.002500  0.000162\n",
            "Iteration 00138    Loss -0.002961  0.000167\n",
            "Iteration 00139    Loss -0.003233  0.000172\n",
            "Iteration 00140    Loss -0.003481  0.000162\n",
            "Iteration 00141    Loss -0.003742  0.000170\n",
            "Iteration 00142    Loss -0.004167  0.000183\n",
            "Iteration 00143    Loss -0.004493  0.000170\n",
            "Iteration 00144    Loss -0.004825  0.000162\n",
            "Iteration 00145    Loss -0.005240  0.000171\n",
            "Iteration 00146    Loss -0.005586  0.000175\n",
            "Iteration 00147    Loss -0.005932  0.000182\n",
            "Iteration 00148    Loss -0.006179  0.000179\n",
            "Iteration 00149    Loss -0.006564  0.000181\n",
            "Iteration 00150    Loss -0.006817  0.000183\n",
            "Iteration 00151    Loss -0.007252  0.000181\n",
            "Iteration 00152    Loss -0.007425  0.000192\n",
            "Iteration 00153    Loss -0.007669  0.000190\n",
            "Iteration 00154    Loss -0.008027  0.000192\n",
            "Iteration 00155    Loss -0.008355  0.000184\n",
            "Iteration 00156    Loss -0.008692  0.000195\n",
            "Iteration 00157    Loss -0.009005  0.000193\n",
            "Iteration 00158    Loss -0.009270  0.000200\n",
            "Iteration 00159    Loss -0.009454  0.000196\n",
            "Iteration 00160    Loss -0.009790  0.000191\n",
            "Iteration 00161    Loss -0.010002  0.000200\n",
            "Iteration 00162    Loss -0.010412  0.000194\n",
            "Iteration 00163    Loss -0.010623  0.000203\n",
            "Iteration 00164    Loss -0.010971  0.000206\n",
            "Iteration 00165    Loss -0.011167  0.000198\n",
            "Iteration 00166    Loss -0.011518  0.000211\n",
            "Iteration 00167    Loss -0.011803  0.000209\n",
            "Iteration 00168    Loss -0.012077  0.000210\n",
            "Iteration 00169    Loss -0.012314  0.000211\n",
            "Iteration 00170    Loss -0.012627  0.000215\n",
            "Iteration 00171    Loss -0.012860  0.000207\n",
            "Iteration 00172    Loss -0.013165  0.000217\n",
            "Iteration 00173    Loss -0.013361  0.000216\n",
            "Iteration 00174    Loss -0.013554  0.000221\n",
            "Iteration 00175    Loss -0.013779  0.000217\n",
            "Iteration 00176    Loss -0.013944  0.000219\n",
            "Iteration 00177    Loss -0.014239  0.000217\n",
            "Iteration 00178    Loss -0.014570  0.000217\n",
            "Iteration 00179    Loss -0.014673  0.000223\n",
            "Iteration 00180    Loss -0.014907  0.000209\n",
            "Iteration 00181    Loss -0.015240  0.000218\n",
            "Iteration 00182    Loss -0.015449  0.000224\n",
            "Iteration 00183    Loss -0.015751  0.000215\n",
            "Iteration 00184    Loss -0.015915  0.000212\n",
            "Iteration 00185    Loss -0.016199  0.000221\n",
            "Iteration 00186    Loss -0.016376  0.000219\n",
            "Iteration 00187    Loss -0.016609  0.000224\n",
            "Iteration 00188    Loss -0.016703  0.000219\n",
            "Iteration 00189    Loss -0.016988  0.000235\n",
            "Iteration 00190    Loss -0.017203  0.000233\n",
            "Iteration 00191    Loss -0.017555  0.000231\n",
            "Iteration 00192    Loss -0.017674  0.000229\n",
            "Iteration 00193    Loss -0.017913  0.000238\n",
            "Iteration 00194    Loss -0.018105  0.000237\n",
            "Iteration 00195    Loss -0.018372  0.000242\n",
            "Iteration 00196    Loss -0.018532  0.000236\n",
            "Iteration 00197    Loss -0.018718  0.000238\n",
            "Iteration 00198    Loss -0.018890  0.000233\n",
            "Iteration 00199    Loss -0.019111  0.000249\n",
            "Iteration 00200    Loss -0.019315  0.000243\n",
            "Iteration 00201    Loss -0.019436  0.000246\n",
            "Iteration 00202    Loss -0.019677  0.000234\n",
            "Iteration 00203    Loss -0.019834  0.000253\n",
            "Iteration 00204    Loss -0.019985  0.000251\n",
            "Iteration 00205    Loss -0.020198  0.000244\n",
            "Iteration 00206    Loss -0.020372  0.000250\n",
            "Iteration 00207    Loss -0.020535  0.000239\n",
            "Iteration 00208    Loss -0.020700  0.000254\n",
            "Iteration 00209    Loss -0.020970  0.000252\n",
            "Iteration 00210    Loss -0.021110  0.000251\n",
            "Iteration 00211    Loss -0.021309  0.000264\n",
            "Iteration 00212    Loss -0.021424  0.000266\n",
            "Iteration 00213    Loss -0.021688  0.000256\n",
            "Iteration 00214    Loss -0.021817  0.000261\n",
            "Iteration 00215    Loss -0.021982  0.000264\n",
            "Iteration 00216    Loss -0.022179  0.000264\n",
            "Iteration 00217    Loss -0.022329  0.000254\n",
            "Iteration 00218    Loss -0.022475  0.000271\n",
            "Iteration 00219    Loss -0.022691  0.000262\n",
            "Iteration 00220    Loss -0.022837  0.000276\n",
            "Iteration 00221    Loss -0.022973  0.000264\n",
            "Iteration 00222    Loss -0.023131  0.000279\n",
            "Iteration 00223    Loss -0.023206  0.000259\n",
            "Iteration 00224    Loss -0.023209  0.000276\n",
            "Iteration 00225    Loss -0.023374  0.000258\n",
            "Iteration 00226    Loss -0.023787  0.000269\n",
            "Iteration 00227    Loss -0.023743  0.000263\n",
            "Iteration 00228    Loss -0.023890  0.000252\n",
            "Iteration 00229    Loss -0.024021  0.000274\n",
            "Iteration 00230    Loss -0.024189  0.000256\n",
            "Iteration 00231    Loss -0.024400  0.000259\n",
            "Iteration 00232    Loss -0.024582  0.000268\n",
            "Iteration 00233    Loss -0.024684  0.000258\n",
            "Iteration 00234    Loss -0.024796  0.000258\n",
            "Iteration 00235    Loss -0.024948  0.000279\n",
            "Iteration 00236    Loss -0.025133  0.000257\n",
            "Iteration 00237    Loss -0.025302  0.000254\n",
            "Iteration 00238    Loss -0.025488  0.000266\n",
            "Iteration 00239    Loss -0.025636  0.000275\n",
            "Iteration 00240    Loss -0.025710  0.000286\n",
            "Iteration 00241    Loss -0.025880  0.000269\n",
            "Iteration 00242    Loss -0.026073  0.000255\n",
            "Iteration 00243    Loss -0.026168  0.000275\n",
            "Iteration 00244    Loss -0.026317  0.000283\n",
            "Iteration 00245    Loss -0.026497  0.000284\n",
            "Iteration 00246    Loss -0.026579  0.000286\n",
            "Iteration 00247    Loss -0.026748  0.000279\n",
            "Iteration 00248    Loss -0.026874  0.000290\n",
            "Iteration 00249    Loss -0.027039  0.000293\n",
            "Iteration 00250    Loss -0.027126  0.000289\n",
            "Iteration 00251    Loss -0.027273  0.000299\n",
            "Iteration 00252    Loss -0.027360  0.000297\n",
            "Iteration 00253    Loss -0.027482  0.000301\n",
            "Iteration 00254    Loss -0.027641  0.000284\n",
            "Iteration 00255    Loss -0.027763  0.000299\n",
            "Iteration 00256    Loss -0.027862  0.000297\n",
            "Iteration 00257    Loss -0.027983  0.000308\n",
            "Iteration 00258    Loss -0.028105  0.000298\n",
            "Iteration 00259    Loss -0.028215  0.000306\n",
            "Iteration 00260    Loss -0.028256  0.000300\n",
            "Iteration 00261    Loss -0.028309  0.000306\n",
            "Iteration 00262    Loss -0.028394  0.000291\n",
            "Iteration 00263    Loss -0.028573  0.000290\n",
            "Iteration 00264    Loss -0.028710  0.000303\n",
            "Iteration 00265    Loss -0.028893  0.000298\n",
            "Iteration 00266    Loss -0.028977  0.000300\n",
            "Iteration 00267    Loss -0.029054  0.000305\n",
            "Iteration 00268    Loss -0.029201  0.000296\n",
            "Iteration 00269    Loss -0.029319  0.000302\n",
            "Iteration 00270    Loss -0.029367  0.000296\n",
            "Iteration 00271    Loss -0.029445  0.000318\n",
            "Iteration 00272    Loss -0.029568  0.000294\n",
            "Iteration 00273    Loss -0.029698  0.000321\n",
            "Iteration 00274    Loss -0.029827  0.000288\n",
            "Iteration 00275    Loss -0.029957  0.000287\n",
            "Iteration 00276    Loss -0.030029  0.000316\n",
            "Iteration 00277    Loss -0.030127  0.000301\n",
            "Iteration 00278    Loss -0.030266  0.000309\n",
            "Iteration 00279    Loss -0.030402  0.000304\n",
            "Iteration 00280    Loss -0.030507  0.000303\n",
            "Iteration 00281    Loss -0.030626  0.000305\n",
            "Iteration 00282    Loss -0.030718  0.000296\n",
            "Iteration 00283    Loss -0.030817  0.000319\n",
            "Iteration 00284    Loss -0.030926  0.000312\n",
            "Iteration 00285    Loss -0.031052  0.000327\n",
            "Iteration 00286    Loss -0.031163  0.000315\n",
            "Iteration 00287    Loss -0.031251  0.000303\n",
            "Iteration 00288    Loss -0.031365  0.000312\n",
            "Iteration 00289    Loss -0.031496  0.000312\n",
            "Iteration 00290    Loss -0.031594  0.000330\n",
            "Iteration 00291    Loss -0.031629  0.000313\n",
            "Iteration 00292    Loss -0.031730  0.000334\n",
            "Iteration 00293    Loss -0.031878  0.000315\n",
            "Iteration 00294    Loss -0.031992  0.000319\n",
            "Iteration 00295    Loss -0.032073  0.000329\n",
            "Iteration 00296    Loss -0.032178  0.000322\n",
            "Iteration 00297    Loss -0.032267  0.000331\n",
            "Iteration 00298    Loss -0.032356  0.000318\n",
            "Iteration 00299    Loss -0.032485  0.000325\n",
            "Iteration 00300    Loss -0.032579  0.000325\n",
            "Iteration 00301    Loss -0.032657  0.000323\n",
            "Iteration 00302    Loss -0.032742  0.000336\n",
            "Iteration 00303    Loss -0.032806  0.000320\n",
            "Iteration 00304    Loss -0.032875  0.000334\n",
            "Iteration 00305    Loss -0.032931  0.000317\n",
            "Iteration 00306    Loss -0.033033  0.000336\n",
            "Iteration 00307    Loss -0.033166  0.000335\n",
            "Iteration 00308    Loss -0.033229  0.000328\n",
            "Iteration 00309    Loss -0.033280  0.000322\n",
            "Iteration 00310    Loss -0.033391  0.000323\n",
            "Iteration 00311    Loss -0.033464  0.000338\n",
            "Iteration 00312    Loss -0.033536  0.000321\n",
            "Iteration 00313    Loss -0.033647  0.000319\n",
            "Iteration 00314    Loss -0.033710  0.000321\n",
            "Iteration 00315    Loss -0.033809  0.000313\n",
            "Iteration 00316    Loss -0.033862  0.000329\n",
            "Iteration 00317    Loss -0.033923  0.000327\n",
            "Iteration 00318    Loss -0.034004  0.000323\n",
            "Iteration 00319    Loss -0.034092  0.000317\n",
            "Iteration 00320    Loss -0.034128  0.000331\n",
            "Iteration 00321    Loss -0.034127  0.000315\n",
            "Iteration 00322    Loss -0.034197  0.000337\n",
            "Iteration 00323    Loss -0.034310  0.000315\n",
            "Iteration 00324    Loss -0.034401  0.000328\n",
            "Iteration 00325    Loss -0.034533  0.000326\n",
            "Iteration 00326    Loss -0.034594  0.000325\n",
            "Iteration 00327    Loss -0.034613  0.000329\n",
            "Iteration 00328    Loss -0.034712  0.000318\n",
            "Iteration 00329    Loss -0.034868  0.000321\n",
            "Iteration 00330    Loss -0.034892  0.000322\n",
            "Iteration 00331    Loss -0.034909  0.000311\n",
            "Iteration 00332    Loss -0.035037  0.000328\n",
            "Iteration 00333    Loss -0.035084  0.000324\n",
            "Iteration 00334    Loss -0.035111  0.000313\n",
            "Iteration 00335    Loss -0.035200  0.000325\n",
            "Iteration 00336    Loss -0.035325  0.000326\n",
            "Iteration 00337    Loss -0.035322  0.000309\n",
            "Iteration 00338    Loss -0.035428  0.000323\n",
            "Iteration 00339    Loss -0.035468  0.000331\n",
            "Iteration 00340    Loss -0.035528  0.000310\n",
            "Iteration 00341    Loss -0.035609  0.000305\n",
            "Iteration 00342    Loss -0.035723  0.000336\n",
            "Iteration 00343    Loss -0.035767  0.000331\n",
            "Iteration 00344    Loss -0.035838  0.000315\n",
            "Iteration 00345    Loss -0.035919  0.000330\n",
            "Iteration 00346    Loss -0.035963  0.000335\n",
            "Iteration 00347    Loss -0.036034  0.000325\n",
            "Iteration 00348    Loss -0.036143  0.000327\n",
            "Iteration 00349    Loss -0.036181  0.000327\n",
            "Iteration 00350    Loss -0.036226  0.000325\n",
            "Iteration 00351    Loss -0.036320  0.000330\n",
            "Iteration 00352    Loss -0.036371  0.000342\n",
            "Iteration 00353    Loss -0.036411  0.000325\n",
            "Iteration 00354    Loss -0.036475  0.000328\n",
            "Iteration 00355    Loss -0.036561  0.000334\n",
            "Iteration 00356    Loss -0.036619  0.000344\n",
            "Iteration 00357    Loss -0.036684  0.000336\n",
            "Iteration 00358    Loss -0.036769  0.000343\n",
            "Iteration 00359    Loss -0.036821  0.000331\n",
            "Iteration 00360    Loss -0.036836  0.000342\n",
            "Iteration 00361    Loss -0.036887  0.000335\n",
            "Iteration 00362    Loss -0.036905  0.000336\n",
            "Iteration 00363    Loss -0.036974  0.000334\n",
            "Iteration 00364    Loss -0.037092  0.000336\n",
            "Iteration 00365    Loss -0.037149  0.000333\n",
            "Iteration 00366    Loss -0.037191  0.000331\n",
            "Iteration 00367    Loss -0.037251  0.000338\n",
            "Iteration 00368    Loss -0.037323  0.000334\n",
            "Iteration 00369    Loss -0.037389  0.000344\n",
            "Iteration 00370    Loss -0.037435  0.000338\n",
            "Iteration 00371    Loss -0.037483  0.000335\n",
            "Iteration 00372    Loss -0.037538  0.000343\n",
            "Iteration 00373    Loss -0.037591  0.000344\n",
            "Iteration 00374    Loss -0.037622  0.000323\n",
            "Iteration 00375    Loss -0.037665  0.000352\n",
            "Iteration 00376    Loss -0.037683  0.000332\n",
            "Iteration 00377    Loss -0.037810  0.000334\n",
            "Iteration 00378    Loss -0.037871  0.000341\n",
            "Iteration 00379    Loss -0.037883  0.000332\n",
            "Iteration 00380    Loss -0.038004  0.000335\n",
            "Iteration 00381    Loss -0.038066  0.000332\n",
            "Iteration 00382    Loss -0.038106  0.000337\n",
            "Iteration 00383    Loss -0.038150  0.000335\n",
            "Iteration 00384    Loss -0.038226  0.000329\n",
            "Iteration 00385    Loss -0.038291  0.000353\n",
            "Iteration 00386    Loss -0.038335  0.000348\n",
            "Iteration 00387    Loss -0.038359  0.000326\n",
            "Iteration 00388    Loss -0.038378  0.000350\n",
            "Iteration 00389    Loss -0.038393  0.000342\n",
            "Iteration 00390    Loss -0.038368  0.000329\n",
            "Iteration 00391    Loss -0.038336  0.000363\n",
            "Iteration 00392    Loss -0.038411  0.000330\n",
            "Iteration 00393    Loss -0.038546  0.000339\n",
            "Iteration 00394    Loss -0.038528  0.000328\n",
            "Iteration 00395    Loss -0.038666  0.000321\n",
            "Iteration 00396    Loss -0.038738  0.000326\n",
            "Iteration 00397    Loss -0.038775  0.000328\n",
            "Iteration 00398    Loss -0.038861  0.000336\n",
            "Iteration 00399    Loss -0.038941  0.000324\n",
            "Iteration 00400    Loss -0.038983  0.000330\n",
            "Iteration 00401    Loss -0.039030  0.000340\n",
            "Iteration 00402    Loss -0.039103  0.000329\n",
            "Iteration 00403    Loss -0.039132  0.000331\n",
            "Iteration 00404    Loss -0.039210  0.000340\n",
            "Iteration 00405    Loss -0.039237  0.000347\n",
            "Iteration 00406    Loss -0.039297  0.000334\n",
            "Iteration 00407    Loss -0.039360  0.000342\n",
            "Iteration 00408    Loss -0.039406  0.000356\n",
            "Iteration 00409    Loss -0.039442  0.000340\n",
            "Iteration 00410    Loss -0.039485  0.000359\n",
            "Iteration 00411    Loss -0.039526  0.000354\n",
            "Iteration 00412    Loss -0.039596  0.000360\n",
            "Iteration 00413    Loss -0.039622  0.000345\n",
            "Iteration 00414    Loss -0.039683  0.000361\n",
            "Iteration 00415    Loss -0.039748  0.000357\n",
            "Iteration 00416    Loss -0.039784  0.000355\n",
            "Iteration 00417    Loss -0.039832  0.000369\n",
            "Iteration 00418    Loss -0.039852  0.000353\n",
            "Iteration 00419    Loss -0.039893  0.000364\n",
            "Iteration 00420    Loss -0.039914  0.000363\n",
            "Iteration 00421    Loss -0.039979  0.000362\n",
            "Iteration 00422    Loss -0.040033  0.000355\n",
            "Iteration 00423    Loss -0.040089  0.000364\n",
            "Iteration 00424    Loss -0.040115  0.000368\n",
            "Iteration 00425    Loss -0.040150  0.000356\n",
            "Iteration 00426    Loss -0.040177  0.000372\n",
            "Iteration 00427    Loss -0.040190  0.000347\n",
            "Iteration 00428    Loss -0.040247  0.000377\n",
            "Iteration 00429    Loss -0.040292  0.000355\n",
            "Iteration 00430    Loss -0.040335  0.000362\n",
            "Iteration 00431    Loss -0.040351  0.000367\n",
            "Iteration 00432    Loss -0.040399  0.000349\n",
            "Iteration 00433    Loss -0.040465  0.000368\n",
            "Iteration 00434    Loss -0.040500  0.000365\n",
            "Iteration 00435    Loss -0.040540  0.000354\n",
            "Iteration 00436    Loss -0.040595  0.000356\n",
            "Iteration 00437    Loss -0.040595  0.000367\n",
            "Iteration 00438    Loss -0.040637  0.000356\n",
            "Iteration 00439    Loss -0.040693  0.000367\n",
            "Iteration 00440    Loss -0.040723  0.000375\n",
            "Iteration 00441    Loss -0.040785  0.000360\n",
            "Iteration 00442    Loss -0.040839  0.000370\n",
            "Iteration 00443    Loss -0.040853  0.000378\n",
            "Iteration 00444    Loss -0.040873  0.000357\n",
            "Iteration 00445    Loss -0.040889  0.000382\n",
            "Iteration 00446    Loss -0.040834  0.000348\n",
            "Iteration 00447    Loss -0.040946  0.000370\n",
            "Iteration 00448    Loss -0.040980  0.000366\n",
            "Iteration 00449    Loss -0.040977  0.000358\n",
            "Iteration 00450    Loss -0.041065  0.000360\n",
            "Iteration 00451    Loss -0.041145  0.000357\n",
            "Iteration 00452    Loss -0.041156  0.000358\n",
            "Iteration 00453    Loss -0.041179  0.000362\n",
            "Iteration 00454    Loss -0.041241  0.000361\n",
            "Iteration 00455    Loss -0.041272  0.000358\n",
            "Iteration 00456    Loss -0.041319  0.000361\n",
            "Iteration 00457    Loss -0.041342  0.000368\n",
            "Iteration 00458    Loss -0.041371  0.000370\n",
            "Iteration 00459    Loss -0.041418  0.000374\n",
            "Iteration 00460    Loss -0.041448  0.000370\n",
            "Iteration 00461    Loss -0.041497  0.000373\n",
            "Iteration 00462    Loss -0.041524  0.000373\n",
            "Iteration 00463    Loss -0.041539  0.000384\n",
            "Iteration 00464    Loss -0.041531  0.000371\n",
            "Iteration 00465    Loss -0.041517  0.000382\n",
            "Iteration 00466    Loss -0.041583  0.000363\n",
            "Iteration 00467    Loss -0.041628  0.000379\n",
            "Iteration 00468    Loss -0.041623  0.000369\n",
            "Iteration 00469    Loss -0.041660  0.000351\n",
            "Iteration 00470    Loss -0.041734  0.000361\n",
            "Iteration 00471    Loss -0.041770  0.000375\n",
            "Iteration 00472    Loss -0.041797  0.000361\n",
            "Iteration 00473    Loss -0.041861  0.000370\n",
            "Iteration 00474    Loss -0.041863  0.000367\n",
            "Iteration 00475    Loss -0.041911  0.000360\n",
            "Iteration 00476    Loss -0.041974  0.000380\n",
            "Iteration 00477    Loss -0.041985  0.000377\n",
            "Iteration 00478    Loss -0.042023  0.000375\n",
            "Iteration 00479    Loss -0.042065  0.000368\n",
            "Iteration 00480    Loss -0.042098  0.000387\n",
            "Iteration 00481    Loss -0.042123  0.000383\n",
            "Iteration 00482    Loss -0.042165  0.000379\n",
            "Iteration 00483    Loss -0.042177  0.000393\n",
            "Iteration 00484    Loss -0.042228  0.000386\n",
            "Iteration 00485    Loss -0.042238  0.000390\n",
            "Iteration 00486    Loss -0.042252  0.000382\n",
            "Iteration 00487    Loss -0.042257  0.000395\n",
            "Iteration 00488    Loss -0.042233  0.000360\n",
            "Iteration 00489    Loss -0.042280  0.000394\n",
            "Iteration 00490    Loss -0.042235  0.000355\n",
            "Iteration 00491    Loss -0.042252  0.000375\n",
            "Iteration 00492    Loss -0.042345  0.000372\n",
            "Iteration 00493    Loss -0.042410  0.000367\n",
            "Iteration 00494    Loss -0.042436  0.000374\n",
            "Iteration 00495    Loss -0.042435  0.000359\n",
            "Iteration 00496    Loss -0.042484  0.000366\n",
            "Iteration 00497    Loss -0.042555  0.000370\n",
            "Iteration 00498    Loss -0.042551  0.000369\n",
            "Iteration 00499    Loss -0.042586  0.000375\n",
            " 99% 496/500 [4:52:31<02:23, 35.80s/it]/content/SOTS/indoor/hazy/1421_4.png\n",
            "Iteration 00000    Loss 0.293202  0.000016\n",
            "Iteration 00001    Loss 31.314463  0.000005\n",
            "Iteration 00002    Loss 0.412970  0.000003\n",
            "Iteration 00003    Loss 1.946344  0.000002\n",
            "Iteration 00004    Loss 1.137639  0.000002\n",
            "Iteration 00005    Loss 1.143591  0.000001\n",
            "Iteration 00006    Loss 0.093935  0.000001\n",
            "Iteration 00007    Loss 0.237551  0.000001\n",
            "Iteration 00008    Loss 0.207628  0.000002\n",
            "Iteration 00009    Loss 0.072179  0.000002\n",
            "Iteration 00010    Loss 0.099880  0.000001\n",
            "Iteration 00011    Loss 0.133923  0.000002\n",
            "Iteration 00012    Loss 0.076788  0.000002\n",
            "Iteration 00013    Loss 0.045045  0.000002\n",
            "Iteration 00014    Loss 0.079211  0.000002\n",
            "Iteration 00015    Loss 0.062919  0.000002\n",
            "Iteration 00016    Loss 0.043030  0.000003\n",
            "Iteration 00017    Loss 0.052459  0.000004\n",
            "Iteration 00018    Loss 0.036103  0.000005\n",
            "Iteration 00019    Loss 0.039883  0.000005\n",
            "Iteration 00020    Loss 0.040610  0.000005\n",
            "Iteration 00021    Loss 0.036389  0.000006\n",
            "Iteration 00022    Loss 0.031732  0.000007\n",
            "Iteration 00023    Loss 0.029972  0.000009\n",
            "Iteration 00024    Loss 0.030226  0.000010\n",
            "Iteration 00025    Loss 0.030688  0.000011\n",
            "Iteration 00026    Loss 0.029557  0.000012\n",
            "Iteration 00027    Loss 0.027178  0.000012\n",
            "Iteration 00028    Loss 0.024744  0.000013\n",
            "Iteration 00029    Loss 0.023389  0.000015\n",
            "Iteration 00030    Loss 0.021993  0.000017\n",
            "Iteration 00031    Loss 0.021204  0.000016\n",
            "Iteration 00032    Loss 0.020516  0.000019\n",
            "Iteration 00033    Loss 0.019280  0.000019\n",
            "Iteration 00034    Loss 0.018256  0.000020\n",
            "Iteration 00035    Loss 0.017174  0.000021\n",
            "Iteration 00036    Loss 0.016461  0.000021\n",
            "Iteration 00037    Loss 0.015422  0.000023\n",
            "Iteration 00038    Loss 0.014461  0.000024\n",
            "Iteration 00039    Loss 0.013632  0.000024\n",
            "Iteration 00040    Loss 0.012843  0.000026\n",
            "Iteration 00041    Loss 0.012222  0.000026\n",
            "Iteration 00042    Loss 0.011532  0.000028\n",
            "Iteration 00043    Loss 0.010956  0.000031\n",
            "Iteration 00044    Loss 0.010199  0.000029\n",
            "Iteration 00045    Loss 0.009452  0.000029\n",
            "Iteration 00046    Loss 0.008833  0.000031\n",
            "Iteration 00047    Loss 0.008163  0.000034\n",
            "Iteration 00048    Loss 0.007568  0.000035\n",
            "Iteration 00049    Loss 0.006826  0.000036\n",
            "Iteration 00050    Loss 0.006207  0.000039\n",
            "Iteration 00051    Loss 0.005561  0.000039\n",
            "Iteration 00052    Loss 0.005050  0.000041\n",
            "Iteration 00053    Loss 0.004437  0.000042\n",
            "Iteration 00054    Loss 0.003868  0.000044\n",
            "Iteration 00055    Loss 0.003295  0.000047\n",
            "Iteration 00056    Loss 0.002770  0.000046\n",
            "Iteration 00057    Loss 0.002230  0.000051\n",
            "Iteration 00058    Loss 0.001599  0.000049\n",
            "Iteration 00059    Loss 0.001214  0.000051\n",
            "Iteration 00060    Loss 0.000570  0.000052\n",
            "Iteration 00061    Loss 0.000090  0.000052\n",
            "Iteration 00062    Loss -0.000294  0.000057\n",
            "Iteration 00063    Loss -0.000863  0.000057\n",
            "Iteration 00064    Loss -0.001341  0.000058\n",
            "Iteration 00065    Loss -0.001696  0.000060\n",
            "Iteration 00066    Loss -0.002354  0.000061\n",
            "Iteration 00067    Loss -0.002618  0.000064\n",
            "Iteration 00068    Loss -0.003162  0.000065\n",
            "Iteration 00069    Loss -0.003724  0.000063\n",
            "Iteration 00070    Loss -0.004015  0.000067\n",
            "Iteration 00071    Loss -0.004426  0.000072\n",
            "Iteration 00072    Loss -0.005036  0.000075\n",
            "Iteration 00073    Loss -0.005491  0.000070\n",
            "Iteration 00074    Loss -0.005958  0.000070\n",
            "Iteration 00075    Loss -0.006329  0.000074\n",
            "Iteration 00076    Loss -0.006590  0.000076\n",
            "Iteration 00077    Loss -0.007081  0.000078\n",
            "Iteration 00078    Loss -0.007579  0.000083\n",
            "Iteration 00079    Loss -0.007913  0.000085\n",
            "Iteration 00080    Loss -0.008411  0.000083\n",
            "Iteration 00081    Loss -0.008664  0.000088\n",
            "Iteration 00082    Loss -0.009128  0.000089\n",
            "Iteration 00083    Loss -0.009354  0.000090\n",
            "Iteration 00084    Loss -0.009850  0.000095\n",
            "Iteration 00085    Loss -0.010219  0.000099\n",
            "Iteration 00086    Loss -0.010590  0.000095\n",
            "Iteration 00087    Loss -0.010607  0.000093\n",
            "Iteration 00088    Loss -0.011192  0.000101\n",
            "Iteration 00089    Loss -0.011666  0.000107\n",
            "Iteration 00090    Loss -0.011965  0.000105\n",
            "Iteration 00091    Loss -0.012273  0.000106\n",
            "Iteration 00092    Loss -0.012456  0.000109\n",
            "Iteration 00093    Loss -0.012961  0.000115\n",
            "Iteration 00094    Loss -0.013232  0.000116\n",
            "Iteration 00095    Loss -0.013513  0.000108\n",
            "Iteration 00096    Loss -0.013831  0.000118\n",
            "Iteration 00097    Loss -0.014002  0.000125\n",
            "Iteration 00098    Loss -0.014154  0.000117\n",
            "Iteration 00099    Loss -0.014733  0.000125\n",
            "Iteration 00100    Loss -0.015080  0.000133\n",
            "Iteration 00101    Loss -0.015267  0.000124\n",
            "Iteration 00102    Loss -0.015521  0.000129\n",
            "Iteration 00103    Loss -0.015838  0.000131\n",
            "Iteration 00104    Loss -0.016180  0.000128\n",
            "Iteration 00105    Loss -0.016577  0.000131\n",
            "Iteration 00106    Loss -0.016831  0.000139\n",
            "Iteration 00107    Loss -0.017063  0.000139\n",
            "Iteration 00108    Loss -0.017183  0.000142\n",
            "Iteration 00109    Loss -0.017499  0.000146\n",
            "Iteration 00110    Loss -0.017898  0.000146\n",
            "Iteration 00111    Loss -0.018110  0.000160\n",
            "Iteration 00112    Loss -0.018442  0.000148\n",
            "Iteration 00113    Loss -0.018635  0.000153\n",
            "Iteration 00114    Loss -0.018979  0.000163\n",
            "Iteration 00115    Loss -0.019180  0.000155\n",
            "Iteration 00116    Loss -0.019462  0.000159\n",
            "Iteration 00117    Loss -0.019589  0.000167\n",
            "Iteration 00118    Loss -0.019987  0.000175\n",
            "Iteration 00119    Loss -0.020063  0.000172\n",
            "Iteration 00120    Loss -0.020287  0.000168\n",
            "Iteration 00121    Loss -0.020637  0.000189\n",
            "Iteration 00122    Loss -0.020877  0.000182\n",
            "Iteration 00123    Loss -0.021094  0.000180\n",
            "Iteration 00124    Loss -0.021357  0.000188\n",
            "Iteration 00125    Loss -0.021625  0.000191\n",
            "Iteration 00126    Loss -0.021730  0.000192\n",
            "Iteration 00127    Loss -0.022049  0.000197\n",
            "Iteration 00128    Loss -0.022274  0.000194\n",
            "Iteration 00129    Loss -0.022380  0.000206\n",
            "Iteration 00130    Loss -0.022488  0.000198\n",
            "Iteration 00131    Loss -0.022703  0.000210\n",
            "Iteration 00132    Loss -0.022947  0.000205\n",
            "Iteration 00133    Loss -0.023176  0.000205\n",
            "Iteration 00134    Loss -0.023425  0.000220\n",
            "Iteration 00135    Loss -0.023650  0.000217\n",
            "Iteration 00136    Loss -0.023775  0.000222\n",
            "Iteration 00137    Loss -0.023893  0.000229\n",
            "Iteration 00138    Loss -0.023970  0.000216\n",
            "Iteration 00139    Loss -0.024426  0.000215\n",
            "Iteration 00140    Loss -0.024512  0.000224\n",
            "Iteration 00141    Loss -0.024458  0.000241\n",
            "Iteration 00142    Loss -0.024953  0.000213\n",
            "Iteration 00143    Loss -0.025126  0.000246\n",
            "Iteration 00144    Loss -0.025205  0.000225\n",
            "Iteration 00145    Loss -0.025381  0.000258\n",
            "Iteration 00146    Loss -0.025614  0.000239\n",
            "Iteration 00147    Loss -0.025850  0.000237\n",
            "Iteration 00148    Loss -0.025945  0.000253\n",
            "Iteration 00149    Loss -0.026180  0.000236\n",
            "Iteration 00150    Loss -0.026353  0.000268\n",
            "Iteration 00151    Loss -0.026339  0.000222\n",
            "Iteration 00152    Loss -0.026756  0.000248\n",
            "Iteration 00153    Loss -0.026864  0.000258\n",
            "Iteration 00154    Loss -0.027002  0.000244\n",
            "Iteration 00155    Loss -0.027109  0.000279\n",
            "Iteration 00156    Loss -0.027379  0.000251\n",
            "Iteration 00157    Loss -0.027374  0.000248\n",
            "Iteration 00158    Loss -0.027726  0.000279\n",
            "Iteration 00159    Loss -0.027674  0.000258\n",
            "Iteration 00160    Loss -0.027847  0.000286\n",
            "Iteration 00161    Loss -0.028082  0.000268\n",
            "Iteration 00162    Loss -0.028198  0.000254\n",
            "Iteration 00163    Loss -0.028200  0.000281\n",
            "Iteration 00164    Loss -0.028616  0.000273\n",
            "Iteration 00165    Loss -0.028831  0.000274\n",
            "Iteration 00166    Loss -0.028885  0.000288\n",
            "Iteration 00167    Loss -0.028985  0.000286\n",
            "Iteration 00168    Loss -0.029068  0.000291\n",
            "Iteration 00169    Loss -0.029454  0.000298\n",
            "Iteration 00170    Loss -0.029447  0.000292\n",
            "Iteration 00171    Loss -0.029352  0.000292\n",
            "Iteration 00172    Loss -0.029696  0.000286\n",
            "Iteration 00173    Loss -0.029816  0.000337\n",
            "Iteration 00174    Loss -0.030044  0.000281\n",
            "Iteration 00175    Loss -0.030159  0.000302\n",
            "Iteration 00176    Loss -0.030300  0.000292\n",
            "Iteration 00177    Loss -0.030504  0.000302\n",
            "Iteration 00178    Loss -0.030593  0.000323\n",
            "Iteration 00179    Loss -0.030677  0.000295\n",
            "Iteration 00180    Loss -0.030714  0.000320\n",
            "Iteration 00181    Loss -0.030911  0.000319\n",
            "Iteration 00182    Loss -0.030953  0.000318\n",
            "Iteration 00183    Loss -0.031152  0.000312\n",
            "Iteration 00184    Loss -0.031168  0.000302\n",
            "Iteration 00185    Loss -0.031308  0.000329\n",
            "Iteration 00186    Loss -0.031637  0.000311\n",
            "Iteration 00187    Loss -0.031568  0.000342\n",
            "Iteration 00188    Loss -0.031613  0.000321\n",
            "Iteration 00189    Loss -0.031930  0.000307\n",
            "Iteration 00190    Loss -0.031715  0.000338\n",
            "Iteration 00191    Loss -0.032117  0.000320\n",
            "Iteration 00192    Loss -0.032235  0.000333\n",
            "Iteration 00193    Loss -0.032349  0.000335\n",
            "Iteration 00194    Loss -0.032449  0.000339\n",
            "Iteration 00195    Loss -0.032535  0.000298\n",
            "Iteration 00196    Loss -0.032629  0.000345\n",
            "Iteration 00197    Loss -0.032875  0.000342\n",
            "Iteration 00198    Loss -0.032851  0.000312\n",
            "Iteration 00199    Loss -0.033188  0.000353\n",
            "Iteration 00200    Loss -0.033253  0.000326\n",
            "Iteration 00201    Loss -0.033438  0.000350\n",
            "Iteration 00202    Loss -0.033292  0.000338\n",
            "Iteration 00203    Loss -0.033555  0.000328\n",
            "Iteration 00204    Loss -0.033666  0.000333\n",
            "Iteration 00205    Loss -0.033846  0.000337\n",
            "Iteration 00206    Loss -0.033952  0.000345\n",
            "Iteration 00207    Loss -0.033998  0.000345\n",
            "Iteration 00208    Loss -0.034104  0.000355\n",
            "Iteration 00209    Loss -0.034204  0.000345\n",
            "Iteration 00210    Loss -0.034427  0.000336\n",
            "Iteration 00211    Loss -0.034031  0.000372\n",
            "Iteration 00212    Loss -0.034529  0.000325\n",
            "Iteration 00213    Loss -0.034524  0.000369\n",
            "Iteration 00214    Loss -0.034599  0.000310\n",
            "Iteration 00215    Loss -0.034723  0.000371\n",
            "Iteration 00216    Loss -0.034818  0.000304\n",
            "Iteration 00217    Loss -0.034891  0.000338\n",
            "Iteration 00218    Loss -0.035166  0.000387\n",
            "Iteration 00219    Loss -0.035081  0.000282\n",
            "Iteration 00220    Loss -0.035301  0.000332\n",
            "Iteration 00221    Loss -0.035370  0.000385\n",
            "Iteration 00222    Loss -0.035229  0.000302\n",
            "Iteration 00223    Loss -0.035258  0.000333\n",
            "Iteration 00224    Loss -0.035435  0.000353\n",
            "Iteration 00225    Loss -0.035673  0.000323\n",
            "Iteration 00226    Loss -0.035741  0.000329\n",
            "Iteration 00227    Loss -0.035699  0.000328\n",
            "Iteration 00228    Loss -0.035746  0.000325\n",
            "Iteration 00229    Loss -0.036126  0.000344\n",
            "Iteration 00230    Loss -0.036025  0.000339\n",
            "Iteration 00231    Loss -0.036230  0.000332\n",
            "Iteration 00232    Loss -0.036012  0.000350\n",
            "Iteration 00233    Loss -0.036476  0.000332\n",
            "Iteration 00234    Loss -0.036555  0.000329\n",
            "Iteration 00235    Loss -0.036467  0.000353\n",
            "Iteration 00236    Loss -0.036553  0.000339\n",
            "Iteration 00237    Loss -0.036680  0.000359\n",
            "Iteration 00238    Loss -0.036781  0.000354\n",
            "Iteration 00239    Loss -0.036923  0.000334\n",
            "Iteration 00240    Loss -0.036920  0.000368\n",
            "Iteration 00241    Loss -0.037058  0.000363\n",
            "Iteration 00242    Loss -0.037129  0.000353\n",
            "Iteration 00243    Loss -0.037044  0.000353\n",
            "Iteration 00244    Loss -0.037262  0.000366\n",
            "Iteration 00245    Loss -0.037368  0.000370\n",
            "Iteration 00246    Loss -0.037248  0.000361\n",
            "Iteration 00247    Loss -0.037531  0.000367\n",
            "Iteration 00248    Loss -0.037595  0.000355\n",
            "Iteration 00249    Loss -0.037734  0.000368\n",
            "Iteration 00250    Loss -0.037675  0.000399\n",
            "Iteration 00251    Loss -0.037809  0.000341\n",
            "Iteration 00252    Loss -0.037883  0.000385\n",
            "Iteration 00253    Loss -0.037982  0.000371\n",
            "Iteration 00254    Loss -0.038152  0.000367\n",
            "Iteration 00255    Loss -0.038201  0.000368\n",
            "Iteration 00256    Loss -0.038165  0.000375\n",
            "Iteration 00257    Loss -0.038110  0.000368\n",
            "Iteration 00258    Loss -0.038075  0.000382\n",
            "Iteration 00259    Loss -0.038391  0.000363\n",
            "Iteration 00260    Loss -0.038482  0.000393\n",
            "Iteration 00261    Loss -0.038543  0.000337\n",
            "Iteration 00262    Loss -0.038346  0.000398\n",
            "Iteration 00263    Loss -0.038653  0.000360\n",
            "Iteration 00264    Loss -0.038663  0.000388\n",
            "Iteration 00265    Loss -0.038804  0.000395\n",
            "Iteration 00266    Loss -0.038663  0.000335\n",
            "Iteration 00267    Loss -0.038893  0.000392\n",
            "Iteration 00268    Loss -0.038901  0.000390\n",
            "Iteration 00269    Loss -0.039037  0.000354\n",
            "Iteration 00270    Loss -0.038835  0.000386\n",
            "Iteration 00271    Loss -0.039123  0.000365\n",
            "Iteration 00272    Loss -0.039229  0.000389\n",
            "Iteration 00273    Loss -0.039261  0.000371\n",
            "Iteration 00274    Loss -0.039395  0.000371\n",
            "Iteration 00275    Loss -0.039453  0.000388\n",
            "Iteration 00276    Loss -0.039504  0.000381\n",
            "Iteration 00277    Loss -0.039604  0.000406\n",
            "Iteration 00278    Loss -0.039643  0.000369\n",
            "Iteration 00279    Loss -0.039750  0.000389\n",
            "Iteration 00280    Loss -0.039717  0.000412\n",
            "Iteration 00281    Loss -0.039745  0.000382\n",
            "Iteration 00282    Loss -0.039700  0.000387\n",
            "Iteration 00283    Loss -0.039834  0.000387\n",
            "Iteration 00284    Loss -0.039804  0.000403\n",
            "Iteration 00285    Loss -0.039913  0.000350\n",
            "Iteration 00286    Loss -0.039910  0.000395\n",
            "Iteration 00287    Loss -0.040022  0.000381\n",
            "Iteration 00288    Loss -0.039891  0.000358\n",
            "Iteration 00289    Loss -0.039962  0.000397\n",
            "Iteration 00290    Loss -0.040173  0.000369\n",
            "Iteration 00291    Loss -0.040238  0.000387\n",
            "Iteration 00292    Loss -0.040323  0.000413\n",
            "Iteration 00293    Loss -0.040313  0.000356\n",
            "Iteration 00294    Loss -0.040455  0.000382\n",
            "Iteration 00295    Loss -0.040397  0.000395\n",
            "Iteration 00296    Loss -0.040452  0.000383\n",
            "Iteration 00297    Loss -0.040576  0.000400\n",
            "Iteration 00298    Loss -0.040656  0.000379\n",
            "Iteration 00299    Loss -0.040702  0.000379\n",
            "Iteration 00300    Loss -0.040633  0.000405\n",
            "Iteration 00301    Loss -0.040702  0.000383\n",
            "Iteration 00302    Loss -0.040881  0.000407\n",
            "Iteration 00303    Loss -0.040912  0.000388\n",
            "Iteration 00304    Loss -0.040941  0.000392\n",
            "Iteration 00305    Loss -0.040965  0.000410\n",
            "Iteration 00306    Loss -0.040861  0.000376\n",
            "Iteration 00307    Loss -0.041065  0.000424\n",
            "Iteration 00308    Loss -0.040946  0.000391\n",
            "Iteration 00309    Loss -0.041118  0.000372\n",
            "Iteration 00310    Loss -0.041095  0.000422\n",
            "Iteration 00311    Loss -0.041192  0.000394\n",
            "Iteration 00312    Loss -0.041300  0.000396\n",
            "Iteration 00313    Loss -0.041334  0.000426\n",
            "Iteration 00314    Loss -0.041301  0.000365\n",
            "Iteration 00315    Loss -0.041338  0.000421\n",
            "Iteration 00316    Loss -0.041433  0.000408\n",
            "Iteration 00317    Loss -0.041490  0.000373\n",
            "Iteration 00318    Loss -0.041573  0.000420\n",
            "Iteration 00319    Loss -0.041351  0.000430\n",
            "Iteration 00320    Loss -0.041524  0.000379\n",
            "Iteration 00321    Loss -0.041626  0.000409\n",
            "Iteration 00322    Loss -0.041737  0.000451\n",
            "Iteration 00323    Loss -0.041733  0.000375\n",
            "Iteration 00324    Loss -0.041748  0.000373\n",
            "Iteration 00325    Loss -0.041820  0.000438\n",
            "Iteration 00326    Loss -0.041895  0.000385\n",
            "Iteration 00327    Loss -0.041920  0.000405\n",
            "Iteration 00328    Loss -0.041762  0.000431\n",
            "Iteration 00329    Loss -0.041985  0.000371\n",
            "Iteration 00330    Loss -0.042018  0.000388\n",
            "Iteration 00331    Loss -0.042033  0.000438\n",
            "Iteration 00332    Loss -0.042106  0.000406\n",
            "Iteration 00333    Loss -0.042131  0.000387\n",
            "Iteration 00334    Loss -0.042103  0.000395\n",
            "Iteration 00335    Loss -0.042189  0.000435\n",
            "Iteration 00336    Loss -0.042286  0.000386\n",
            "Iteration 00337    Loss -0.042100  0.000397\n",
            "Iteration 00338    Loss -0.042322  0.000445\n",
            "Iteration 00339    Loss -0.042233  0.000407\n",
            "Iteration 00340    Loss -0.042343  0.000407\n",
            "Iteration 00341    Loss -0.042442  0.000422\n",
            "Iteration 00342    Loss -0.042479  0.000419\n",
            "Iteration 00343    Loss -0.042456  0.000398\n",
            "Iteration 00344    Loss -0.042532  0.000410\n",
            "Iteration 00345    Loss -0.042381  0.000427\n",
            "Iteration 00346    Loss -0.042592  0.000420\n",
            "Iteration 00347    Loss -0.042649  0.000433\n",
            "Iteration 00348    Loss -0.042567  0.000425\n",
            "Iteration 00349    Loss -0.042704  0.000427\n",
            "Iteration 00350    Loss -0.042713  0.000421\n",
            "Iteration 00351    Loss -0.042744  0.000425\n",
            "Iteration 00352    Loss -0.042789  0.000428\n",
            "Iteration 00353    Loss -0.042812  0.000440\n",
            "Iteration 00354    Loss -0.042951  0.000415\n",
            "Iteration 00355    Loss -0.042932  0.000439\n",
            "Iteration 00356    Loss -0.042837  0.000426\n",
            "Iteration 00357    Loss -0.042834  0.000431\n",
            "Iteration 00358    Loss -0.043001  0.000436\n",
            "Iteration 00359    Loss -0.043042  0.000430\n",
            "Iteration 00360    Loss -0.043088  0.000425\n",
            "Iteration 00361    Loss -0.043075  0.000445\n",
            "Iteration 00362    Loss -0.043143  0.000410\n",
            "Iteration 00363    Loss -0.043185  0.000420\n",
            "Iteration 00364    Loss -0.043200  0.000449\n",
            "Iteration 00365    Loss -0.043164  0.000422\n",
            "Iteration 00366    Loss -0.043227  0.000429\n",
            "Iteration 00367    Loss -0.043162  0.000425\n",
            "Iteration 00368    Loss -0.043222  0.000429\n",
            "Iteration 00369    Loss -0.043333  0.000413\n",
            "Iteration 00370    Loss -0.043177  0.000428\n",
            "Iteration 00371    Loss -0.043316  0.000410\n",
            "Iteration 00372    Loss -0.043271  0.000403\n",
            "Iteration 00373    Loss -0.043440  0.000431\n",
            "Iteration 00374    Loss -0.043481  0.000423\n",
            "Iteration 00375    Loss -0.043452  0.000414\n",
            "Iteration 00376    Loss -0.043499  0.000426\n",
            "Iteration 00377    Loss -0.043596  0.000437\n",
            "Iteration 00378    Loss -0.043634  0.000409\n",
            "Iteration 00379    Loss -0.043503  0.000440\n",
            "Iteration 00380    Loss -0.043656  0.000434\n",
            "Iteration 00381    Loss -0.043675  0.000422\n",
            "Iteration 00382    Loss -0.043586  0.000419\n",
            "Iteration 00383    Loss -0.043668  0.000421\n",
            "Iteration 00384    Loss -0.043665  0.000464\n",
            "Iteration 00385    Loss -0.043749  0.000403\n",
            "Iteration 00386    Loss -0.043833  0.000428\n",
            "Iteration 00387    Loss -0.043778  0.000443\n",
            "Iteration 00388    Loss -0.043851  0.000422\n",
            "Iteration 00389    Loss -0.043850  0.000446\n",
            "Iteration 00390    Loss -0.043896  0.000424\n",
            "Iteration 00391    Loss -0.043960  0.000410\n",
            "Iteration 00392    Loss -0.043955  0.000449\n",
            "Iteration 00393    Loss -0.043994  0.000431\n",
            "Iteration 00394    Loss -0.043961  0.000415\n",
            "Iteration 00395    Loss -0.044058  0.000436\n",
            "Iteration 00396    Loss -0.044069  0.000428\n",
            "Iteration 00397    Loss -0.043991  0.000438\n",
            "Iteration 00398    Loss -0.044086  0.000428\n",
            "Iteration 00399    Loss -0.044124  0.000425\n",
            "Iteration 00400    Loss -0.044146  0.000447\n",
            "Iteration 00401    Loss -0.044198  0.000416\n",
            "Iteration 00402    Loss -0.044220  0.000428\n",
            "Iteration 00403    Loss -0.044244  0.000444\n",
            "Iteration 00404    Loss -0.044202  0.000437\n",
            "Iteration 00405    Loss -0.044292  0.000446\n",
            "Iteration 00406    Loss -0.044325  0.000430\n",
            "Iteration 00407    Loss -0.044336  0.000444\n",
            "Iteration 00408    Loss -0.044395  0.000438\n",
            "Iteration 00409    Loss -0.044367  0.000450\n",
            "Iteration 00410    Loss -0.044459  0.000448\n",
            "Iteration 00411    Loss -0.044424  0.000447\n",
            "Iteration 00412    Loss -0.044385  0.000451\n",
            "Iteration 00413    Loss -0.044508  0.000449\n",
            "Iteration 00414    Loss -0.044530  0.000454\n",
            "Iteration 00415    Loss -0.044544  0.000437\n",
            "Iteration 00416    Loss -0.044460  0.000468\n",
            "Iteration 00417    Loss -0.044454  0.000444\n",
            "Iteration 00418    Loss -0.044566  0.000438\n",
            "Iteration 00419    Loss -0.044537  0.000457\n",
            "Iteration 00420    Loss -0.044493  0.000452\n",
            "Iteration 00421    Loss -0.044523  0.000462\n",
            "Iteration 00422    Loss -0.044623  0.000444\n",
            "Iteration 00423    Loss -0.044521  0.000440\n",
            "Iteration 00424    Loss -0.044566  0.000434\n",
            "Iteration 00425    Loss -0.044502  0.000414\n",
            "Iteration 00426    Loss -0.044668  0.000466\n",
            "Iteration 00427    Loss -0.044627  0.000399\n",
            "Iteration 00428    Loss -0.044645  0.000420\n",
            "Iteration 00429    Loss -0.044739  0.000438\n",
            "Iteration 00430    Loss -0.044562  0.000439\n",
            "Iteration 00431    Loss -0.044626  0.000420\n",
            "Iteration 00432    Loss -0.044726  0.000420\n",
            "Iteration 00433    Loss -0.044739  0.000419\n",
            "Iteration 00434    Loss -0.044752  0.000410\n",
            "Iteration 00435    Loss -0.044818  0.000426\n",
            "Iteration 00436    Loss -0.044838  0.000427\n",
            "Iteration 00437    Loss -0.044863  0.000418\n",
            "Iteration 00438    Loss -0.044915  0.000422\n",
            "Iteration 00439    Loss -0.044973  0.000437\n",
            "Iteration 00440    Loss -0.044877  0.000434\n",
            "Iteration 00441    Loss -0.045005  0.000418\n",
            "Iteration 00442    Loss -0.045017  0.000418\n",
            "Iteration 00443    Loss -0.045027  0.000429\n",
            "Iteration 00444    Loss -0.045091  0.000446\n",
            "Iteration 00445    Loss -0.045088  0.000434\n",
            "Iteration 00446    Loss -0.045146  0.000441\n",
            "Iteration 00447    Loss -0.045116  0.000455\n",
            "Iteration 00448    Loss -0.045149  0.000457\n",
            "Iteration 00449    Loss -0.045168  0.000432\n",
            "Iteration 00450    Loss -0.045192  0.000451\n",
            "Iteration 00451    Loss -0.045056  0.000444\n",
            "Iteration 00452    Loss -0.045250  0.000462\n",
            "Iteration 00453    Loss -0.045265  0.000471\n",
            "Iteration 00454    Loss -0.045214  0.000450\n",
            "Iteration 00455    Loss -0.045166  0.000449\n",
            "Iteration 00456    Loss -0.045306  0.000452\n",
            "Iteration 00457    Loss -0.045322  0.000478\n",
            "Iteration 00458    Loss -0.045291  0.000450\n",
            "Iteration 00459    Loss -0.045370  0.000450\n",
            "Iteration 00460    Loss -0.045253  0.000467\n",
            "Iteration 00461    Loss -0.045367  0.000429\n",
            "Iteration 00462    Loss -0.045387  0.000465\n",
            "Iteration 00463    Loss -0.045407  0.000464\n",
            "Iteration 00464    Loss -0.045453  0.000450\n",
            "Iteration 00465    Loss -0.045416  0.000454\n",
            "Iteration 00466    Loss -0.045475  0.000463\n",
            "Iteration 00467    Loss -0.045455  0.000471\n",
            "Iteration 00468    Loss -0.045476  0.000444\n",
            "Iteration 00469    Loss -0.045514  0.000484\n",
            "Iteration 00470    Loss -0.045532  0.000460\n",
            "Iteration 00471    Loss -0.045430  0.000435\n",
            "Iteration 00472    Loss -0.045518  0.000471\n",
            "Iteration 00473    Loss -0.045533  0.000442\n",
            "Iteration 00474    Loss -0.045558  0.000458\n",
            "Iteration 00475    Loss -0.045590  0.000454\n",
            "Iteration 00476    Loss -0.045615  0.000457\n",
            "Iteration 00477    Loss -0.045651  0.000473\n",
            "Iteration 00478    Loss -0.045648  0.000463\n",
            "Iteration 00479    Loss -0.045675  0.000452\n",
            "Iteration 00480    Loss -0.045619  0.000471\n",
            "Iteration 00481    Loss -0.045675  0.000447\n",
            "Iteration 00482    Loss -0.045710  0.000484\n",
            "Iteration 00483    Loss -0.045691  0.000446\n",
            "Iteration 00484    Loss -0.045670  0.000474\n",
            "Iteration 00485    Loss -0.045747  0.000472\n",
            "Iteration 00486    Loss -0.045691  0.000466\n",
            "Iteration 00487    Loss -0.045710  0.000474\n",
            "Iteration 00488    Loss -0.045690  0.000474\n",
            "Iteration 00489    Loss -0.045755  0.000459\n",
            "Iteration 00490    Loss -0.045812  0.000447\n",
            "Iteration 00491    Loss -0.045724  0.000473\n",
            "Iteration 00492    Loss -0.045755  0.000434\n",
            "Iteration 00493    Loss -0.045756  0.000483\n",
            "Iteration 00494    Loss -0.045702  0.000434\n",
            "Iteration 00495    Loss -0.045672  0.000407\n",
            "Iteration 00496    Loss -0.045803  0.000489\n",
            "Iteration 00497    Loss -0.045856  0.000421\n",
            "Iteration 00498    Loss -0.045851  0.000411\n",
            "Iteration 00499    Loss -0.045887  0.000465\n",
            " 99% 497/500 [4:53:06<01:47, 35.74s/it]/content/SOTS/indoor/hazy/1439_7.png\n",
            "Iteration 00000    Loss 0.447880  0.000022\n",
            "Iteration 00001    Loss 34.299358  0.000008\n",
            "Iteration 00002    Loss 0.424664  0.000003\n",
            "Iteration 00003    Loss 0.315216  0.000001\n",
            "Iteration 00004    Loss 0.292266  0.000001\n",
            "Iteration 00005    Loss 0.298539  0.000001\n",
            "Iteration 00006    Loss 0.195165  0.000001\n",
            "Iteration 00007    Loss 0.220943  0.000001\n",
            "Iteration 00008    Loss 0.161382  0.000001\n",
            "Iteration 00009    Loss 0.145556  0.000001\n",
            "Iteration 00010    Loss 0.132676  0.000001\n",
            "Iteration 00011    Loss 0.124082  0.000001\n",
            "Iteration 00012    Loss 0.117468  0.000001\n",
            "Iteration 00013    Loss 0.110943  0.000002\n",
            "Iteration 00014    Loss 0.105844  0.000002\n",
            "Iteration 00015    Loss 0.101005  0.000002\n",
            "Iteration 00016    Loss 0.099652  0.000002\n",
            "Iteration 00017    Loss 0.094026  0.000002\n",
            "Iteration 00018    Loss 0.091010  0.000002\n",
            "Iteration 00019    Loss 0.088689  0.000003\n",
            "Iteration 00020    Loss 0.086318  0.000003\n",
            "Iteration 00021    Loss 0.084069  0.000004\n",
            "Iteration 00022    Loss 0.082047  0.000005\n",
            "Iteration 00023    Loss 0.080042  0.000007\n",
            "Iteration 00024    Loss 0.078279  0.000009\n",
            "Iteration 00025    Loss 0.076934  0.000011\n",
            "Iteration 00026    Loss 0.075432  0.000012\n",
            "Iteration 00027    Loss 0.073780  0.000016\n",
            "Iteration 00028    Loss 0.072289  0.000023\n",
            "Iteration 00029    Loss 0.070678  0.000029\n",
            "Iteration 00030    Loss 0.069281  0.000034\n",
            "Iteration 00031    Loss 0.067946  0.000041\n",
            "Iteration 00032    Loss 0.066661  0.000048\n",
            "Iteration 00033    Loss 0.065310  0.000054\n",
            "Iteration 00034    Loss 0.063975  0.000059\n",
            "Iteration 00035    Loss 0.062670  0.000065\n",
            "Iteration 00036    Loss 0.061334  0.000076\n",
            "Iteration 00037    Loss 0.060204  0.000079\n",
            "Iteration 00038    Loss 0.059014  0.000085\n",
            "Iteration 00039    Loss 0.057775  0.000094\n",
            "Iteration 00040    Loss 0.056491  0.000101\n",
            "Iteration 00041    Loss 0.055432  0.000107\n",
            "Iteration 00042    Loss 0.054294  0.000111\n",
            "Iteration 00043    Loss 0.053499  0.000114\n",
            "Iteration 00044    Loss 0.052782  0.000117\n",
            "Iteration 00045    Loss 0.051453  0.000121\n",
            "Iteration 00046    Loss 0.050159  0.000114\n",
            "Iteration 00047    Loss 0.049042  0.000126\n",
            "Iteration 00048    Loss 0.047964  0.000136\n",
            "Iteration 00049    Loss 0.046770  0.000132\n",
            "Iteration 00050    Loss 0.045603  0.000140\n",
            "Iteration 00051    Loss 0.044454  0.000148\n",
            "Iteration 00052    Loss 0.042955  0.000142\n",
            "Iteration 00053    Loss 0.041941  0.000138\n",
            "Iteration 00054    Loss 0.040648  0.000144\n",
            "Iteration 00055    Loss 0.039658  0.000157\n",
            "Iteration 00056    Loss 0.038415  0.000159\n",
            "Iteration 00057    Loss 0.037371  0.000160\n",
            "Iteration 00058    Loss 0.036232  0.000176\n",
            "Iteration 00059    Loss 0.034943  0.000184\n",
            "Iteration 00060    Loss 0.034179  0.000178\n",
            "Iteration 00061    Loss 0.033411  0.000177\n",
            "Iteration 00062    Loss 0.032179  0.000193\n",
            "Iteration 00063    Loss 0.031440  0.000207\n",
            "Iteration 00064    Loss 0.030527  0.000213\n",
            "Iteration 00065    Loss 0.029799  0.000207\n",
            "Iteration 00066    Loss 0.029022  0.000212\n",
            "Iteration 00067    Loss 0.028012  0.000234\n",
            "Iteration 00068    Loss 0.027374  0.000255\n",
            "Iteration 00069    Loss 0.026574  0.000266\n",
            "Iteration 00070    Loss 0.025777  0.000275\n",
            "Iteration 00071    Loss 0.024949  0.000274\n",
            "Iteration 00072    Loss 0.024298  0.000273\n",
            "Iteration 00073    Loss 0.023565  0.000290\n",
            "Iteration 00074    Loss 0.022945  0.000329\n",
            "Iteration 00075    Loss 0.022261  0.000314\n",
            "Iteration 00076    Loss 0.021455  0.000306\n",
            "Iteration 00077    Loss 0.020852  0.000321\n",
            "Iteration 00078    Loss 0.020142  0.000359\n",
            "Iteration 00079    Loss 0.019450  0.000363\n",
            "Iteration 00080    Loss 0.018764  0.000373\n",
            "Iteration 00081    Loss 0.018072  0.000375\n",
            "Iteration 00082    Loss 0.017416  0.000383\n",
            "Iteration 00083    Loss 0.016757  0.000396\n",
            "Iteration 00084    Loss 0.016120  0.000416\n",
            "Iteration 00085    Loss 0.015658  0.000438\n",
            "Iteration 00086    Loss 0.015053  0.000425\n",
            "Iteration 00087    Loss 0.014496  0.000452\n",
            "Iteration 00088    Loss 0.013978  0.000462\n",
            "Iteration 00089    Loss 0.013401  0.000461\n",
            "Iteration 00090    Loss 0.012952  0.000470\n",
            "Iteration 00091    Loss 0.012352  0.000478\n",
            "Iteration 00092    Loss 0.011867  0.000499\n",
            "Iteration 00093    Loss 0.011371  0.000491\n",
            "Iteration 00094    Loss 0.010957  0.000520\n",
            "Iteration 00095    Loss 0.010538  0.000498\n",
            "Iteration 00096    Loss 0.010052  0.000505\n",
            "Iteration 00097    Loss 0.009637  0.000497\n",
            "Iteration 00098    Loss 0.009076  0.000518\n",
            "Iteration 00099    Loss 0.008608  0.000498\n",
            "Iteration 00100    Loss 0.008183  0.000525\n",
            "Iteration 00101    Loss 0.007835  0.000571\n",
            "Iteration 00102    Loss 0.007340  0.000531\n",
            "Iteration 00103    Loss 0.006867  0.000539\n",
            "Iteration 00104    Loss 0.006436  0.000591\n",
            "Iteration 00105    Loss 0.006094  0.000577\n",
            "Iteration 00106    Loss 0.005621  0.000568\n",
            "Iteration 00107    Loss 0.005166  0.000604\n",
            "Iteration 00108    Loss 0.004877  0.000620\n",
            "Iteration 00109    Loss 0.004393  0.000600\n",
            "Iteration 00110    Loss 0.004016  0.000582\n",
            "Iteration 00111    Loss 0.003610  0.000621\n",
            "Iteration 00112    Loss 0.003229  0.000643\n",
            "Iteration 00113    Loss 0.002825  0.000616\n",
            "Iteration 00114    Loss 0.002489  0.000663\n",
            "Iteration 00115    Loss 0.002081  0.000664\n",
            "Iteration 00116    Loss 0.001727  0.000662\n",
            "Iteration 00117    Loss 0.001372  0.000677\n",
            "Iteration 00118    Loss 0.000982  0.000671\n",
            "Iteration 00119    Loss 0.000716  0.000675\n",
            "Iteration 00120    Loss 0.000425  0.000690\n",
            "Iteration 00121    Loss 0.000246  0.000630\n",
            "Iteration 00122    Loss -0.000225  0.000621\n",
            "Iteration 00123    Loss -0.000689  0.000663\n",
            "Iteration 00124    Loss -0.000976  0.000671\n",
            "Iteration 00125    Loss -0.001380  0.000657\n",
            "Iteration 00126    Loss -0.001718  0.000713\n",
            "Iteration 00127    Loss -0.002011  0.000706\n",
            "Iteration 00128    Loss -0.002359  0.000705\n",
            "Iteration 00129    Loss -0.002751  0.000708\n",
            "Iteration 00130    Loss -0.003046  0.000714\n",
            "Iteration 00131    Loss -0.003418  0.000756\n",
            "Iteration 00132    Loss -0.003701  0.000755\n",
            "Iteration 00133    Loss -0.004025  0.000745\n",
            "Iteration 00134    Loss -0.004375  0.000744\n",
            "Iteration 00135    Loss -0.004661  0.000783\n",
            "Iteration 00136    Loss -0.004999  0.000754\n",
            "Iteration 00137    Loss -0.005304  0.000769\n",
            "Iteration 00138    Loss -0.005593  0.000812\n",
            "Iteration 00139    Loss -0.005896  0.000826\n",
            "Iteration 00140    Loss -0.006200  0.000806\n",
            "Iteration 00141    Loss -0.006441  0.000801\n",
            "Iteration 00142    Loss -0.006678  0.000798\n",
            "Iteration 00143    Loss -0.006918  0.000822\n",
            "Iteration 00144    Loss -0.007135  0.000830\n",
            "Iteration 00145    Loss -0.007406  0.000795\n",
            "Iteration 00146    Loss -0.007714  0.000848\n",
            "Iteration 00147    Loss -0.007983  0.000794\n",
            "Iteration 00148    Loss -0.008286  0.000817\n",
            "Iteration 00149    Loss -0.008574  0.000863\n",
            "Iteration 00150    Loss -0.008835  0.000856\n",
            "Iteration 00151    Loss -0.009103  0.000823\n",
            "Iteration 00152    Loss -0.009369  0.000846\n",
            "Iteration 00153    Loss -0.009643  0.000890\n",
            "Iteration 00154    Loss -0.009922  0.000920\n",
            "Iteration 00155    Loss -0.010147  0.000825\n",
            "Iteration 00156    Loss -0.010396  0.000931\n",
            "Iteration 00157    Loss -0.010683  0.000880\n",
            "Iteration 00158    Loss -0.010934  0.000916\n",
            "Iteration 00159    Loss -0.011191  0.000876\n",
            "Iteration 00160    Loss -0.011433  0.000930\n",
            "Iteration 00161    Loss -0.011715  0.000936\n",
            "Iteration 00162    Loss -0.011951  0.000894\n",
            "Iteration 00163    Loss -0.012186  0.000942\n",
            "Iteration 00164    Loss -0.012408  0.000901\n",
            "Iteration 00165    Loss -0.012624  0.000941\n",
            "Iteration 00166    Loss -0.012866  0.000912\n",
            "Iteration 00167    Loss -0.013095  0.000941\n",
            "Iteration 00168    Loss -0.013350  0.000937\n",
            "Iteration 00169    Loss -0.013582  0.000958\n",
            "Iteration 00170    Loss -0.013833  0.000941\n",
            "Iteration 00171    Loss -0.014040  0.000970\n",
            "Iteration 00172    Loss -0.014240  0.000956\n",
            "Iteration 00173    Loss -0.014427  0.000962\n",
            "Iteration 00174    Loss -0.014540  0.000981\n",
            "Iteration 00175    Loss -0.014540  0.000841\n",
            "Iteration 00176    Loss -0.014863  0.000851\n",
            "Iteration 00177    Loss -0.015145  0.000863\n",
            "Iteration 00178    Loss -0.015376  0.000958\n",
            "Iteration 00179    Loss -0.015597  0.000933\n",
            "Iteration 00180    Loss -0.015833  0.000896\n",
            "Iteration 00181    Loss -0.016025  0.000921\n",
            "Iteration 00182    Loss -0.016309  0.000965\n",
            "Iteration 00183    Loss -0.016453  0.000886\n",
            "Iteration 00184    Loss -0.016688  0.000908\n",
            "Iteration 00185    Loss -0.016961  0.000925\n",
            "Iteration 00186    Loss -0.017156  0.000949\n",
            "Iteration 00187    Loss -0.017346  0.000964\n",
            "Iteration 00188    Loss -0.017517  0.000954\n",
            "Iteration 00189    Loss -0.017744  0.001007\n",
            "Iteration 00190    Loss -0.017970  0.000960\n",
            "Iteration 00191    Loss -0.018150  0.000966\n",
            "Iteration 00192    Loss -0.018336  0.000988\n",
            "Iteration 00193    Loss -0.018574  0.001007\n",
            "Iteration 00194    Loss -0.018780  0.000996\n",
            "Iteration 00195    Loss -0.018946  0.000962\n",
            "Iteration 00196    Loss -0.019093  0.001042\n",
            "Iteration 00197    Loss -0.019093  0.000945\n",
            "Iteration 00198    Loss -0.019049  0.001060\n",
            "Iteration 00199    Loss -0.019315  0.000892\n",
            "Iteration 00200    Loss -0.019750  0.001012\n",
            "Iteration 00201    Loss -0.019814  0.001004\n",
            "Iteration 00202    Loss -0.020048  0.000929\n",
            "Iteration 00203    Loss -0.020342  0.000987\n",
            "Iteration 00204    Loss -0.020453  0.001008\n",
            "Iteration 00205    Loss -0.020662  0.000971\n",
            "Iteration 00206    Loss -0.020858  0.000956\n",
            "Iteration 00207    Loss -0.020991  0.000997\n",
            "Iteration 00208    Loss -0.021290  0.000968\n",
            "Iteration 00209    Loss -0.021382  0.000977\n",
            "Iteration 00210    Loss -0.021606  0.001025\n",
            "Iteration 00211    Loss -0.021785  0.000969\n",
            "Iteration 00212    Loss -0.021948  0.001001\n",
            "Iteration 00213    Loss -0.022135  0.000989\n",
            "Iteration 00214    Loss -0.022340  0.001017\n",
            "Iteration 00215    Loss -0.022491  0.000975\n",
            "Iteration 00216    Loss -0.022659  0.001034\n",
            "Iteration 00217    Loss -0.022844  0.001008\n",
            "Iteration 00218    Loss -0.022976  0.001013\n",
            "Iteration 00219    Loss -0.023141  0.001001\n",
            "Iteration 00220    Loss -0.023224  0.001026\n",
            "Iteration 00221    Loss -0.023352  0.000948\n",
            "Iteration 00222    Loss -0.023628  0.001000\n",
            "Iteration 00223    Loss -0.023826  0.000997\n",
            "Iteration 00224    Loss -0.024010  0.000993\n",
            "Iteration 00225    Loss -0.024097  0.001005\n",
            "Iteration 00226    Loss -0.024292  0.001004\n",
            "Iteration 00227    Loss -0.024444  0.001019\n",
            "Iteration 00228    Loss -0.024621  0.001027\n",
            "Iteration 00229    Loss -0.024772  0.001024\n",
            "Iteration 00230    Loss -0.024925  0.001029\n",
            "Iteration 00231    Loss -0.025070  0.001038\n",
            "Iteration 00232    Loss -0.025162  0.000951\n",
            "Iteration 00233    Loss -0.025209  0.001082\n",
            "Iteration 00234    Loss -0.025242  0.000899\n",
            "Iteration 00235    Loss -0.025546  0.000994\n",
            "Iteration 00236    Loss -0.025726  0.001004\n",
            "Iteration 00237    Loss -0.025837  0.000918\n",
            "Iteration 00238    Loss -0.026026  0.000992\n",
            "Iteration 00239    Loss -0.026176  0.001010\n",
            "Iteration 00240    Loss -0.026321  0.000947\n",
            "Iteration 00241    Loss -0.026507  0.000991\n",
            "Iteration 00242    Loss -0.026650  0.001012\n",
            "Iteration 00243    Loss -0.026748  0.000994\n",
            "Iteration 00244    Loss -0.026859  0.000955\n",
            "Iteration 00245    Loss -0.026939  0.001046\n",
            "Iteration 00246    Loss -0.027027  0.000955\n",
            "Iteration 00247    Loss -0.027189  0.000981\n",
            "Iteration 00248    Loss -0.027442  0.001018\n",
            "Iteration 00249    Loss -0.027645  0.000968\n",
            "Iteration 00250    Loss -0.027751  0.001008\n",
            "Iteration 00251    Loss -0.027874  0.001013\n",
            "Iteration 00252    Loss -0.028077  0.001009\n",
            "Iteration 00253    Loss -0.028207  0.000991\n",
            "Iteration 00254    Loss -0.028344  0.001009\n",
            "Iteration 00255    Loss -0.028511  0.001021\n",
            "Iteration 00256    Loss -0.028635  0.001012\n",
            "Iteration 00257    Loss -0.028775  0.001033\n",
            "Iteration 00258    Loss -0.028927  0.001031\n",
            "Iteration 00259    Loss -0.029055  0.000996\n",
            "Iteration 00260    Loss -0.029155  0.001038\n",
            "Iteration 00261    Loss -0.029287  0.001023\n",
            "Iteration 00262    Loss -0.029419  0.001014\n",
            "Iteration 00263    Loss -0.029517  0.001044\n",
            "Iteration 00264    Loss -0.029611  0.001047\n",
            "Iteration 00265    Loss -0.029793  0.001043\n",
            "Iteration 00266    Loss -0.029876  0.001064\n",
            "Iteration 00267    Loss -0.029962  0.001006\n",
            "Iteration 00268    Loss -0.029987  0.001103\n",
            "Iteration 00269    Loss -0.029981  0.000909\n",
            "Iteration 00270    Loss -0.030195  0.001026\n",
            "Iteration 00271    Loss -0.030441  0.001012\n",
            "Iteration 00272    Loss -0.030539  0.000946\n",
            "Iteration 00273    Loss -0.030616  0.001073\n",
            "Iteration 00274    Loss -0.030802  0.000943\n",
            "Iteration 00275    Loss -0.030929  0.000978\n",
            "Iteration 00276    Loss -0.031043  0.001031\n",
            "Iteration 00277    Loss -0.031205  0.000964\n",
            "Iteration 00278    Loss -0.031310  0.000995\n",
            "Iteration 00279    Loss -0.031381  0.001023\n",
            "Iteration 00280    Loss -0.031511  0.000950\n",
            "Iteration 00281    Loss -0.031619  0.000979\n",
            "Iteration 00282    Loss -0.031758  0.000978\n",
            "Iteration 00283    Loss -0.031916  0.000999\n",
            "Iteration 00284    Loss -0.032013  0.000969\n",
            "Iteration 00285    Loss -0.032103  0.001000\n",
            "Iteration 00286    Loss -0.032258  0.001008\n",
            "Iteration 00287    Loss -0.032352  0.000961\n",
            "Iteration 00288    Loss -0.032444  0.001010\n",
            "Iteration 00289    Loss -0.032550  0.000998\n",
            "Iteration 00290    Loss -0.032666  0.000998\n",
            "Iteration 00291    Loss -0.032690  0.001002\n",
            "Iteration 00292    Loss -0.032648  0.000940\n",
            "Iteration 00293    Loss -0.032549  0.001031\n",
            "Iteration 00294    Loss -0.032685  0.000930\n",
            "Iteration 00295    Loss -0.033033  0.000934\n",
            "Iteration 00296    Loss -0.033154  0.000951\n",
            "Iteration 00297    Loss -0.033201  0.000968\n",
            "Iteration 00298    Loss -0.033332  0.000952\n",
            "Iteration 00299    Loss -0.033469  0.000928\n",
            "Iteration 00300    Loss -0.033561  0.000950\n",
            "Iteration 00301    Loss -0.033642  0.000961\n",
            "Iteration 00302    Loss -0.033793  0.000951\n",
            "Iteration 00303    Loss -0.033886  0.000953\n",
            "Iteration 00304    Loss -0.033973  0.000979\n",
            "Iteration 00305    Loss -0.034079  0.000965\n",
            "Iteration 00306    Loss -0.034192  0.000974\n",
            "Iteration 00307    Loss -0.034279  0.000984\n",
            "Iteration 00308    Loss -0.034372  0.000979\n",
            "Iteration 00309    Loss -0.034490  0.000979\n",
            "Iteration 00310    Loss -0.034548  0.000991\n",
            "Iteration 00311    Loss -0.034674  0.000992\n",
            "Iteration 00312    Loss -0.034767  0.001009\n",
            "Iteration 00313    Loss -0.034851  0.001008\n",
            "Iteration 00314    Loss -0.034931  0.001008\n",
            "Iteration 00315    Loss -0.035027  0.001032\n",
            "Iteration 00316    Loss -0.035096  0.000974\n",
            "Iteration 00317    Loss -0.035179  0.001023\n",
            "Iteration 00318    Loss -0.035253  0.001014\n",
            "Iteration 00319    Loss -0.035339  0.000992\n",
            "Iteration 00320    Loss -0.035402  0.001029\n",
            "Iteration 00321    Loss -0.035454  0.000993\n",
            "Iteration 00322    Loss -0.035486  0.000991\n",
            "Iteration 00323    Loss -0.035506  0.001036\n",
            "Iteration 00324    Loss -0.035533  0.000963\n",
            "Iteration 00325    Loss -0.035649  0.001002\n",
            "Iteration 00326    Loss -0.035701  0.000978\n",
            "Iteration 00327    Loss -0.035690  0.000892\n",
            "Iteration 00328    Loss -0.035775  0.000988\n",
            "Iteration 00329    Loss -0.036001  0.000908\n",
            "Iteration 00330    Loss -0.036100  0.000905\n",
            "Iteration 00331    Loss -0.036080  0.001026\n",
            "Iteration 00332    Loss -0.036266  0.000918\n",
            "Iteration 00333    Loss -0.036396  0.000929\n",
            "Iteration 00334    Loss -0.036378  0.001005\n",
            "Iteration 00335    Loss -0.036515  0.000944\n",
            "Iteration 00336    Loss -0.036577  0.000938\n",
            "Iteration 00337    Loss -0.036643  0.000978\n",
            "Iteration 00338    Loss -0.036789  0.000996\n",
            "Iteration 00339    Loss -0.036799  0.000944\n",
            "Iteration 00340    Loss -0.036883  0.000982\n",
            "Iteration 00341    Loss -0.036998  0.001014\n",
            "Iteration 00342    Loss -0.037018  0.000949\n",
            "Iteration 00343    Loss -0.037132  0.000999\n",
            "Iteration 00344    Loss -0.037190  0.001021\n",
            "Iteration 00345    Loss -0.037269  0.000976\n",
            "Iteration 00346    Loss -0.037304  0.000973\n",
            "Iteration 00347    Loss -0.037374  0.001035\n",
            "Iteration 00348    Loss -0.037440  0.000980\n",
            "Iteration 00349    Loss -0.037496  0.001015\n",
            "Iteration 00350    Loss -0.037534  0.000972\n",
            "Iteration 00351    Loss -0.037626  0.001043\n",
            "Iteration 00352    Loss -0.037672  0.000951\n",
            "Iteration 00353    Loss -0.037778  0.001014\n",
            "Iteration 00354    Loss -0.037820  0.001011\n",
            "Iteration 00355    Loss -0.037865  0.000961\n",
            "Iteration 00356    Loss -0.037895  0.001035\n",
            "Iteration 00357    Loss -0.037864  0.000937\n",
            "Iteration 00358    Loss -0.037814  0.001001\n",
            "Iteration 00359    Loss -0.037835  0.000983\n",
            "Iteration 00360    Loss -0.037978  0.000944\n",
            "Iteration 00361    Loss -0.038189  0.000939\n",
            "Iteration 00362    Loss -0.038175  0.000960\n",
            "Iteration 00363    Loss -0.038278  0.000958\n",
            "Iteration 00364    Loss -0.038361  0.000947\n",
            "Iteration 00365    Loss -0.038424  0.000963\n",
            "Iteration 00366    Loss -0.038502  0.000973\n",
            "Iteration 00367    Loss -0.038580  0.000954\n",
            "Iteration 00368    Loss -0.038589  0.000987\n",
            "Iteration 00369    Loss -0.038664  0.000933\n",
            "Iteration 00370    Loss -0.038805  0.000972\n",
            "Iteration 00371    Loss -0.038806  0.001010\n",
            "Iteration 00372    Loss -0.038875  0.000937\n",
            "Iteration 00373    Loss -0.038958  0.000960\n",
            "Iteration 00374    Loss -0.039002  0.001019\n",
            "Iteration 00375    Loss -0.039076  0.000996\n",
            "Iteration 00376    Loss -0.039150  0.000948\n",
            "Iteration 00377    Loss -0.039186  0.001015\n",
            "Iteration 00378    Loss -0.039261  0.000964\n",
            "Iteration 00379    Loss -0.039304  0.001009\n",
            "Iteration 00380    Loss -0.039355  0.000981\n",
            "Iteration 00381    Loss -0.039426  0.001002\n",
            "Iteration 00382    Loss -0.039436  0.000972\n",
            "Iteration 00383    Loss -0.039435  0.001070\n",
            "Iteration 00384    Loss -0.039422  0.000879\n",
            "Iteration 00385    Loss -0.039448  0.001060\n",
            "Iteration 00386    Loss -0.039578  0.000922\n",
            "Iteration 00387    Loss -0.039654  0.000992\n",
            "Iteration 00388    Loss -0.039696  0.000947\n",
            "Iteration 00389    Loss -0.039739  0.000934\n",
            "Iteration 00390    Loss -0.039821  0.001011\n",
            "Iteration 00391    Loss -0.039857  0.000952\n",
            "Iteration 00392    Loss -0.039945  0.000954\n",
            "Iteration 00393    Loss -0.039995  0.001003\n",
            "Iteration 00394    Loss -0.040053  0.000951\n",
            "Iteration 00395    Loss -0.040104  0.000998\n",
            "Iteration 00396    Loss -0.040167  0.000951\n",
            "Iteration 00397    Loss -0.040224  0.000983\n",
            "Iteration 00398    Loss -0.040251  0.001002\n",
            "Iteration 00399    Loss -0.040334  0.000955\n",
            "Iteration 00400    Loss -0.040345  0.001004\n",
            "Iteration 00401    Loss -0.040388  0.000961\n",
            "Iteration 00402    Loss -0.040414  0.000986\n",
            "Iteration 00403    Loss -0.040445  0.000993\n",
            "Iteration 00404    Loss -0.040459  0.000976\n",
            "Iteration 00405    Loss -0.040503  0.000938\n",
            "Iteration 00406    Loss -0.040575  0.000985\n",
            "Iteration 00407    Loss -0.040650  0.000965\n",
            "Iteration 00408    Loss -0.040691  0.000982\n",
            "Iteration 00409    Loss -0.040753  0.000978\n",
            "Iteration 00410    Loss -0.040787  0.000975\n",
            "Iteration 00411    Loss -0.040805  0.000982\n",
            "Iteration 00412    Loss -0.040861  0.000921\n",
            "Iteration 00413    Loss -0.040928  0.001017\n",
            "Iteration 00414    Loss -0.040936  0.000917\n",
            "Iteration 00415    Loss -0.040982  0.000986\n",
            "Iteration 00416    Loss -0.041036  0.000986\n",
            "Iteration 00417    Loss -0.041065  0.000956\n",
            "Iteration 00418    Loss -0.041082  0.000997\n",
            "Iteration 00419    Loss -0.041128  0.000939\n",
            "Iteration 00420    Loss -0.041265  0.000953\n",
            "Iteration 00421    Loss -0.041272  0.001006\n",
            "Iteration 00422    Loss -0.041302  0.000957\n",
            "Iteration 00423    Loss -0.041355  0.000996\n",
            "Iteration 00424    Loss -0.041388  0.000968\n",
            "Iteration 00425    Loss -0.041456  0.000985\n",
            "Iteration 00426    Loss -0.041477  0.001010\n",
            "Iteration 00427    Loss -0.041532  0.000952\n",
            "Iteration 00428    Loss -0.041559  0.001008\n",
            "Iteration 00429    Loss -0.041607  0.000965\n",
            "Iteration 00430    Loss -0.041661  0.001014\n",
            "Iteration 00431    Loss -0.041729  0.000973\n",
            "Iteration 00432    Loss -0.041748  0.000984\n",
            "Iteration 00433    Loss -0.041749  0.001012\n",
            "Iteration 00434    Loss -0.041753  0.000935\n",
            "Iteration 00435    Loss -0.041753  0.001040\n",
            "Iteration 00436    Loss -0.041759  0.000938\n",
            "Iteration 00437    Loss -0.041759  0.000958\n",
            "Iteration 00438    Loss -0.041761  0.001000\n",
            "Iteration 00439    Loss -0.041715  0.000874\n",
            "Iteration 00440    Loss -0.041902  0.001004\n",
            "Iteration 00441    Loss -0.041944  0.000953\n",
            "Iteration 00442    Loss -0.041899  0.000925\n",
            "Iteration 00443    Loss -0.041988  0.000967\n",
            "Iteration 00444    Loss -0.042123  0.000958\n",
            "Iteration 00445    Loss -0.042127  0.000921\n",
            "Iteration 00446    Loss -0.042171  0.000968\n",
            "Iteration 00447    Loss -0.042230  0.000990\n",
            "Iteration 00448    Loss -0.042248  0.000964\n",
            "Iteration 00449    Loss -0.042312  0.000983\n",
            "Iteration 00450    Loss -0.042346  0.000978\n",
            "Iteration 00451    Loss -0.042387  0.000981\n",
            "Iteration 00452    Loss -0.042398  0.000960\n",
            "Iteration 00453    Loss -0.042459  0.001009\n",
            "Iteration 00454    Loss -0.042485  0.000943\n",
            "Iteration 00455    Loss -0.042551  0.001003\n",
            "Iteration 00456    Loss -0.042576  0.001009\n",
            "Iteration 00457    Loss -0.042607  0.000964\n",
            "Iteration 00458    Loss -0.042649  0.001005\n",
            "Iteration 00459    Loss -0.042685  0.000986\n",
            "Iteration 00460    Loss -0.042694  0.000978\n",
            "Iteration 00461    Loss -0.042711  0.001021\n",
            "Iteration 00462    Loss -0.042716  0.000951\n",
            "Iteration 00463    Loss -0.042681  0.001029\n",
            "Iteration 00464    Loss -0.042641  0.000891\n",
            "Iteration 00465    Loss -0.042620  0.001005\n",
            "Iteration 00466    Loss -0.042712  0.000900\n",
            "Iteration 00467    Loss -0.042744  0.000959\n",
            "Iteration 00468    Loss -0.042672  0.000922\n",
            "Iteration 00469    Loss -0.042755  0.000926\n",
            "Iteration 00470    Loss -0.042860  0.000897\n",
            "Iteration 00471    Loss -0.042891  0.000948\n",
            "Iteration 00472    Loss -0.042956  0.000950\n",
            "Iteration 00473    Loss -0.043016  0.000937\n",
            "Iteration 00474    Loss -0.043065  0.000984\n",
            "Iteration 00475    Loss -0.043092  0.000918\n",
            "Iteration 00476    Loss -0.043135  0.000942\n",
            "Iteration 00477    Loss -0.043194  0.000967\n",
            "Iteration 00478    Loss -0.043213  0.000951\n",
            "Iteration 00479    Loss -0.043222  0.000966\n",
            "Iteration 00480    Loss -0.043285  0.000964\n",
            "Iteration 00481    Loss -0.043296  0.000977\n",
            "Iteration 00482    Loss -0.043354  0.000980\n",
            "Iteration 00483    Loss -0.043377  0.000993\n",
            "Iteration 00484    Loss -0.043433  0.000960\n",
            "Iteration 00485    Loss -0.043456  0.000990\n",
            "Iteration 00486    Loss -0.043468  0.000985\n",
            "Iteration 00487    Loss -0.043511  0.000987\n",
            "Iteration 00488    Loss -0.043524  0.000999\n",
            "Iteration 00489    Loss -0.043538  0.000969\n",
            "Iteration 00490    Loss -0.043537  0.001010\n",
            "Iteration 00491    Loss -0.043496  0.000966\n",
            "Iteration 00492    Loss -0.043508  0.000944\n",
            "Iteration 00493    Loss -0.043537  0.001012\n",
            "Iteration 00494    Loss -0.043592  0.000890\n",
            "Iteration 00495    Loss -0.043649  0.001020\n",
            "Iteration 00496    Loss -0.043677  0.000930\n",
            "Iteration 00497    Loss -0.043703  0.000947\n",
            "Iteration 00498    Loss -0.043729  0.001006\n",
            "Iteration 00499    Loss -0.043787  0.000915\n",
            "100% 498/500 [4:53:42<01:11, 35.75s/it]/content/SOTS/indoor/hazy/1404_10.png\n",
            "Iteration 00000    Loss 0.536658  0.000023\n",
            "Iteration 00001    Loss 19.498646  0.000008\n",
            "Iteration 00002    Loss 0.493554  0.000003\n",
            "Iteration 00003    Loss 0.373695  0.000002\n",
            "Iteration 00004    Loss 0.307097  0.000002\n",
            "Iteration 00005    Loss 0.242886  0.000001\n",
            "Iteration 00006    Loss 0.207530  0.000001\n",
            "Iteration 00007    Loss 0.176630  0.000001\n",
            "Iteration 00008    Loss 0.150582  0.000001\n",
            "Iteration 00009    Loss 0.126853  0.000001\n",
            "Iteration 00010    Loss 0.107682  0.000001\n",
            "Iteration 00011    Loss 0.091856  0.000001\n",
            "Iteration 00012    Loss 0.078252  0.000001\n",
            "Iteration 00013    Loss 0.066955  0.000001\n",
            "Iteration 00014    Loss 0.058740  0.000002\n",
            "Iteration 00015    Loss 0.050911  0.000002\n",
            "Iteration 00016    Loss 0.044616  0.000002\n",
            "Iteration 00017    Loss 0.039690  0.000002\n",
            "Iteration 00018    Loss 0.034837  0.000002\n",
            "Iteration 00019    Loss 0.031586  0.000002\n",
            "Iteration 00020    Loss 0.028582  0.000002\n",
            "Iteration 00021    Loss 0.026390  0.000003\n",
            "Iteration 00022    Loss 0.024127  0.000003\n",
            "Iteration 00023    Loss 0.022009  0.000003\n",
            "Iteration 00024    Loss 0.020196  0.000003\n",
            "Iteration 00025    Loss 0.019247  0.000004\n",
            "Iteration 00026    Loss 0.017348  0.000004\n",
            "Iteration 00027    Loss 0.016199  0.000004\n",
            "Iteration 00028    Loss 0.015068  0.000005\n",
            "Iteration 00029    Loss 0.013885  0.000005\n",
            "Iteration 00030    Loss 0.012880  0.000005\n",
            "Iteration 00031    Loss 0.012389  0.000006\n",
            "Iteration 00032    Loss 0.011108  0.000006\n",
            "Iteration 00033    Loss 0.010197  0.000007\n",
            "Iteration 00034    Loss 0.009738  0.000008\n",
            "Iteration 00035    Loss 0.009360  0.000009\n",
            "Iteration 00036    Loss 0.008005  0.000010\n",
            "Iteration 00037    Loss 0.007346  0.000010\n",
            "Iteration 00038    Loss 0.006631  0.000011\n",
            "Iteration 00039    Loss 0.005748  0.000012\n",
            "Iteration 00040    Loss 0.005222  0.000014\n",
            "Iteration 00041    Loss 0.004755  0.000015\n",
            "Iteration 00042    Loss 0.003995  0.000016\n",
            "Iteration 00043    Loss 0.003582  0.000017\n",
            "Iteration 00044    Loss 0.002595  0.000019\n",
            "Iteration 00045    Loss 0.002168  0.000020\n",
            "Iteration 00046    Loss 0.001603  0.000022\n",
            "Iteration 00047    Loss 0.001032  0.000023\n",
            "Iteration 00048    Loss 0.000575  0.000024\n",
            "Iteration 00049    Loss -0.000014  0.000027\n",
            "Iteration 00050    Loss -0.000492  0.000029\n",
            "Iteration 00051    Loss -0.000990  0.000028\n",
            "Iteration 00052    Loss -0.001592  0.000030\n",
            "Iteration 00053    Loss -0.001950  0.000031\n",
            "Iteration 00054    Loss -0.002476  0.000034\n",
            "Iteration 00055    Loss -0.002882  0.000035\n",
            "Iteration 00056    Loss -0.003413  0.000035\n",
            "Iteration 00057    Loss -0.003776  0.000037\n",
            "Iteration 00058    Loss -0.004110  0.000040\n",
            "Iteration 00059    Loss -0.004603  0.000042\n",
            "Iteration 00060    Loss -0.005001  0.000039\n",
            "Iteration 00061    Loss -0.005441  0.000041\n",
            "Iteration 00062    Loss -0.005877  0.000044\n",
            "Iteration 00063    Loss -0.006223  0.000049\n",
            "Iteration 00064    Loss -0.006549  0.000051\n",
            "Iteration 00065    Loss -0.006965  0.000047\n",
            "Iteration 00066    Loss -0.007373  0.000049\n",
            "Iteration 00067    Loss -0.007729  0.000055\n",
            "Iteration 00068    Loss -0.008137  0.000056\n",
            "Iteration 00069    Loss -0.008505  0.000054\n",
            "Iteration 00070    Loss -0.008792  0.000056\n",
            "Iteration 00071    Loss -0.009209  0.000058\n",
            "Iteration 00072    Loss -0.009531  0.000061\n",
            "Iteration 00073    Loss -0.009794  0.000061\n",
            "Iteration 00074    Loss -0.010253  0.000062\n",
            "Iteration 00075    Loss -0.010557  0.000065\n",
            "Iteration 00076    Loss -0.010847  0.000065\n",
            "Iteration 00077    Loss -0.011178  0.000065\n",
            "Iteration 00078    Loss -0.011521  0.000069\n",
            "Iteration 00079    Loss -0.011837  0.000073\n",
            "Iteration 00080    Loss -0.012163  0.000071\n",
            "Iteration 00081    Loss -0.012450  0.000070\n",
            "Iteration 00082    Loss -0.012780  0.000073\n",
            "Iteration 00083    Loss -0.012986  0.000079\n",
            "Iteration 00084    Loss -0.013314  0.000081\n",
            "Iteration 00085    Loss -0.013653  0.000078\n",
            "Iteration 00086    Loss -0.013932  0.000081\n",
            "Iteration 00087    Loss -0.014279  0.000088\n",
            "Iteration 00088    Loss -0.014493  0.000089\n",
            "Iteration 00089    Loss -0.014760  0.000085\n",
            "Iteration 00090    Loss -0.014949  0.000085\n",
            "Iteration 00091    Loss -0.015155  0.000091\n",
            "Iteration 00092    Loss -0.015373  0.000088\n",
            "Iteration 00093    Loss -0.015688  0.000094\n",
            "Iteration 00094    Loss -0.015964  0.000087\n",
            "Iteration 00095    Loss -0.016196  0.000092\n",
            "Iteration 00096    Loss -0.016487  0.000105\n",
            "Iteration 00097    Loss -0.016784  0.000096\n",
            "Iteration 00098    Loss -0.016976  0.000096\n",
            "Iteration 00099    Loss -0.017219  0.000099\n",
            "Iteration 00100    Loss -0.017400  0.000101\n",
            "Iteration 00101    Loss -0.017751  0.000100\n",
            "Iteration 00102    Loss -0.018022  0.000103\n",
            "Iteration 00103    Loss -0.018211  0.000104\n",
            "Iteration 00104    Loss -0.018393  0.000104\n",
            "Iteration 00105    Loss -0.018622  0.000110\n",
            "Iteration 00106    Loss -0.018835  0.000107\n",
            "Iteration 00107    Loss -0.019137  0.000108\n",
            "Iteration 00108    Loss -0.019290  0.000110\n",
            "Iteration 00109    Loss -0.019594  0.000119\n",
            "Iteration 00110    Loss -0.019742  0.000112\n",
            "Iteration 00111    Loss -0.019943  0.000110\n",
            "Iteration 00112    Loss -0.020074  0.000126\n",
            "Iteration 00113    Loss -0.020369  0.000118\n",
            "Iteration 00114    Loss -0.020570  0.000118\n",
            "Iteration 00115    Loss -0.020744  0.000118\n",
            "Iteration 00116    Loss -0.020962  0.000121\n",
            "Iteration 00117    Loss -0.021133  0.000123\n",
            "Iteration 00118    Loss -0.021451  0.000122\n",
            "Iteration 00119    Loss -0.021587  0.000123\n",
            "Iteration 00120    Loss -0.021724  0.000122\n",
            "Iteration 00121    Loss -0.021937  0.000122\n",
            "Iteration 00122    Loss -0.022057  0.000117\n",
            "Iteration 00123    Loss -0.022246  0.000131\n",
            "Iteration 00124    Loss -0.022521  0.000129\n",
            "Iteration 00125    Loss -0.022704  0.000125\n",
            "Iteration 00126    Loss -0.022895  0.000128\n",
            "Iteration 00127    Loss -0.023073  0.000135\n",
            "Iteration 00128    Loss -0.023221  0.000133\n",
            "Iteration 00129    Loss -0.023452  0.000124\n",
            "Iteration 00130    Loss -0.023619  0.000138\n",
            "Iteration 00131    Loss -0.023811  0.000140\n",
            "Iteration 00132    Loss -0.023900  0.000127\n",
            "Iteration 00133    Loss -0.024148  0.000139\n",
            "Iteration 00134    Loss -0.024379  0.000139\n",
            "Iteration 00135    Loss -0.024538  0.000139\n",
            "Iteration 00136    Loss -0.024747  0.000147\n",
            "Iteration 00137    Loss -0.024912  0.000139\n",
            "Iteration 00138    Loss -0.025053  0.000144\n",
            "Iteration 00139    Loss -0.025237  0.000150\n",
            "Iteration 00140    Loss -0.025416  0.000144\n",
            "Iteration 00141    Loss -0.025561  0.000146\n",
            "Iteration 00142    Loss -0.025659  0.000146\n",
            "Iteration 00143    Loss -0.025857  0.000148\n",
            "Iteration 00144    Loss -0.025916  0.000146\n",
            "Iteration 00145    Loss -0.025838  0.000142\n",
            "Iteration 00146    Loss -0.026021  0.000144\n",
            "Iteration 00147    Loss -0.026338  0.000157\n",
            "Iteration 00148    Loss -0.026501  0.000143\n",
            "Iteration 00149    Loss -0.026602  0.000132\n",
            "Iteration 00150    Loss -0.026854  0.000150\n",
            "Iteration 00151    Loss -0.026977  0.000162\n",
            "Iteration 00152    Loss -0.027188  0.000137\n",
            "Iteration 00153    Loss -0.027341  0.000134\n",
            "Iteration 00154    Loss -0.027505  0.000154\n",
            "Iteration 00155    Loss -0.027606  0.000161\n",
            "Iteration 00156    Loss -0.027768  0.000142\n",
            "Iteration 00157    Loss -0.027952  0.000140\n",
            "Iteration 00158    Loss -0.028108  0.000154\n",
            "Iteration 00159    Loss -0.028262  0.000159\n",
            "Iteration 00160    Loss -0.028436  0.000149\n",
            "Iteration 00161    Loss -0.028579  0.000150\n",
            "Iteration 00162    Loss -0.028686  0.000161\n",
            "Iteration 00163    Loss -0.028827  0.000157\n",
            "Iteration 00164    Loss -0.028917  0.000148\n",
            "Iteration 00165    Loss -0.029135  0.000158\n",
            "Iteration 00166    Loss -0.029291  0.000174\n",
            "Iteration 00167    Loss -0.029384  0.000159\n",
            "Iteration 00168    Loss -0.029520  0.000155\n",
            "Iteration 00169    Loss -0.029625  0.000163\n",
            "Iteration 00170    Loss -0.029733  0.000166\n",
            "Iteration 00171    Loss -0.029897  0.000163\n",
            "Iteration 00172    Loss -0.030007  0.000170\n",
            "Iteration 00173    Loss -0.030040  0.000166\n",
            "Iteration 00174    Loss -0.030115  0.000163\n",
            "Iteration 00175    Loss -0.030281  0.000165\n",
            "Iteration 00176    Loss -0.030430  0.000170\n",
            "Iteration 00177    Loss -0.030560  0.000170\n",
            "Iteration 00178    Loss -0.030685  0.000172\n",
            "Iteration 00179    Loss -0.030822  0.000162\n",
            "Iteration 00180    Loss -0.030940  0.000170\n",
            "Iteration 00181    Loss -0.031056  0.000175\n",
            "Iteration 00182    Loss -0.031175  0.000168\n",
            "Iteration 00183    Loss -0.031281  0.000167\n",
            "Iteration 00184    Loss -0.031404  0.000175\n",
            "Iteration 00185    Loss -0.031575  0.000174\n",
            "Iteration 00186    Loss -0.031661  0.000179\n",
            "Iteration 00187    Loss -0.031764  0.000174\n",
            "Iteration 00188    Loss -0.031865  0.000178\n",
            "Iteration 00189    Loss -0.031948  0.000170\n",
            "Iteration 00190    Loss -0.032019  0.000168\n",
            "Iteration 00191    Loss -0.032162  0.000186\n",
            "Iteration 00192    Loss -0.032237  0.000190\n",
            "Iteration 00193    Loss -0.032334  0.000156\n",
            "Iteration 00194    Loss -0.032425  0.000177\n",
            "Iteration 00195    Loss -0.032589  0.000181\n",
            "Iteration 00196    Loss -0.032705  0.000169\n",
            "Iteration 00197    Loss -0.032766  0.000179\n",
            "Iteration 00198    Loss -0.032920  0.000177\n",
            "Iteration 00199    Loss -0.033035  0.000181\n",
            "Iteration 00200    Loss -0.033150  0.000179\n",
            "Iteration 00201    Loss -0.033260  0.000178\n",
            "Iteration 00202    Loss -0.033283  0.000184\n",
            "Iteration 00203    Loss -0.033418  0.000177\n",
            "Iteration 00204    Loss -0.033516  0.000188\n",
            "Iteration 00205    Loss -0.033607  0.000198\n",
            "Iteration 00206    Loss -0.033755  0.000183\n",
            "Iteration 00207    Loss -0.033846  0.000191\n",
            "Iteration 00208    Loss -0.033904  0.000195\n",
            "Iteration 00209    Loss -0.033985  0.000190\n",
            "Iteration 00210    Loss -0.034045  0.000197\n",
            "Iteration 00211    Loss -0.034126  0.000196\n",
            "Iteration 00212    Loss -0.034161  0.000182\n",
            "Iteration 00213    Loss -0.034265  0.000195\n",
            "Iteration 00214    Loss -0.034400  0.000190\n",
            "Iteration 00215    Loss -0.034506  0.000194\n",
            "Iteration 00216    Loss -0.034594  0.000201\n",
            "Iteration 00217    Loss -0.034626  0.000187\n",
            "Iteration 00218    Loss -0.034682  0.000187\n",
            "Iteration 00219    Loss -0.034865  0.000199\n",
            "Iteration 00220    Loss -0.034922  0.000199\n",
            "Iteration 00221    Loss -0.035033  0.000192\n",
            "Iteration 00222    Loss -0.035106  0.000198\n",
            "Iteration 00223    Loss -0.035120  0.000208\n",
            "Iteration 00224    Loss -0.035294  0.000197\n",
            "Iteration 00225    Loss -0.035330  0.000199\n",
            "Iteration 00226    Loss -0.035408  0.000194\n",
            "Iteration 00227    Loss -0.035461  0.000214\n",
            "Iteration 00228    Loss -0.035616  0.000208\n",
            "Iteration 00229    Loss -0.035661  0.000201\n",
            "Iteration 00230    Loss -0.035719  0.000208\n",
            "Iteration 00231    Loss -0.035781  0.000213\n",
            "Iteration 00232    Loss -0.035845  0.000196\n",
            "Iteration 00233    Loss -0.035944  0.000216\n",
            "Iteration 00234    Loss -0.036031  0.000212\n",
            "Iteration 00235    Loss -0.036112  0.000213\n",
            "Iteration 00236    Loss -0.036178  0.000214\n",
            "Iteration 00237    Loss -0.036257  0.000222\n",
            "Iteration 00238    Loss -0.036286  0.000213\n",
            "Iteration 00239    Loss -0.036376  0.000223\n",
            "Iteration 00240    Loss -0.036451  0.000219\n",
            "Iteration 00241    Loss -0.036483  0.000220\n",
            "Iteration 00242    Loss -0.036427  0.000221\n",
            "Iteration 00243    Loss -0.036408  0.000208\n",
            "Iteration 00244    Loss -0.036380  0.000206\n",
            "Iteration 00245    Loss -0.036501  0.000198\n",
            "Iteration 00246    Loss -0.036742  0.000203\n",
            "Iteration 00247    Loss -0.036742  0.000193\n",
            "Iteration 00248    Loss -0.036827  0.000173\n",
            "Iteration 00249    Loss -0.036922  0.000200\n",
            "Iteration 00250    Loss -0.037007  0.000211\n",
            "Iteration 00251    Loss -0.037078  0.000199\n",
            "Iteration 00252    Loss -0.037144  0.000195\n",
            "Iteration 00253    Loss -0.037168  0.000199\n",
            "Iteration 00254    Loss -0.037255  0.000197\n",
            "Iteration 00255    Loss -0.037377  0.000207\n",
            "Iteration 00256    Loss -0.037385  0.000209\n",
            "Iteration 00257    Loss -0.037453  0.000198\n",
            "Iteration 00258    Loss -0.037584  0.000205\n",
            "Iteration 00259    Loss -0.037625  0.000215\n",
            "Iteration 00260    Loss -0.037717  0.000215\n",
            "Iteration 00261    Loss -0.037780  0.000210\n",
            "Iteration 00262    Loss -0.037856  0.000212\n",
            "Iteration 00263    Loss -0.037847  0.000218\n",
            "Iteration 00264    Loss -0.037957  0.000216\n",
            "Iteration 00265    Loss -0.038032  0.000219\n",
            "Iteration 00266    Loss -0.038058  0.000226\n",
            "Iteration 00267    Loss -0.038128  0.000230\n",
            "Iteration 00268    Loss -0.038231  0.000230\n",
            "Iteration 00269    Loss -0.038243  0.000226\n",
            "Iteration 00270    Loss -0.038295  0.000228\n",
            "Iteration 00271    Loss -0.038374  0.000238\n",
            "Iteration 00272    Loss -0.038451  0.000236\n",
            "Iteration 00273    Loss -0.038496  0.000235\n",
            "Iteration 00274    Loss -0.038570  0.000242\n",
            "Iteration 00275    Loss -0.038612  0.000236\n",
            "Iteration 00276    Loss -0.038606  0.000248\n",
            "Iteration 00277    Loss -0.038555  0.000229\n",
            "Iteration 00278    Loss -0.038480  0.000239\n",
            "Iteration 00279    Loss -0.038607  0.000225\n",
            "Iteration 00280    Loss -0.038842  0.000235\n",
            "Iteration 00281    Loss -0.038857  0.000238\n",
            "Iteration 00282    Loss -0.038892  0.000209\n",
            "Iteration 00283    Loss -0.039033  0.000213\n",
            "Iteration 00284    Loss -0.039105  0.000231\n",
            "Iteration 00285    Loss -0.039134  0.000225\n",
            "Iteration 00286    Loss -0.039169  0.000224\n",
            "Iteration 00287    Loss -0.038905  0.000223\n",
            "Iteration 00288    Loss -0.039189  0.000228\n",
            "Iteration 00289    Loss -0.039315  0.000220\n",
            "Iteration 00290    Loss -0.039372  0.000227\n",
            "Iteration 00291    Loss -0.039487  0.000227\n",
            "Iteration 00292    Loss -0.039565  0.000233\n",
            "Iteration 00293    Loss -0.039643  0.000232\n",
            "Iteration 00294    Loss -0.039754  0.000221\n",
            "Iteration 00295    Loss -0.039818  0.000231\n",
            "Iteration 00296    Loss -0.039954  0.000233\n",
            "Iteration 00297    Loss -0.040030  0.000233\n",
            "Iteration 00298    Loss -0.040113  0.000233\n",
            "Iteration 00299    Loss -0.040237  0.000222\n",
            "Iteration 00300    Loss -0.040329  0.000232\n",
            "Iteration 00301    Loss -0.040423  0.000242\n",
            "Iteration 00302    Loss -0.040507  0.000226\n",
            "Iteration 00303    Loss -0.040604  0.000236\n",
            "Iteration 00304    Loss -0.040607  0.000247\n",
            "Iteration 00305    Loss -0.040665  0.000254\n",
            "Iteration 00306    Loss -0.040750  0.000239\n",
            "Iteration 00307    Loss -0.040845  0.000243\n",
            "Iteration 00308    Loss -0.040902  0.000257\n",
            "Iteration 00309    Loss -0.040970  0.000249\n",
            "Iteration 00310    Loss -0.041023  0.000254\n",
            "Iteration 00311    Loss -0.041095  0.000245\n",
            "Iteration 00312    Loss -0.041193  0.000261\n",
            "Iteration 00313    Loss -0.041253  0.000253\n",
            "Iteration 00314    Loss -0.041275  0.000253\n",
            "Iteration 00315    Loss -0.041355  0.000255\n",
            "Iteration 00316    Loss -0.041418  0.000244\n",
            "Iteration 00317    Loss -0.041488  0.000269\n",
            "Iteration 00318    Loss -0.041503  0.000248\n",
            "Iteration 00319    Loss -0.041571  0.000254\n",
            "Iteration 00320    Loss -0.041642  0.000272\n",
            "Iteration 00321    Loss -0.041704  0.000246\n",
            "Iteration 00322    Loss -0.041769  0.000261\n",
            "Iteration 00323    Loss -0.041789  0.000263\n",
            "Iteration 00324    Loss -0.041845  0.000264\n",
            "Iteration 00325    Loss -0.041918  0.000270\n",
            "Iteration 00326    Loss -0.041953  0.000259\n",
            "Iteration 00327    Loss -0.042005  0.000260\n",
            "Iteration 00328    Loss -0.042040  0.000268\n",
            "Iteration 00329    Loss -0.042094  0.000270\n",
            "Iteration 00330    Loss -0.042148  0.000265\n",
            "Iteration 00331    Loss -0.042198  0.000276\n",
            "Iteration 00332    Loss -0.042235  0.000260\n",
            "Iteration 00333    Loss -0.042292  0.000277\n",
            "Iteration 00334    Loss -0.042314  0.000265\n",
            "Iteration 00335    Loss -0.042356  0.000278\n",
            "Iteration 00336    Loss -0.042370  0.000269\n",
            "Iteration 00337    Loss -0.042402  0.000276\n",
            "Iteration 00338    Loss -0.042439  0.000260\n",
            "Iteration 00339    Loss -0.042539  0.000272\n",
            "Iteration 00340    Loss -0.042550  0.000260\n",
            "Iteration 00341    Loss -0.042534  0.000265\n",
            "Iteration 00342    Loss -0.042630  0.000263\n",
            "Iteration 00343    Loss -0.042603  0.000259\n",
            "Iteration 00344    Loss -0.042716  0.000268\n",
            "Iteration 00345    Loss -0.042712  0.000263\n",
            "Iteration 00346    Loss -0.042766  0.000261\n",
            "Iteration 00347    Loss -0.042748  0.000251\n",
            "Iteration 00348    Loss -0.042829  0.000278\n",
            "Iteration 00349    Loss -0.042874  0.000254\n",
            "Iteration 00350    Loss -0.042952  0.000265\n",
            "Iteration 00351    Loss -0.042951  0.000275\n",
            "Iteration 00352    Loss -0.043030  0.000251\n",
            "Iteration 00353    Loss -0.043030  0.000261\n",
            "Iteration 00354    Loss -0.043109  0.000264\n",
            "Iteration 00355    Loss -0.043128  0.000269\n",
            "Iteration 00356    Loss -0.043178  0.000271\n",
            "Iteration 00357    Loss -0.043217  0.000261\n",
            "Iteration 00358    Loss -0.043241  0.000277\n",
            "Iteration 00359    Loss -0.043280  0.000278\n",
            "Iteration 00360    Loss -0.043332  0.000277\n",
            "Iteration 00361    Loss -0.043340  0.000285\n",
            "Iteration 00362    Loss -0.043402  0.000269\n",
            "Iteration 00363    Loss -0.043408  0.000283\n",
            "Iteration 00364    Loss -0.043455  0.000269\n",
            "Iteration 00365    Loss -0.043462  0.000288\n",
            "Iteration 00366    Loss -0.043518  0.000268\n",
            "Iteration 00367    Loss -0.043527  0.000295\n",
            "Iteration 00368    Loss -0.043564  0.000268\n",
            "Iteration 00369    Loss -0.043605  0.000285\n",
            "Iteration 00370    Loss -0.043642  0.000279\n",
            "Iteration 00371    Loss -0.043678  0.000288\n",
            "Iteration 00372    Loss -0.043699  0.000284\n",
            "Iteration 00373    Loss -0.043729  0.000274\n",
            "Iteration 00374    Loss -0.043758  0.000298\n",
            "Iteration 00375    Loss -0.043802  0.000269\n",
            "Iteration 00376    Loss -0.043823  0.000289\n",
            "Iteration 00377    Loss -0.043862  0.000278\n",
            "Iteration 00378    Loss -0.043882  0.000287\n",
            "Iteration 00379    Loss -0.043916  0.000284\n",
            "Iteration 00380    Loss -0.043930  0.000284\n",
            "Iteration 00381    Loss -0.043982  0.000288\n",
            "Iteration 00382    Loss -0.044011  0.000279\n",
            "Iteration 00383    Loss -0.044022  0.000294\n",
            "Iteration 00384    Loss -0.044059  0.000280\n",
            "Iteration 00385    Loss -0.044088  0.000293\n",
            "Iteration 00386    Loss -0.044102  0.000295\n",
            "Iteration 00387    Loss -0.044144  0.000277\n",
            "Iteration 00388    Loss -0.044149  0.000299\n",
            "Iteration 00389    Loss -0.044172  0.000265\n",
            "Iteration 00390    Loss -0.044195  0.000306\n",
            "Iteration 00391    Loss -0.044206  0.000270\n",
            "Iteration 00392    Loss -0.044255  0.000278\n",
            "Iteration 00393    Loss -0.044288  0.000294\n",
            "Iteration 00394    Loss -0.044312  0.000277\n",
            "Iteration 00395    Loss -0.044367  0.000285\n",
            "Iteration 00396    Loss -0.044383  0.000290\n",
            "Iteration 00397    Loss -0.044414  0.000283\n",
            "Iteration 00398    Loss -0.044448  0.000289\n",
            "Iteration 00399    Loss -0.044464  0.000295\n",
            "Iteration 00400    Loss -0.044470  0.000297\n",
            "Iteration 00401    Loss -0.044495  0.000281\n",
            "Iteration 00402    Loss -0.044532  0.000300\n",
            "Iteration 00403    Loss -0.044564  0.000289\n",
            "Iteration 00404    Loss -0.044560  0.000302\n",
            "Iteration 00405    Loss -0.044604  0.000288\n",
            "Iteration 00406    Loss -0.044608  0.000283\n",
            "Iteration 00407    Loss -0.044645  0.000305\n",
            "Iteration 00408    Loss -0.044661  0.000287\n",
            "Iteration 00409    Loss -0.044696  0.000290\n",
            "Iteration 00410    Loss -0.044731  0.000307\n",
            "Iteration 00411    Loss -0.044740  0.000285\n",
            "Iteration 00412    Loss -0.044776  0.000295\n",
            "Iteration 00413    Loss -0.044788  0.000289\n",
            "Iteration 00414    Loss -0.044807  0.000304\n",
            "Iteration 00415    Loss -0.044823  0.000287\n",
            "Iteration 00416    Loss -0.044831  0.000308\n",
            "Iteration 00417    Loss -0.044850  0.000273\n",
            "Iteration 00418    Loss -0.044876  0.000313\n",
            "Iteration 00419    Loss -0.044905  0.000281\n",
            "Iteration 00420    Loss -0.044956  0.000291\n",
            "Iteration 00421    Loss -0.044970  0.000299\n",
            "Iteration 00422    Loss -0.044984  0.000290\n",
            "Iteration 00423    Loss -0.045025  0.000306\n",
            "Iteration 00424    Loss -0.045033  0.000291\n",
            "Iteration 00425    Loss -0.045066  0.000299\n",
            "Iteration 00426    Loss -0.045092  0.000310\n",
            "Iteration 00427    Loss -0.045097  0.000289\n",
            "Iteration 00428    Loss -0.045126  0.000310\n",
            "Iteration 00429    Loss -0.045128  0.000300\n",
            "Iteration 00430    Loss -0.045160  0.000309\n",
            "Iteration 00431    Loss -0.045182  0.000300\n",
            "Iteration 00432    Loss -0.045186  0.000305\n",
            "Iteration 00433    Loss -0.045160  0.000302\n",
            "Iteration 00434    Loss -0.045079  0.000292\n",
            "Iteration 00435    Loss -0.044978  0.000310\n",
            "Iteration 00436    Loss -0.045064  0.000266\n",
            "Iteration 00437    Loss -0.045221  0.000296\n",
            "Iteration 00438    Loss -0.045184  0.000283\n",
            "Iteration 00439    Loss -0.045154  0.000262\n",
            "Iteration 00440    Loss -0.045318  0.000276\n",
            "Iteration 00441    Loss -0.045271  0.000277\n",
            "Iteration 00442    Loss -0.045277  0.000276\n",
            "Iteration 00443    Loss -0.045337  0.000274\n",
            "Iteration 00444    Loss -0.045332  0.000306\n",
            "Iteration 00445    Loss -0.045377  0.000284\n",
            "Iteration 00446    Loss -0.045411  0.000273\n",
            "Iteration 00447    Loss -0.045444  0.000301\n",
            "Iteration 00448    Loss -0.045460  0.000297\n",
            "Iteration 00449    Loss -0.045483  0.000283\n",
            "Iteration 00450    Loss -0.045501  0.000286\n",
            "Iteration 00451    Loss -0.045523  0.000295\n",
            "Iteration 00452    Loss -0.045539  0.000283\n",
            "Iteration 00453    Loss -0.045565  0.000296\n",
            "Iteration 00454    Loss -0.045579  0.000292\n",
            "Iteration 00455    Loss -0.045604  0.000290\n",
            "Iteration 00456    Loss -0.045630  0.000299\n",
            "Iteration 00457    Loss -0.045636  0.000305\n",
            "Iteration 00458    Loss -0.045661  0.000296\n",
            "Iteration 00459    Loss -0.045687  0.000293\n",
            "Iteration 00460    Loss -0.045693  0.000306\n",
            "Iteration 00461    Loss -0.045710  0.000296\n",
            "Iteration 00462    Loss -0.045736  0.000301\n",
            "Iteration 00463    Loss -0.045744  0.000301\n",
            "Iteration 00464    Loss -0.045771  0.000299\n",
            "Iteration 00465    Loss -0.045784  0.000304\n",
            "Iteration 00466    Loss -0.045793  0.000313\n",
            "Iteration 00467    Loss -0.045813  0.000292\n",
            "Iteration 00468    Loss -0.045814  0.000313\n",
            "Iteration 00469    Loss -0.045765  0.000291\n",
            "Iteration 00470    Loss -0.045800  0.000315\n",
            "Iteration 00471    Loss -0.045819  0.000274\n",
            "Iteration 00472    Loss -0.045875  0.000310\n",
            "Iteration 00473    Loss -0.045873  0.000295\n",
            "Iteration 00474    Loss -0.045835  0.000284\n",
            "Iteration 00475    Loss -0.045910  0.000305\n",
            "Iteration 00476    Loss -0.045922  0.000301\n",
            "Iteration 00477    Loss -0.045942  0.000283\n",
            "Iteration 00478    Loss -0.045964  0.000299\n",
            "Iteration 00479    Loss -0.045966  0.000306\n",
            "Iteration 00480    Loss -0.045987  0.000284\n",
            "Iteration 00481    Loss -0.046015  0.000301\n",
            "Iteration 00482    Loss -0.046033  0.000309\n",
            "Iteration 00483    Loss -0.046053  0.000290\n",
            "Iteration 00484    Loss -0.046055  0.000307\n",
            "Iteration 00485    Loss -0.046082  0.000305\n",
            "Iteration 00486    Loss -0.046107  0.000301\n",
            "Iteration 00487    Loss -0.046114  0.000306\n",
            "Iteration 00488    Loss -0.046134  0.000296\n",
            "Iteration 00489    Loss -0.046140  0.000314\n",
            "Iteration 00490    Loss -0.046140  0.000299\n",
            "Iteration 00491    Loss -0.046156  0.000300\n",
            "Iteration 00492    Loss -0.046177  0.000295\n",
            "Iteration 00493    Loss -0.046188  0.000313\n",
            "Iteration 00494    Loss -0.046220  0.000301\n",
            "Iteration 00495    Loss -0.046233  0.000296\n",
            "Iteration 00496    Loss -0.046237  0.000309\n",
            "Iteration 00497    Loss -0.046257  0.000298\n",
            "Iteration 00498    Loss -0.046272  0.000309\n",
            "Iteration 00499    Loss -0.046302  0.000310\n",
            "100% 499/500 [4:54:17<00:35, 35.66s/it]/content/SOTS/indoor/hazy/1407_6.png\n",
            "Iteration 00000    Loss 0.451773  0.000017\n",
            "Iteration 00001    Loss 32.802700  0.000005\n",
            "Iteration 00002    Loss 0.408402  0.000002\n",
            "Iteration 00003    Loss 0.270980  0.000002\n",
            "Iteration 00004    Loss 0.345929  0.000001\n",
            "Iteration 00005    Loss 0.198553  0.000001\n",
            "Iteration 00006    Loss 0.169980  0.000001\n",
            "Iteration 00007    Loss 0.145395  0.000001\n",
            "Iteration 00008    Loss 0.127209  0.000001\n",
            "Iteration 00009    Loss 0.113214  0.000001\n",
            "Iteration 00010    Loss 0.099418  0.000001\n",
            "Iteration 00011    Loss 0.088919  0.000001\n",
            "Iteration 00012    Loss 0.080823  0.000001\n",
            "Iteration 00013    Loss 0.073795  0.000001\n",
            "Iteration 00014    Loss 0.066783  0.000002\n",
            "Iteration 00015    Loss 0.062151  0.000002\n",
            "Iteration 00016    Loss 0.057893  0.000002\n",
            "Iteration 00017    Loss 0.054373  0.000002\n",
            "Iteration 00018    Loss 0.051512  0.000002\n",
            "Iteration 00019    Loss 0.049242  0.000002\n",
            "Iteration 00020    Loss 0.046865  0.000003\n",
            "Iteration 00021    Loss 0.044677  0.000003\n",
            "Iteration 00022    Loss 0.042594  0.000004\n",
            "Iteration 00023    Loss 0.040902  0.000004\n",
            "Iteration 00024    Loss 0.038409  0.000004\n",
            "Iteration 00025    Loss 0.128143  0.000005\n",
            "Iteration 00026    Loss 0.035505  0.000006\n",
            "Iteration 00027    Loss 0.034966  0.000006\n",
            "Iteration 00028    Loss 0.033721  0.000007\n",
            "Iteration 00029    Loss 0.031959  0.000008\n",
            "Iteration 00030    Loss 0.031232  0.000008\n",
            "Iteration 00031    Loss 0.029314  0.000010\n",
            "Iteration 00032    Loss 0.028486  0.000010\n",
            "Iteration 00033    Loss 0.027783  0.000011\n",
            "Iteration 00034    Loss 0.027193  0.000012\n",
            "Iteration 00035    Loss 0.026320  0.000013\n",
            "Iteration 00036    Loss 0.025211  0.000014\n",
            "Iteration 00037    Loss 0.023888  0.000013\n",
            "Iteration 00038    Loss 0.023793  0.000015\n",
            "Iteration 00039    Loss 0.023068  0.000012\n",
            "Iteration 00040    Loss 0.022061  0.000011\n",
            "Iteration 00041    Loss 0.020789  0.000013\n",
            "Iteration 00042    Loss 0.019857  0.000015\n",
            "Iteration 00043    Loss 0.019317  0.000016\n",
            "Iteration 00044    Loss 0.018484  0.000016\n",
            "Iteration 00045    Loss 0.017742  0.000017\n",
            "Iteration 00046    Loss 0.016831  0.000018\n",
            "Iteration 00047    Loss 0.016150  0.000019\n",
            "Iteration 00048    Loss 0.015566  0.000020\n",
            "Iteration 00049    Loss 0.014818  0.000021\n",
            "Iteration 00050    Loss 0.014053  0.000022\n",
            "Iteration 00051    Loss 0.013378  0.000023\n",
            "Iteration 00052    Loss 0.012745  0.000024\n",
            "Iteration 00053    Loss 0.012194  0.000026\n",
            "Iteration 00054    Loss 0.011538  0.000027\n",
            "Iteration 00055    Loss 0.010930  0.000028\n",
            "Iteration 00056    Loss 0.010294  0.000029\n",
            "Iteration 00057    Loss 0.009806  0.000029\n",
            "Iteration 00058    Loss 0.009231  0.000030\n",
            "Iteration 00059    Loss 0.008636  0.000033\n",
            "Iteration 00060    Loss 0.008099  0.000035\n",
            "Iteration 00061    Loss 0.007510  0.000035\n",
            "Iteration 00062    Loss 0.007035  0.000035\n",
            "Iteration 00063    Loss 0.006494  0.000036\n",
            "Iteration 00064    Loss 0.005958  0.000038\n",
            "Iteration 00065    Loss 0.005439  0.000039\n",
            "Iteration 00066    Loss 0.004936  0.000041\n",
            "Iteration 00067    Loss 0.004431  0.000042\n",
            "Iteration 00068    Loss 0.003949  0.000044\n",
            "Iteration 00069    Loss 0.003455  0.000046\n",
            "Iteration 00070    Loss 0.002949  0.000048\n",
            "Iteration 00071    Loss 0.002507  0.000050\n",
            "Iteration 00072    Loss 0.002051  0.000052\n",
            "Iteration 00073    Loss 0.001571  0.000055\n",
            "Iteration 00074    Loss 0.001144  0.000057\n",
            "Iteration 00075    Loss 0.000655  0.000058\n",
            "Iteration 00076    Loss 0.000139  0.000060\n",
            "Iteration 00077    Loss -0.000345  0.000063\n",
            "Iteration 00078    Loss -0.000734  0.000064\n",
            "Iteration 00079    Loss -0.001166  0.000066\n",
            "Iteration 00080    Loss -0.001556  0.000068\n",
            "Iteration 00081    Loss -0.002079  0.000071\n",
            "Iteration 00082    Loss -0.002548  0.000073\n",
            "Iteration 00083    Loss -0.003005  0.000076\n",
            "Iteration 00084    Loss -0.003362  0.000079\n",
            "Iteration 00085    Loss -0.003924  0.000080\n",
            "Iteration 00086    Loss -0.004286  0.000085\n",
            "Iteration 00087    Loss -0.004479  0.000083\n",
            "Iteration 00088    Loss -0.005065  0.000087\n",
            "Iteration 00089    Loss -0.005319  0.000090\n",
            "Iteration 00090    Loss -0.005902  0.000092\n",
            "Iteration 00091    Loss -0.006181  0.000090\n",
            "Iteration 00092    Loss -0.006611  0.000096\n",
            "Iteration 00093    Loss -0.007033  0.000103\n",
            "Iteration 00094    Loss -0.007395  0.000096\n",
            "Iteration 00095    Loss -0.007772  0.000104\n",
            "Iteration 00096    Loss -0.008105  0.000108\n",
            "Iteration 00097    Loss -0.008485  0.000109\n",
            "Iteration 00098    Loss -0.008793  0.000108\n",
            "Iteration 00099    Loss -0.009195  0.000113\n",
            "Iteration 00100    Loss -0.009494  0.000120\n",
            "Iteration 00101    Loss -0.009864  0.000117\n",
            "Iteration 00102    Loss -0.010186  0.000123\n",
            "Iteration 00103    Loss -0.010553  0.000136\n",
            "Iteration 00104    Loss -0.010809  0.000131\n",
            "Iteration 00105    Loss -0.011183  0.000124\n",
            "Iteration 00106    Loss -0.011462  0.000130\n",
            "Iteration 00107    Loss -0.011791  0.000144\n",
            "Iteration 00108    Loss -0.012106  0.000139\n",
            "Iteration 00109    Loss -0.012414  0.000146\n",
            "Iteration 00110    Loss -0.012679  0.000148\n",
            "Iteration 00111    Loss -0.012964  0.000154\n",
            "Iteration 00112    Loss -0.013309  0.000147\n",
            "Iteration 00113    Loss -0.013566  0.000168\n",
            "Iteration 00114    Loss -0.013816  0.000145\n",
            "Iteration 00115    Loss -0.014157  0.000164\n",
            "Iteration 00116    Loss -0.014421  0.000171\n",
            "Iteration 00117    Loss -0.014698  0.000161\n",
            "Iteration 00118    Loss -0.014943  0.000168\n",
            "Iteration 00119    Loss -0.015207  0.000180\n",
            "Iteration 00120    Loss -0.015507  0.000193\n",
            "Iteration 00121    Loss -0.015736  0.000168\n",
            "Iteration 00122    Loss -0.015994  0.000158\n",
            "Iteration 00123    Loss -0.016242  0.000189\n",
            "Iteration 00124    Loss -0.016514  0.000181\n",
            "Iteration 00125    Loss -0.016740  0.000202\n",
            "Iteration 00126    Loss -0.017024  0.000196\n",
            "Iteration 00127    Loss -0.017287  0.000205\n",
            "Iteration 00128    Loss -0.017485  0.000208\n",
            "Iteration 00129    Loss -0.017714  0.000194\n",
            "Iteration 00130    Loss -0.017943  0.000215\n",
            "Iteration 00131    Loss -0.018172  0.000202\n",
            "Iteration 00132    Loss -0.018327  0.000212\n",
            "Iteration 00133    Loss -0.018468  0.000220\n",
            "Iteration 00134    Loss -0.018644  0.000207\n",
            "Iteration 00135    Loss -0.018960  0.000220\n",
            "Iteration 00136    Loss -0.019273  0.000213\n",
            "Iteration 00137    Loss -0.019404  0.000258\n",
            "Iteration 00138    Loss -0.019655  0.000222\n",
            "Iteration 00139    Loss -0.019955  0.000233\n",
            "Iteration 00140    Loss -0.020102  0.000250\n",
            "Iteration 00141    Loss -0.020325  0.000236\n",
            "Iteration 00142    Loss -0.020584  0.000241\n",
            "Iteration 00143    Loss -0.020743  0.000260\n",
            "Iteration 00144    Loss -0.020940  0.000247\n",
            "Iteration 00145    Loss -0.021148  0.000277\n",
            "Iteration 00146    Loss -0.021304  0.000233\n",
            "Iteration 00147    Loss -0.021573  0.000262\n",
            "Iteration 00148    Loss -0.021806  0.000264\n",
            "Iteration 00149    Loss -0.021941  0.000266\n",
            "Iteration 00150    Loss -0.022141  0.000294\n",
            "Iteration 00151    Loss -0.022360  0.000264\n",
            "Iteration 00152    Loss -0.022541  0.000307\n",
            "Iteration 00153    Loss -0.022713  0.000271\n",
            "Iteration 00154    Loss -0.022901  0.000314\n",
            "Iteration 00155    Loss -0.023078  0.000291\n",
            "Iteration 00156    Loss -0.023206  0.000303\n",
            "Iteration 00157    Loss -0.023327  0.000304\n",
            "Iteration 00158    Loss -0.023370  0.000256\n",
            "Iteration 00159    Loss -0.023762  0.000303\n",
            "Iteration 00160    Loss -0.023854  0.000292\n",
            "Iteration 00161    Loss -0.023998  0.000254\n",
            "Iteration 00162    Loss -0.024239  0.000293\n",
            "Iteration 00163    Loss -0.024361  0.000311\n",
            "Iteration 00164    Loss -0.024603  0.000273\n",
            "Iteration 00165    Loss -0.024742  0.000298\n",
            "Iteration 00166    Loss -0.024923  0.000310\n",
            "Iteration 00167    Loss -0.025116  0.000305\n",
            "Iteration 00168    Loss -0.025290  0.000325\n",
            "Iteration 00169    Loss -0.025440  0.000314\n",
            "Iteration 00170    Loss -0.025622  0.000349\n",
            "Iteration 00171    Loss -0.025742  0.000299\n",
            "Iteration 00172    Loss -0.025931  0.000337\n",
            "Iteration 00173    Loss -0.026099  0.000336\n",
            "Iteration 00174    Loss -0.026241  0.000345\n",
            "Iteration 00175    Loss -0.026414  0.000349\n",
            "Iteration 00176    Loss -0.026583  0.000344\n",
            "Iteration 00177    Loss -0.026710  0.000394\n",
            "Iteration 00178    Loss -0.026785  0.000315\n",
            "Iteration 00179    Loss -0.026861  0.000424\n",
            "Iteration 00180    Loss -0.026998  0.000284\n",
            "Iteration 00181    Loss -0.027210  0.000329\n",
            "Iteration 00182    Loss -0.027362  0.000357\n",
            "Iteration 00183    Loss -0.027437  0.000317\n",
            "Iteration 00184    Loss -0.027619  0.000332\n",
            "Iteration 00185    Loss -0.027753  0.000345\n",
            "Iteration 00186    Loss -0.027905  0.000341\n",
            "Iteration 00187    Loss -0.028064  0.000358\n",
            "Iteration 00188    Loss -0.028212  0.000357\n",
            "Iteration 00189    Loss -0.028375  0.000358\n",
            "Iteration 00190    Loss -0.028522  0.000370\n",
            "Iteration 00191    Loss -0.028671  0.000386\n",
            "Iteration 00192    Loss -0.028813  0.000392\n",
            "Iteration 00193    Loss -0.028943  0.000397\n",
            "Iteration 00194    Loss -0.029067  0.000395\n",
            "Iteration 00195    Loss -0.029228  0.000417\n",
            "Iteration 00196    Loss -0.029330  0.000409\n",
            "Iteration 00197    Loss -0.029478  0.000410\n",
            "Iteration 00198    Loss -0.029595  0.000425\n",
            "Iteration 00199    Loss -0.029720  0.000439\n",
            "Iteration 00200    Loss -0.029845  0.000435\n",
            "Iteration 00201    Loss -0.029967  0.000424\n",
            "Iteration 00202    Loss -0.030032  0.000468\n",
            "Iteration 00203    Loss -0.030094  0.000384\n",
            "Iteration 00204    Loss -0.030226  0.000441\n",
            "Iteration 00205    Loss -0.030393  0.000426\n",
            "Iteration 00206    Loss -0.030522  0.000419\n",
            "Iteration 00207    Loss -0.030597  0.000441\n",
            "Iteration 00208    Loss -0.030737  0.000420\n",
            "Iteration 00209    Loss -0.030868  0.000415\n",
            "Iteration 00210    Loss -0.030947  0.000443\n",
            "Iteration 00211    Loss -0.031128  0.000436\n",
            "Iteration 00212    Loss -0.031259  0.000447\n",
            "Iteration 00213    Loss -0.031345  0.000432\n",
            "Iteration 00214    Loss -0.031467  0.000487\n",
            "Iteration 00215    Loss -0.031570  0.000442\n",
            "Iteration 00216    Loss -0.031660  0.000482\n",
            "Iteration 00217    Loss -0.031716  0.000398\n",
            "Iteration 00218    Loss -0.031839  0.000495\n",
            "Iteration 00219    Loss -0.031909  0.000417\n",
            "Iteration 00220    Loss -0.031943  0.000473\n",
            "Iteration 00221    Loss -0.031917  0.000453\n",
            "Iteration 00222    Loss -0.032098  0.000440\n",
            "Iteration 00223    Loss -0.032346  0.000482\n",
            "Iteration 00224    Loss -0.032404  0.000458\n",
            "Iteration 00225    Loss -0.032455  0.000469\n",
            "Iteration 00226    Loss -0.032660  0.000455\n",
            "Iteration 00227    Loss -0.032718  0.000466\n",
            "Iteration 00228    Loss -0.032853  0.000472\n",
            "Iteration 00229    Loss -0.032981  0.000475\n",
            "Iteration 00230    Loss -0.033053  0.000473\n",
            "Iteration 00231    Loss -0.033155  0.000511\n",
            "Iteration 00232    Loss -0.033232  0.000478\n",
            "Iteration 00233    Loss -0.033358  0.000479\n",
            "Iteration 00234    Loss -0.033421  0.000515\n",
            "Iteration 00235    Loss -0.033532  0.000494\n",
            "Iteration 00236    Loss -0.033614  0.000460\n",
            "Iteration 00237    Loss -0.033705  0.000517\n",
            "Iteration 00238    Loss -0.033788  0.000467\n",
            "Iteration 00239    Loss -0.033872  0.000528\n",
            "Iteration 00240    Loss -0.033925  0.000408\n",
            "Iteration 00241    Loss -0.034035  0.000557\n",
            "Iteration 00242    Loss -0.034121  0.000425\n",
            "Iteration 00243    Loss -0.034278  0.000465\n",
            "Iteration 00244    Loss -0.034320  0.000549\n",
            "Iteration 00245    Loss -0.034403  0.000427\n",
            "Iteration 00246    Loss -0.034556  0.000476\n",
            "Iteration 00247    Loss -0.034583  0.000537\n",
            "Iteration 00248    Loss -0.034643  0.000458\n",
            "Iteration 00249    Loss -0.034791  0.000495\n",
            "Iteration 00250    Loss -0.034872  0.000511\n",
            "Iteration 00251    Loss -0.034920  0.000473\n",
            "Iteration 00252    Loss -0.035060  0.000506\n",
            "Iteration 00253    Loss -0.035120  0.000510\n",
            "Iteration 00254    Loss -0.035217  0.000485\n",
            "Iteration 00255    Loss -0.035273  0.000533\n",
            "Iteration 00256    Loss -0.035390  0.000516\n",
            "Iteration 00257    Loss -0.035437  0.000528\n",
            "Iteration 00258    Loss -0.035526  0.000505\n",
            "Iteration 00259    Loss -0.035590  0.000548\n",
            "Iteration 00260    Loss -0.035677  0.000499\n",
            "Iteration 00261    Loss -0.035754  0.000555\n",
            "Iteration 00262    Loss -0.035821  0.000486\n",
            "Iteration 00263    Loss -0.035941  0.000563\n",
            "Iteration 00264    Loss -0.036012  0.000519\n",
            "Iteration 00265    Loss -0.036092  0.000534\n",
            "Iteration 00266    Loss -0.036153  0.000558\n",
            "Iteration 00267    Loss -0.036248  0.000529\n",
            "Iteration 00268    Loss -0.036312  0.000571\n",
            "Iteration 00269    Loss -0.036392  0.000524\n",
            "Iteration 00270    Loss -0.036436  0.000589\n",
            "Iteration 00271    Loss -0.036474  0.000488\n",
            "Iteration 00272    Loss -0.036465  0.000595\n",
            "Iteration 00273    Loss -0.036478  0.000449\n",
            "Iteration 00274    Loss -0.036671  0.000557\n",
            "Iteration 00275    Loss -0.036719  0.000534\n",
            "Iteration 00276    Loss -0.036756  0.000475\n",
            "Iteration 00277    Loss -0.036880  0.000545\n",
            "Iteration 00278    Loss -0.036954  0.000542\n",
            "Iteration 00279    Loss -0.037011  0.000511\n",
            "Iteration 00280    Loss -0.037119  0.000539\n",
            "Iteration 00281    Loss -0.037113  0.000547\n",
            "Iteration 00282    Loss -0.037140  0.000544\n",
            "Iteration 00283    Loss -0.037191  0.000526\n",
            "Iteration 00284    Loss -0.037224  0.000536\n",
            "Iteration 00285    Loss -0.037346  0.000563\n",
            "Iteration 00286    Loss -0.037451  0.000500\n",
            "Iteration 00287    Loss -0.037537  0.000556\n",
            "Iteration 00288    Loss -0.037602  0.000541\n",
            "Iteration 00289    Loss -0.037687  0.000548\n",
            "Iteration 00290    Loss -0.037760  0.000562\n",
            "Iteration 00291    Loss -0.037823  0.000591\n",
            "Iteration 00292    Loss -0.037884  0.000557\n",
            "Iteration 00293    Loss -0.037961  0.000580\n",
            "Iteration 00294    Loss -0.038013  0.000591\n",
            "Iteration 00295    Loss -0.038071  0.000564\n",
            "Iteration 00296    Loss -0.038178  0.000569\n",
            "Iteration 00297    Loss -0.038223  0.000617\n",
            "Iteration 00298    Loss -0.038270  0.000565\n",
            "Iteration 00299    Loss -0.038342  0.000597\n",
            "Iteration 00300    Loss -0.038446  0.000571\n",
            "Iteration 00301    Loss -0.038463  0.000610\n",
            "Iteration 00302    Loss -0.038519  0.000543\n",
            "Iteration 00303    Loss -0.038482  0.000656\n",
            "Iteration 00304    Loss -0.038405  0.000438\n",
            "Iteration 00305    Loss -0.038560  0.000609\n",
            "Iteration 00306    Loss -0.038639  0.000569\n",
            "Iteration 00307    Loss -0.038664  0.000476\n",
            "Iteration 00308    Loss -0.038748  0.000568\n",
            "Iteration 00309    Loss -0.038856  0.000542\n",
            "Iteration 00310    Loss -0.038905  0.000516\n",
            "Iteration 00311    Loss -0.038982  0.000553\n",
            "Iteration 00312    Loss -0.039043  0.000553\n",
            "Iteration 00313    Loss -0.039100  0.000552\n",
            "Iteration 00314    Loss -0.039160  0.000554\n",
            "Iteration 00315    Loss -0.039219  0.000588\n",
            "Iteration 00316    Loss -0.039294  0.000566\n",
            "Iteration 00317    Loss -0.039349  0.000561\n",
            "Iteration 00318    Loss -0.039407  0.000612\n",
            "Iteration 00319    Loss -0.039486  0.000581\n",
            "Iteration 00320    Loss -0.039539  0.000570\n",
            "Iteration 00321    Loss -0.039586  0.000609\n",
            "Iteration 00322    Loss -0.039651  0.000592\n",
            "Iteration 00323    Loss -0.039702  0.000613\n",
            "Iteration 00324    Loss -0.039754  0.000620\n",
            "Iteration 00325    Loss -0.039817  0.000588\n",
            "Iteration 00326    Loss -0.039846  0.000643\n",
            "Iteration 00327    Loss -0.039892  0.000596\n",
            "Iteration 00328    Loss -0.039851  0.000663\n",
            "Iteration 00329    Loss -0.039820  0.000527\n",
            "Iteration 00330    Loss -0.039968  0.000647\n",
            "Iteration 00331    Loss -0.040079  0.000596\n",
            "Iteration 00332    Loss -0.040043  0.000535\n",
            "Iteration 00333    Loss -0.040162  0.000638\n",
            "Iteration 00334    Loss -0.040226  0.000592\n",
            "Iteration 00335    Loss -0.040264  0.000586\n",
            "Iteration 00336    Loss -0.040292  0.000638\n",
            "Iteration 00337    Loss -0.040354  0.000594\n",
            "Iteration 00338    Loss -0.040436  0.000613\n",
            "Iteration 00339    Loss -0.040480  0.000629\n",
            "Iteration 00340    Loss -0.040483  0.000587\n",
            "Iteration 00341    Loss -0.040572  0.000645\n",
            "Iteration 00342    Loss -0.040604  0.000611\n",
            "Iteration 00343    Loss -0.040578  0.000595\n",
            "Iteration 00344    Loss -0.040659  0.000647\n",
            "Iteration 00345    Loss -0.040739  0.000573\n",
            "Iteration 00346    Loss -0.040772  0.000658\n",
            "Iteration 00347    Loss -0.040769  0.000581\n",
            "Iteration 00348    Loss -0.040825  0.000656\n",
            "Iteration 00349    Loss -0.040923  0.000583\n",
            "Iteration 00350    Loss -0.040916  0.000628\n",
            "Iteration 00351    Loss -0.040932  0.000619\n",
            "Iteration 00352    Loss -0.041039  0.000610\n",
            "Iteration 00353    Loss -0.041087  0.000636\n",
            "Iteration 00354    Loss -0.041104  0.000619\n",
            "Iteration 00355    Loss -0.041148  0.000641\n",
            "Iteration 00356    Loss -0.041184  0.000636\n",
            "Iteration 00357    Loss -0.041235  0.000633\n",
            "Iteration 00358    Loss -0.041268  0.000645\n",
            "Iteration 00359    Loss -0.041254  0.000597\n",
            "Iteration 00360    Loss -0.041313  0.000671\n",
            "Iteration 00361    Loss -0.041297  0.000571\n",
            "Iteration 00362    Loss -0.041403  0.000636\n",
            "Iteration 00363    Loss -0.041459  0.000615\n",
            "Iteration 00364    Loss -0.041442  0.000629\n",
            "Iteration 00365    Loss -0.041513  0.000626\n",
            "Iteration 00366    Loss -0.041605  0.000620\n",
            "Iteration 00367    Loss -0.041648  0.000630\n",
            "Iteration 00368    Loss -0.041680  0.000632\n",
            "Iteration 00369    Loss -0.041696  0.000632\n",
            "Iteration 00370    Loss -0.041752  0.000653\n",
            "Iteration 00371    Loss -0.041785  0.000630\n",
            "Iteration 00372    Loss -0.041835  0.000643\n",
            "Iteration 00373    Loss -0.041886  0.000660\n",
            "Iteration 00374    Loss -0.041921  0.000643\n",
            "Iteration 00375    Loss -0.041952  0.000639\n",
            "Iteration 00376    Loss -0.041987  0.000682\n",
            "Iteration 00377    Loss -0.042013  0.000594\n",
            "Iteration 00378    Loss -0.042077  0.000707\n",
            "Iteration 00379    Loss -0.042046  0.000584\n",
            "Iteration 00380    Loss -0.042071  0.000696\n",
            "Iteration 00381    Loss -0.042106  0.000556\n",
            "Iteration 00382    Loss -0.042209  0.000675\n",
            "Iteration 00383    Loss -0.042216  0.000649\n",
            "Iteration 00384    Loss -0.042246  0.000588\n",
            "Iteration 00385    Loss -0.042278  0.000693\n",
            "Iteration 00386    Loss -0.042339  0.000601\n",
            "Iteration 00387    Loss -0.042417  0.000643\n",
            "Iteration 00388    Loss -0.042422  0.000690\n",
            "Iteration 00389    Loss -0.042452  0.000587\n",
            "Iteration 00390    Loss -0.042444  0.000670\n",
            "Iteration 00391    Loss -0.042513  0.000660\n",
            "Iteration 00392    Loss -0.042449  0.000597\n",
            "Iteration 00393    Loss -0.042459  0.000681\n",
            "Iteration 00394    Loss -0.042476  0.000571\n",
            "Iteration 00395    Loss -0.042615  0.000621\n",
            "Iteration 00396    Loss -0.042602  0.000638\n",
            "Iteration 00397    Loss -0.042593  0.000619\n",
            "Iteration 00398    Loss -0.042684  0.000637\n",
            "Iteration 00399    Loss -0.042705  0.000654\n",
            "Iteration 00400    Loss -0.042727  0.000617\n",
            "Iteration 00401    Loss -0.042800  0.000650\n",
            "Iteration 00402    Loss -0.042818  0.000630\n",
            "Iteration 00403    Loss -0.042832  0.000625\n",
            "Iteration 00404    Loss -0.042926  0.000652\n",
            "Iteration 00405    Loss -0.042889  0.000629\n",
            "Iteration 00406    Loss -0.042950  0.000656\n",
            "Iteration 00407    Loss -0.042970  0.000615\n",
            "Iteration 00408    Loss -0.043027  0.000687\n",
            "Iteration 00409    Loss -0.043077  0.000629\n",
            "Iteration 00410    Loss -0.043102  0.000637\n",
            "Iteration 00411    Loss -0.043151  0.000677\n",
            "Iteration 00412    Loss -0.043170  0.000633\n",
            "Iteration 00413    Loss -0.043229  0.000677\n",
            "Iteration 00414    Loss -0.043251  0.000639\n",
            "Iteration 00415    Loss -0.043309  0.000692\n",
            "Iteration 00416    Loss -0.043339  0.000634\n",
            "Iteration 00417    Loss -0.043353  0.000674\n",
            "Iteration 00418    Loss -0.043379  0.000676\n",
            "Iteration 00419    Loss -0.043410  0.000664\n",
            "Iteration 00420    Loss -0.043423  0.000650\n",
            "Iteration 00421    Loss -0.043453  0.000682\n",
            "Iteration 00422    Loss -0.043480  0.000664\n",
            "Iteration 00423    Loss -0.043476  0.000670\n",
            "Iteration 00424    Loss -0.043457  0.000628\n",
            "Iteration 00425    Loss -0.043502  0.000718\n",
            "Iteration 00426    Loss -0.043547  0.000572\n",
            "Iteration 00427    Loss -0.043583  0.000702\n",
            "Iteration 00428    Loss -0.043603  0.000635\n",
            "Iteration 00429    Loss -0.043664  0.000649\n",
            "Iteration 00430    Loss -0.043709  0.000675\n",
            "Iteration 00431    Loss -0.043719  0.000620\n",
            "Iteration 00432    Loss -0.043745  0.000686\n",
            "Iteration 00433    Loss -0.043794  0.000633\n",
            "Iteration 00434    Loss -0.043853  0.000672\n",
            "Iteration 00435    Loss -0.043874  0.000650\n",
            "Iteration 00436    Loss -0.043873  0.000665\n",
            "Iteration 00437    Loss -0.043904  0.000676\n",
            "Iteration 00438    Loss -0.043927  0.000641\n",
            "Iteration 00439    Loss -0.043946  0.000634\n",
            "Iteration 00440    Loss -0.043953  0.000714\n",
            "Iteration 00441    Loss -0.043981  0.000599\n",
            "Iteration 00442    Loss -0.044032  0.000694\n",
            "Iteration 00443    Loss -0.044102  0.000652\n",
            "Iteration 00444    Loss -0.044135  0.000659\n",
            "Iteration 00445    Loss -0.044172  0.000663\n",
            "Iteration 00446    Loss -0.044175  0.000653\n",
            "Iteration 00447    Loss -0.044231  0.000701\n",
            "Iteration 00448    Loss -0.044215  0.000617\n",
            "Iteration 00449    Loss -0.044275  0.000691\n",
            "Iteration 00450    Loss -0.044273  0.000659\n",
            "Iteration 00451    Loss -0.044286  0.000692\n",
            "Iteration 00452    Loss -0.044191  0.000605\n",
            "Iteration 00453    Loss -0.044273  0.000676\n",
            "Iteration 00454    Loss -0.044286  0.000669\n",
            "Iteration 00455    Loss -0.044377  0.000614\n",
            "Iteration 00456    Loss -0.044382  0.000686\n",
            "Iteration 00457    Loss -0.044366  0.000643\n",
            "Iteration 00458    Loss -0.044458  0.000672\n",
            "Iteration 00459    Loss -0.044451  0.000633\n",
            "Iteration 00460    Loss -0.044522  0.000670\n",
            "Iteration 00461    Loss -0.044540  0.000648\n",
            "Iteration 00462    Loss -0.044561  0.000666\n",
            "Iteration 00463    Loss -0.044598  0.000672\n",
            "Iteration 00464    Loss -0.044604  0.000646\n",
            "Iteration 00465    Loss -0.044657  0.000697\n",
            "Iteration 00466    Loss -0.044700  0.000656\n",
            "Iteration 00467    Loss -0.044716  0.000670\n",
            "Iteration 00468    Loss -0.044747  0.000683\n",
            "Iteration 00469    Loss -0.044754  0.000674\n",
            "Iteration 00470    Loss -0.044815  0.000687\n",
            "Iteration 00471    Loss -0.044803  0.000658\n",
            "Iteration 00472    Loss -0.044822  0.000728\n",
            "Iteration 00473    Loss -0.044767  0.000603\n",
            "Iteration 00474    Loss -0.044786  0.000740\n",
            "Iteration 00475    Loss -0.044820  0.000602\n",
            "Iteration 00476    Loss -0.044843  0.000688\n",
            "Iteration 00477    Loss -0.044848  0.000653\n",
            "Iteration 00478    Loss -0.044814  0.000625\n",
            "Iteration 00479    Loss -0.044829  0.000675\n",
            "Iteration 00480    Loss -0.044866  0.000640\n",
            "Iteration 00481    Loss -0.044970  0.000644\n",
            "Iteration 00482    Loss -0.044973  0.000660\n",
            "Iteration 00483    Loss -0.044971  0.000659\n",
            "Iteration 00484    Loss -0.045015  0.000635\n",
            "Iteration 00485    Loss -0.045004  0.000692\n",
            "Iteration 00486    Loss -0.045043  0.000619\n",
            "Iteration 00487    Loss -0.045065  0.000683\n",
            "Iteration 00488    Loss -0.045100  0.000644\n",
            "Iteration 00489    Loss -0.045142  0.000652\n",
            "Iteration 00490    Loss -0.045156  0.000662\n",
            "Iteration 00491    Loss -0.045191  0.000681\n",
            "Iteration 00492    Loss -0.045197  0.000641\n",
            "Iteration 00493    Loss -0.045215  0.000684\n",
            "Iteration 00494    Loss -0.045269  0.000682\n",
            "Iteration 00495    Loss -0.045233  0.000659\n",
            "Iteration 00496    Loss -0.045266  0.000693\n",
            "Iteration 00497    Loss -0.045305  0.000657\n",
            "Iteration 00498    Loss -0.045283  0.000709\n",
            "Iteration 00499    Loss -0.045199  0.000627\n",
            "100% 500/500 [4:54:53<00:00, 35.39s/it]\n"
          ]
        }
      ],
      "source": [
        "!python RW_dehazing.py -input /content/SOTS/indoor/hazy/ -output out/indoor/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ij5yD9n_TjR6",
        "outputId": "8b17348a-4273-4b5f-d711-0d0385d324fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "updating: out/ (stored 0%)\n",
            "updating: out/outdoor/ (stored 0%)\n",
            "updating: out/outdoor1759_0.8_0.12.jpg (deflated 0%)\n",
            "updating: out/outdoor1743_0.85_0.08.jpg (deflated 1%)\n",
            "updating: out/outdoor0039_0.9_0.2.jpg (deflated 0%)\n",
            "updating: out/outdoor1815_1_0.2.jpg (deflated 0%)\n",
            "updating: out/outdoor1950_1_0.08.jpg (deflated 0%)\n",
            "updating: out/outdoor0155_1_0.08.jpg (deflated 1%)\n",
            "updating: out/outdoor0284_0.8_0.16.jpg (deflated 0%)\n",
            "updating: out/outdoor0066_1_0.08.jpg (deflated 2%)\n",
            "updating: out/outdoor0282_0.85_0.16.jpg (deflated 0%)\n",
            "updating: out/outdoor1005_0.8_0.12.jpg (deflated 0%)\n",
            "updating: out/outdoor0101_0.9_0.08.jpg (deflated 0%)\n",
            "updating: out/outdoor1977_0.8_0.08.jpg (deflated 0%)\n",
            "updating: out/indoor/ (stored 0%)\n",
            "updating: out/outdoor0249_1_0.12.jpg (deflated 0%)\n",
            "updating: out/outdoor0385_0.8_0.08.jpg (deflated 1%)\n",
            "updating: out/outdoor0048_0.9_0.2.jpg (deflated 0%)\n",
            "updating: out/outdoor1048_0.9_0.2.jpg (deflated 0%)\n",
            "updating: out/outdoor0238_0.8_0.2.jpg (deflated 1%)\n",
            "updating: out/outdoor1981_0.8_0.2.jpg (deflated 0%)\n",
            "updating: out/outdoor0105_0.95_0.2.jpg (deflated 0%)\n",
            "updating: out/outdoor0259_0.9_0.08.jpg (deflated 1%)\n",
            "updating: out/outdoor1945_0.9_0.12.jpg (deflated 0%)\n",
            "updating: out/outdoor0251_0.8_0.16.jpg (deflated 0%)\n",
            "updating: out/outdoor0051_0.95_0.12.jpg (deflated 0%)\n",
            "updating: out/outdoor0054_0.85_0.16.jpg (deflated 0%)\n",
            "updating: out/outdoor0240_1_0.08.jpg (deflated 0%)\n",
            "updating: out/outdoor1756_0.9_0.12.jpg (deflated 0%)\n",
            "updating: out/outdoor0369_0.85_0.2.jpg (deflated 0%)\n",
            "updating: out/outdoor1006_1_0.16.jpg (deflated 0%)\n",
            "updating: out/outdoor0242_0.8_0.12.jpg (deflated 0%)\n",
            "updating: out/outdoor0181_0.9_0.08.jpg (deflated 0%)\n",
            "updating: out/outdoor0143_1_0.2.jpg (deflated 0%)\n",
            "updating: out/outdoor0110_0.8_0.12.jpg (deflated 0%)\n",
            "updating: out/outdoor0298_1_0.12.jpg (deflated 0%)\n",
            "updating: out/outdoor0331_0.95_0.12.jpg (deflated 0%)\n",
            "updating: out/outdoor1015_0.8_0.12.jpg (deflated 1%)\n",
            "updating: out/outdoor0297_0.95_0.08.jpg (deflated 0%)\n",
            "updating: out/outdoor0195_1_0.08.jpg (deflated 1%)\n",
            "updating: out/outdoor0196_0.9_0.2.jpg (deflated 0%)\n",
            "updating: out/outdoor0307_0.8_0.12.jpg (deflated 1%)\n",
            "updating: out/outdoor0286_0.8_0.08.jpg (deflated 0%)\n",
            "updating: out/outdoor0172_0.85_0.08.jpg (deflated 0%)\n",
            "updating: out/outdoor1853_0.85_0.08.jpg (deflated 0%)\n",
            "updating: out/outdoor1936_0.95_0.08.jpg (deflated 0%)\n",
            "updating: out/outdoor0106_0.9_0.08.jpg (deflated 0%)\n",
            "updating: out/outdoor0108_1_0.2.jpg (deflated 0%)\n",
            "updating: out/outdoor1872_0.95_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0249_1_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0143_1_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0369_0.85_0.2.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0172_0.85_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1756_0.9_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1015_0.8_0.12.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0155_1_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0105_0.95_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0106_0.9_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0066_1_0.08.jpg (deflated 2%)\n",
            "  adding: out/outdoor/0242_0.8_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1005_0.8_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1936_0.95_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0259_0.9_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1048_0.9_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1853_0.85_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0238_0.8_0.2.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1759_0.8_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0054_0.85_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0048_0.9_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0051_0.95_0.12.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1981_0.8_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1945_0.9_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0331_0.95_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0101_0.9_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1872_0.95_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0251_0.8_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1815_1_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0108_1_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0307_0.8_0.12.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0297_0.95_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0298_1_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1950_1_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0385_0.8_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0110_0.8_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0240_1_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0282_0.85_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0181_0.9_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1743_0.85_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0039_0.9_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0284_0.8_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0286_0.8_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0195_1_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0196_0.9_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1977_0.8_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1006_1_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1898_0.85_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0120_0.85_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0366_0.9_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0279_0.8_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0390_0.9_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1724_0.95_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1852_0.85_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0248_0.8_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1747_0.8_0.16.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1057_0.95_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0150_0.8_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0413_0.95_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1837_0.9_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0271_0.85_0.12.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0321_1_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1858_0.95_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1001_0.8_0.16.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0029_0.85_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0175_0.85_0.16.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1018_0.85_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0096_0.8_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1821_0.8_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1716_0.95_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0046_0.9_0.16.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1975_0.85_0.12.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0079_0.8_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0273_0.85_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1784_0.8_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1765_0.85_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0309_1_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0400_0.9_0.12.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1933_0.8_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0040_1_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0344_0.85_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1843_0.8_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1928_0.9_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1982_1_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0171_0.85_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1862_0.8_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0359_0.85_0.16.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0003_0.8_0.2.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1040_1_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0063_0.8_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1012_0.95_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0162_0.95_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0131_0.95_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0082_0.85_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0199_1_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0169_0.9_0.12.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1915_1_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0163_0.95_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0033_0.85_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0093_0.8_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0264_0.95_0.2.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0324_0.8_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1865_0.85_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0053_0.9_0.12.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0157_0.8_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1920_0.95_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1053_0.95_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1822_1_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0270_0.85_0.16.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0329_0.8_0.12.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1016_0.9_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1800_1_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0280_0.9_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0338_0.9_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0327_1_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0010_0.95_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0300_1_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0291_0.95_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0313_0.8_0.2.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1921_0.95_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0121_0.85_0.2.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1857_0.8_0.12.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1848_0.8_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0341_0.85_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1736_0.8_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0261_0.85_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1940_0.9_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0069_0.9_0.16.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1008_0.9_0.12.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1757_0.85_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1730_0.85_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1958_0.9_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1893_0.9_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0104_0.95_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0216_1_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0340_0.85_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0200_1_0.12.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1738_1_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0225_0.8_0.2.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1027_0.8_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0092_0.85_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0317_0.85_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0244_0.9_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0208_0.8_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0393_1_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0056_0.8_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0308_1_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0116_0.9_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0411_0.95_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1846_0.9_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0117_0.9_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0109_0.85_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0164_0.9_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1861_0.85_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0258_0.8_0.2.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1011_0.95_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0086_0.95_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0218_0.9_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0294_1_0.2.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1713_1_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0235_0.9_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0375_0.85_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0260_0.9_0.2.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0187_0.9_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0182_0.8_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1749_0.8_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0085_0.95_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0154_0.8_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1060_0.85_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0351_0.95_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0318_0.85_0.2.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1878_0.9_0.12.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0161_0.85_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1034_0.9_0.16.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0115_0.8_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0167_0.8_0.12.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1966_0.9_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0178_0.8_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1778_0.9_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0217_1_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1790_1_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0140_0.95_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0129_0.8_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0019_0.8_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1744_0.85_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0349_0.9_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1742_0.85_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0061_0.8_0.2.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0166_0.8_0.16.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0042_0.8_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0269_0.95_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0201_1_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0064_0.8_0.2.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1875_0.85_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0268_0.9_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1771_1_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0323_0.8_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0409_1_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1930_0.8_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1726_0.85_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0215_0.8_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1839_0.8_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1899_0.8_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1887_0.95_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0118_0.9_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0207_1_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0049_0.85_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0148_0.9_0.12.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0335_1_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0022_0.8_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0149_0.9_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0266_0.8_0.12.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0281_0.9_0.12.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1953_0.8_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0090_0.85_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1728_0.9_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1919_1_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0006_0.85_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1734_1_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0100_0.9_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0337_0.8_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0305_0.9_0.2.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0233_0.8_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1926_0.9_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0097_0.85_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0127_0.8_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0326_0.9_0.12.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0262_0.95_0.12.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0133_1_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1818_0.85_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1732_0.95_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0026_0.95_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1741_0.85_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0205_1_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0302_0.9_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1007_0.95_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1038_1_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0222_0.85_0.16.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1828_0.85_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0290_0.85_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0074_0.8_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1867_0.8_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0062_0.9_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0051_0.8_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0288_0.95_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1880_0.9_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0160_0.9_0.2.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0153_0.8_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0353_0.9_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0123_1_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1891_0.9_0.16.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1824_0.95_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1055_0.95_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0023_0.85_0.12.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0151_0.8_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0065_0.9_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0395_0.95_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1869_1_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1002_1_0.16.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0334_1_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0060_0.9_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1944_0.9_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1960_0.85_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0355_0.85_0.12.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1722_0.95_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0311_0.8_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1022_0.85_0.12.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0404_0.8_0.2.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0198_0.95_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0277_1_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0045_0.8_0.16.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0183_0.8_0.16.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1909_0.85_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0112_0.8_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0274_0.95_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1871_0.95_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0025_0.8_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0176_1_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0052_0.9_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0083_0.85_0.12.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1942_0.85_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0084_0.95_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0058_0.8_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0333_0.95_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1903_0.95_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0397_0.85_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1876_1_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0209_0.85_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1805_0.85_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0007_0.9_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1845_0.9_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1984_0.85_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0255_0.85_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0068_1_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0076_1_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0204_0.8_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1859_0.85_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0315_0.85_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1030_0.95_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0319_0.9_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0332_0.95_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0371_0.9_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0364_0.85_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0316_0.85_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0168_0.9_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0226_0.8_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1051_1_0.12.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1826_0.9_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1063_0.9_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0009_0.8_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0095_0.8_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0380_0.85_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1923_0.9_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0102_0.85_0.12.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0057_0.8_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1988_0.8_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1927_0.9_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1924_0.85_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0099_0.9_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0002_0.8_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0142_0.95_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0098_0.85_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0076_0.8_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0088_0.9_0.12.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0194_1_0.2.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0354_0.9_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0180_0.85_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0304_0.9_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1840_0.85_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0108_0.8_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0018_0.9_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0137_0.8_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0134_0.95_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1714_1_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1962_0.9_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1900_0.8_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0191_1_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1932_0.8_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1889_0.85_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0325_0.9_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0330_0.8_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0348_0.8_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0179_0.8_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0212_0.9_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1986_0.9_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0220_0.85_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0263_0.95_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1760_1_0.16.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0343_0.95_0.2.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0312_0.8_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0267_0.9_0.16.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0011_0.95_0.16.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1917_0.95_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0139_0.85_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0306_0.8_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0055_0.85_0.12.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0146_0.8_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0089_0.9_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0243_0.8_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0125_0.8_0.12.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0091_0.85_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0362_0.95_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0287_0.95_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0350_0.85_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1059_0.95_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1882_0.85_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0372_0.85_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0406_1_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0087_0.9_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1050_0.8_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1812_1_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0030_0.95_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0197_0.8_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0059_0.9_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0135_0.95_0.12.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1868_1_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0036_1_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0405_1_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0256_0.95_0.16.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0386_1_0.12.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1009_0.85_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0388_0.95_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0285_0.85_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0075_0.8_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0004_0.9_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0165_0.9_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0202_0.85_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0126_0.8_0.2.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0346_0.95_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0314_0.9_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0111_0.8_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0283_0.85_0.12.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0185_1_0.16.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0356_0.85_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1964_0.85_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1046_0.9_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1881_0.8_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0086_0.8_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1020_0.95_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0345_0.85_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0292_1_0.16.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1971_1_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1913_0.95_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1044_0.85_0.12.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0021_0.8_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1731_0.85_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0072_0.9_0.2.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0295_0.85_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0402_0.8_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1970_0.9_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0113_0.9_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0073_0.8_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0034_0.9_0.16.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0276_1_0.12.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0213_1_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0070_0.9_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0145_0.8_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0239_0.8_0.2.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0299_0.9_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0047_0.9_0.12.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0119_0.85_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1718_0.9_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0357_0.95_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1968_0.8_0.12.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0077_0.8_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0228_1_0.2.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1753_0.9_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0253_0.8_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1832_0.95_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0391_1_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1879_0.9_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0399_0.8_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1849_1_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0024_0.85_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0184_0.95_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0174_0.95_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0219_0.85_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1855_0.9_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0071_0.8_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1873_0.85_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1831_0.8_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1851_0.85_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0320_0.9_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0158_0.8_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0152_0.8_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1042_0.95_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1956_0.8_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0017_0.9_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1874_0.85_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1781_0.8_0.2.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1896_0.9_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1934_1_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0246_0.85_0.12.jpg (deflated 2%)\n",
            "  adding: out/outdoor/0223_0.95_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0188_0.85_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0320_1_0.16.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0094_0.8_0.2.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0170_0.8_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0296_0.95_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0210_0.85_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1973_0.95_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0016_0.8_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1037_0.8_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0287_0.8_0.2.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1877_0.9_0.12.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1947_0.8_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1954_1_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0237_0.8_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0392_1_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1709_0.95_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0303_0.9_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0189_0.85_0.2.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0272_0.85_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0383_0.9_0.2.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0206_1_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0081_0.8_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0245_1_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0132_1_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0361_0.85_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0382_0.9_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1863_1_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/0138_0.9_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1834_1_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0141_0.95_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0230_0.9_0.16.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1883_0.8_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0330_0.95_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0014_0.8_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1949_0.8_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0275_1_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0342_0.95_0.08.jpg (deflated 1%)\n",
            "  adding: out/outdoor/1938_0.85_0.12.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0001_0.8_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0253_1_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1010_0.85_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1931_0.8_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0407_0.95_0.16.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1774_0.95_0.2.jpg (deflated 0%)\n",
            "  adding: out/outdoor/1712_0.8_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0107_0.9_0.08.jpg (deflated 0%)\n",
            "  adding: out/outdoor/0147_1_0.16.jpg (deflated 1%)\n",
            "  adding: out/indoor/1447_7.jpg (deflated 0%)\n",
            "  adding: out/indoor/1430_6.jpg (deflated 1%)\n",
            "  adding: out/indoor/1446_3.jpg (deflated 0%)\n",
            "  adding: out/indoor/1403_2.jpg (deflated 0%)\n",
            "  adding: out/indoor/1401_8.jpg (deflated 0%)\n",
            "  adding: out/indoor/1438_8.jpg (deflated 1%)\n",
            "  adding: out/indoor/1410_9.jpg (deflated 0%)\n",
            "  adding: out/indoor/1418_10.jpg (deflated 0%)\n",
            "  adding: out/indoor/1414_7.jpg (deflated 1%)\n",
            "  adding: out/indoor/1440_10.jpg (deflated 1%)\n",
            "  adding: out/indoor/1405_4.jpg (deflated 0%)\n",
            "  adding: out/indoor/1405_9.jpg (deflated 1%)\n",
            "  adding: out/indoor/1430_9.jpg (deflated 1%)\n",
            "  adding: out/indoor/1427_2.jpg (deflated 1%)\n",
            "  adding: out/indoor/1433_5.jpg (deflated 0%)\n",
            "  adding: out/indoor/1445_9.jpg (deflated 0%)\n",
            "  adding: out/indoor/1412_10.jpg (deflated 1%)\n",
            "  adding: out/indoor/1448_8.jpg (deflated 1%)\n",
            "  adding: out/indoor/1427_1.jpg (deflated 1%)\n",
            "  adding: out/indoor/1400_2.jpg (deflated 0%)\n",
            "  adding: out/indoor/1414_3.jpg (deflated 1%)\n",
            "  adding: out/indoor/1418_2.jpg (deflated 0%)\n",
            "  adding: out/indoor/1440_3.jpg (deflated 1%)\n",
            "  adding: out/indoor/1431_4.jpg (deflated 1%)\n",
            "  adding: out/indoor/1412_7.jpg (deflated 1%)\n",
            "  adding: out/indoor/1420_10.jpg (deflated 0%)\n",
            "  adding: out/indoor/1409_9.jpg (deflated 0%)\n",
            "  adding: out/indoor/1432_7.jpg (deflated 1%)\n",
            "  adding: out/indoor/1445_7.jpg (deflated 0%)\n",
            "  adding: out/indoor/1419_7.jpg (deflated 0%)\n",
            "  adding: out/indoor/1401_7.jpg (deflated 0%)\n",
            "  adding: out/indoor/1424_10.jpg (deflated 0%)\n",
            "  adding: out/indoor/1403_1.jpg (deflated 0%)\n",
            "  adding: out/indoor/1416_4.jpg (deflated 0%)\n",
            "  adding: out/indoor/1440_5.jpg (deflated 1%)\n",
            "  adding: out/indoor/1419_5.jpg (deflated 0%)\n",
            "  adding: out/indoor/1409_6.jpg (deflated 0%)\n",
            "  adding: out/indoor/1427_10.jpg (deflated 1%)\n",
            "  adding: out/indoor/1428_7.jpg (deflated 0%)\n",
            "  adding: out/indoor/1422_8.jpg (deflated 0%)\n",
            "  adding: out/indoor/1445_5.jpg (deflated 0%)\n",
            "  adding: out/indoor/1424_7.jpg (deflated 0%)\n",
            "  adding: out/indoor/1429_5.jpg (deflated 0%)\n",
            "  adding: out/indoor/1425_7.jpg (deflated 0%)\n",
            "  adding: out/indoor/1416_2.jpg (deflated 0%)\n",
            "  adding: out/indoor/1428_2.jpg (deflated 0%)\n",
            "  adding: out/indoor/1441_4.jpg (deflated 1%)\n",
            "  adding: out/indoor/1418_5.jpg (deflated 0%)\n",
            "  adding: out/indoor/1448_7.jpg (deflated 1%)\n",
            "  adding: out/indoor/1441_10.jpg (deflated 1%)\n",
            "  adding: out/indoor/1440_1.jpg (deflated 1%)\n",
            "  adding: out/indoor/1437_9.jpg (deflated 1%)\n",
            "  adding: out/indoor/1438_2.jpg (deflated 1%)\n",
            "  adding: out/indoor/1449_4.jpg (deflated 0%)\n",
            "  adding: out/indoor/1444_5.jpg (deflated 0%)\n",
            "  adding: out/indoor/1419_1.jpg (deflated 0%)\n",
            "  adding: out/indoor/1413_1.jpg (deflated 0%)\n",
            "  adding: out/indoor/1430_5.jpg (deflated 1%)\n",
            "  adding: out/indoor/1423_8.jpg (deflated 0%)\n",
            "  adding: out/indoor/1449_1.jpg (deflated 0%)\n",
            "  adding: out/indoor/1441_3.jpg (deflated 1%)\n",
            "  adding: out/indoor/1428_5.jpg (deflated 0%)\n",
            "  adding: out/indoor/1423_4.jpg (deflated 0%)\n",
            "  adding: out/indoor/1417_3.jpg (deflated 0%)\n",
            "  adding: out/indoor/1409_10.jpg (deflated 0%)\n",
            "  adding: out/indoor/1405_7.jpg (deflated 1%)\n",
            "  adding: out/indoor/1407_5.jpg (deflated 0%)\n",
            "  adding: out/indoor/1412_2.jpg (deflated 1%)\n",
            "  adding: out/indoor/1447_1.jpg (deflated 0%)\n",
            "  adding: out/indoor/1413_7.jpg (deflated 0%)\n",
            "  adding: out/indoor/1427_4.jpg (deflated 1%)\n",
            "  adding: out/indoor/1443_8.jpg (deflated 1%)\n",
            "  adding: out/indoor/1437_3.jpg (deflated 1%)\n",
            "  adding: out/indoor/1439_6.jpg (deflated 0%)\n",
            "  adding: out/indoor/1411_6.jpg (deflated 2%)\n",
            "  adding: out/indoor/1442_10.jpg (deflated 0%)\n",
            "  adding: out/indoor/1406_6.jpg (deflated 0%)\n",
            "  adding: out/indoor/1422_10.jpg (deflated 0%)\n",
            "  adding: out/indoor/1443_4.jpg (deflated 0%)\n",
            "  adding: out/indoor/1407_1.jpg (deflated 0%)\n",
            "  adding: out/indoor/1438_1.jpg (deflated 0%)\n",
            "  adding: out/indoor/1404_6.jpg (deflated 1%)\n",
            "  adding: out/indoor/1420_9.jpg (deflated 0%)\n",
            "  adding: out/indoor/1402_5.jpg (deflated 1%)\n",
            "  adding: out/indoor/1407_10.jpg (deflated 0%)\n",
            "  adding: out/indoor/1430_2.jpg (deflated 1%)\n",
            "  adding: out/indoor/1401_4.jpg (deflated 0%)\n",
            "  adding: out/indoor/1441_8.jpg (deflated 1%)\n",
            "  adding: out/indoor/1447_4.jpg (deflated 0%)\n",
            "  adding: out/indoor/1409_3.jpg (deflated 0%)\n",
            "  adding: out/indoor/1406_1.jpg (deflated 0%)\n",
            "  adding: out/indoor/1411_10.jpg (deflated 2%)\n",
            "  adding: out/indoor/1437_10.jpg (deflated 1%)\n",
            "  adding: out/indoor/1423_5.jpg (deflated 0%)\n",
            "  adding: out/indoor/1422_9.jpg (deflated 0%)\n",
            "  adding: out/indoor/1422_1.jpg (deflated 0%)\n",
            "  adding: out/indoor/1446_10.jpg (deflated 0%)\n",
            "  adding: out/indoor/1411_2.jpg (deflated 1%)\n",
            "  adding: out/indoor/1427_5.jpg (deflated 1%)\n",
            "  adding: out/indoor/1436_8.jpg (deflated 0%)\n",
            "  adding: out/indoor/1407_6.jpg (deflated 0%)\n",
            "  adding: out/indoor/1425_9.jpg (deflated 0%)\n",
            "  adding: out/indoor/1449_3.jpg (deflated 0%)\n",
            "  adding: out/indoor/1445_10.jpg (deflated 0%)\n",
            "  adding: out/indoor/1432_5.jpg (deflated 1%)\n",
            "  adding: out/indoor/1433_8.jpg (deflated 0%)\n",
            "  adding: out/indoor/1427_9.jpg (deflated 1%)\n",
            "  adding: out/indoor/1430_3.jpg (deflated 1%)\n",
            "  adding: out/indoor/1412_8.jpg (deflated 1%)\n",
            "  adding: out/indoor/1401_5.jpg (deflated 0%)\n",
            "  adding: out/indoor/1442_4.jpg (deflated 0%)\n",
            "  adding: out/indoor/1449_2.jpg (deflated 0%)\n",
            "  adding: out/indoor/1437_6.jpg (deflated 1%)\n",
            "  adding: out/indoor/1431_1.jpg (deflated 1%)\n",
            "  adding: out/indoor/1415_7.jpg (deflated 1%)\n",
            "  adding: out/indoor/1406_7.jpg (deflated 1%)\n",
            "  adding: out/indoor/1438_7.jpg (deflated 0%)\n",
            "  adding: out/indoor/1425_6.jpg (deflated 0%)\n",
            "  adding: out/indoor/1408_1.jpg (deflated 0%)\n",
            "  adding: out/indoor/1425_4.jpg (deflated 0%)\n",
            "  adding: out/indoor/1414_2.jpg (deflated 1%)\n",
            "  adding: out/indoor/1419_3.jpg (deflated 0%)\n",
            "  adding: out/indoor/1406_8.jpg (deflated 1%)\n",
            "  adding: out/indoor/1414_10.jpg (deflated 1%)\n",
            "  adding: out/indoor/1438_5.jpg (deflated 0%)\n",
            "  adding: out/indoor/1439_5.jpg (deflated 0%)\n",
            "  adding: out/indoor/1418_4.jpg (deflated 0%)\n",
            "  adding: out/indoor/1410_7.jpg (deflated 1%)\n",
            "  adding: out/indoor/1448_2.jpg (deflated 0%)\n",
            "  adding: out/indoor/1416_1.jpg (deflated 0%)\n",
            "  adding: out/indoor/1407_9.jpg (deflated 0%)\n",
            "  adding: out/indoor/1448_6.jpg (deflated 0%)\n",
            "  adding: out/indoor/1446_9.jpg (deflated 0%)\n",
            "  adding: out/indoor/1438_4.jpg (deflated 0%)\n",
            "  adding: out/indoor/1443_6.jpg (deflated 0%)\n",
            "  adding: out/indoor/1409_5.jpg (deflated 0%)\n",
            "  adding: out/indoor/1414_8.jpg (deflated 1%)\n",
            "  adding: out/indoor/1420_6.jpg (deflated 0%)\n",
            "  adding: out/indoor/1444_4.jpg (deflated 0%)\n",
            "  adding: out/indoor/1429_8.jpg (deflated 0%)\n",
            "  adding: out/indoor/1424_8.jpg (deflated 0%)\n",
            "  adding: out/indoor/1442_8.jpg (deflated 0%)\n",
            "  adding: out/indoor/1421_4.jpg (deflated 0%)\n",
            "  adding: out/indoor/1403_7.jpg (deflated 0%)\n",
            "  adding: out/indoor/1429_3.jpg (deflated 0%)\n",
            "  adding: out/indoor/1408_5.jpg (deflated 0%)\n",
            "  adding: out/indoor/1423_2.jpg (deflated 0%)\n",
            "  adding: out/indoor/1425_3.jpg (deflated 0%)\n",
            "  adding: out/indoor/1441_7.jpg (deflated 1%)\n",
            "  adding: out/indoor/1440_9.jpg (deflated 1%)\n",
            "  adding: out/indoor/1447_5.jpg (deflated 0%)\n",
            "  adding: out/indoor/1432_3.jpg (deflated 1%)\n",
            "  adding: out/indoor/1445_8.jpg (deflated 0%)\n",
            "  adding: out/indoor/1416_8.jpg (deflated 0%)\n",
            "  adding: out/indoor/1430_10.jpg (deflated 1%)\n",
            "  adding: out/indoor/1401_3.jpg (deflated 0%)\n",
            "  adding: out/indoor/1405_3.jpg (deflated 1%)\n",
            "  adding: out/indoor/1408_3.jpg (deflated 0%)\n",
            "  adding: out/indoor/1447_2.jpg (deflated 0%)\n",
            "  adding: out/indoor/1442_3.jpg (deflated 0%)\n",
            "  adding: out/indoor/1403_3.jpg (deflated 0%)\n",
            "  adding: out/indoor/1430_7.jpg (deflated 1%)\n",
            "  adding: out/indoor/1418_7.jpg (deflated 0%)\n",
            "  adding: out/indoor/1405_10.jpg (deflated 1%)\n",
            "  adding: out/indoor/1409_7.jpg (deflated 0%)\n",
            "  adding: out/indoor/1416_5.jpg (deflated 0%)\n",
            "  adding: out/indoor/1423_7.jpg (deflated 0%)\n",
            "  adding: out/indoor/1448_9.jpg (deflated 1%)\n",
            "  adding: out/indoor/1431_8.jpg (deflated 1%)\n",
            "  adding: out/indoor/1407_2.jpg (deflated 0%)\n",
            "  adding: out/indoor/1420_8.jpg (deflated 0%)\n",
            "  adding: out/indoor/1426_5.jpg (deflated 0%)\n",
            "  adding: out/indoor/1444_2.jpg (deflated 0%)\n",
            "  adding: out/indoor/1436_7.jpg (deflated 0%)\n",
            "  adding: out/indoor/1427_6.jpg (deflated 0%)\n",
            "  adding: out/indoor/1410_4.jpg (deflated 1%)\n",
            "  adding: out/indoor/1423_9.jpg (deflated 0%)\n",
            "  adding: out/indoor/1414_5.jpg (deflated 1%)\n",
            "  adding: out/indoor/1435_1.jpg (deflated 0%)\n",
            "  adding: out/indoor/1449_9.jpg (deflated 0%)\n",
            "  adding: out/indoor/1445_2.jpg (deflated 0%)\n",
            "  adding: out/indoor/1446_6.jpg (deflated 0%)\n",
            "  adding: out/indoor/1447_10.jpg (deflated 0%)\n",
            "  adding: out/indoor/1410_1.jpg (deflated 1%)\n",
            "  adding: out/indoor/1400_4.jpg (deflated 0%)\n",
            "  adding: out/indoor/1434_9.jpg (deflated 0%)\n",
            "  adding: out/indoor/1435_9.jpg (deflated 1%)\n",
            "  adding: out/indoor/1415_1.jpg (deflated 1%)\n",
            "  adding: out/indoor/1403_9.jpg (deflated 0%)\n",
            "  adding: out/indoor/1403_8.jpg (deflated 0%)\n",
            "  adding: out/indoor/1420_1.jpg (deflated 0%)\n",
            "  adding: out/indoor/1429_1.jpg (deflated 0%)\n",
            "  adding: out/indoor/1433_4.jpg (deflated 0%)\n",
            "  adding: out/indoor/1426_7.jpg (deflated 0%)\n",
            "  adding: out/indoor/1433_7.jpg (deflated 0%)\n",
            "  adding: out/indoor/1413_10.jpg (deflated 0%)\n",
            "  adding: out/indoor/1434_4.jpg (deflated 1%)\n",
            "  adding: out/indoor/1426_3.jpg (deflated 0%)\n",
            "  adding: out/indoor/1428_6.jpg (deflated 0%)\n",
            "  adding: out/indoor/1441_5.jpg (deflated 1%)\n",
            "  adding: out/indoor/1444_1.jpg (deflated 0%)\n",
            "  adding: out/indoor/1436_2.jpg (deflated 0%)\n",
            "  adding: out/indoor/1435_8.jpg (deflated 1%)\n",
            "  adding: out/indoor/1432_8.jpg (deflated 1%)\n",
            "  adding: out/indoor/1433_2.jpg (deflated 0%)\n",
            "  adding: out/indoor/1421_7.jpg (deflated 0%)\n",
            "  adding: out/indoor/1402_1.jpg (deflated 1%)\n",
            "  adding: out/indoor/1441_1.jpg (deflated 1%)\n",
            "  adding: out/indoor/1443_3.jpg (deflated 0%)\n",
            "  adding: out/indoor/1436_1.jpg (deflated 0%)\n",
            "  adding: out/indoor/1438_6.jpg (deflated 0%)\n",
            "  adding: out/indoor/1411_4.jpg (deflated 2%)\n",
            "  adding: out/indoor/1419_10.jpg (deflated 0%)\n",
            "  adding: out/indoor/1449_10.jpg (deflated 0%)\n",
            "  adding: out/indoor/1434_2.jpg (deflated 1%)\n",
            "  adding: out/indoor/1424_4.jpg (deflated 0%)\n",
            "  adding: out/indoor/1408_7.jpg (deflated 0%)\n",
            "  adding: out/indoor/1400_10.jpg (deflated 0%)\n",
            "  adding: out/indoor/1435_5.jpg (deflated 1%)\n",
            "  adding: out/indoor/1444_7.jpg (deflated 0%)\n",
            "  adding: out/indoor/1420_5.jpg (deflated 0%)\n",
            "  adding: out/indoor/1446_5.jpg (deflated 0%)\n",
            "  adding: out/indoor/1410_8.jpg (deflated 0%)\n",
            "  adding: out/indoor/1427_8.jpg (deflated 0%)\n",
            "  adding: out/indoor/1400_9.jpg (deflated 0%)\n",
            "  adding: out/indoor/1433_6.jpg (deflated 0%)\n",
            "  adding: out/indoor/1404_7.jpg (deflated 1%)\n",
            "  adding: out/indoor/1421_8.jpg (deflated 0%)\n",
            "  adding: out/indoor/1409_4.jpg (deflated 0%)\n",
            "  adding: out/indoor/1418_3.jpg (deflated 0%)\n",
            "  adding: out/indoor/1427_7.jpg (deflated 1%)\n",
            "  adding: out/indoor/1440_2.jpg (deflated 1%)\n",
            "  adding: out/indoor/1412_6.jpg (deflated 0%)\n",
            "  adding: out/indoor/1431_3.jpg (deflated 1%)\n",
            "  adding: out/indoor/1437_7.jpg (deflated 1%)\n",
            "  adding: out/indoor/1429_2.jpg (deflated 0%)\n",
            "  adding: out/indoor/1437_5.jpg (deflated 1%)\n",
            "  adding: out/indoor/1441_6.jpg (deflated 1%)\n",
            "  adding: out/indoor/1411_9.jpg (deflated 2%)\n",
            "  adding: out/indoor/1409_2.jpg (deflated 0%)\n",
            "  adding: out/indoor/1442_5.jpg (deflated 0%)\n",
            "  adding: out/indoor/1408_2.jpg (deflated 0%)\n",
            "  adding: out/indoor/1400_3.jpg (deflated 0%)\n",
            "  adding: out/indoor/1406_2.jpg (deflated 1%)\n",
            "  adding: out/indoor/1421_6.jpg (deflated 0%)\n",
            "  adding: out/indoor/1415_2.jpg (deflated 1%)\n",
            "  adding: out/indoor/1420_4.jpg (deflated 0%)\n",
            "  adding: out/indoor/1424_9.jpg (deflated 0%)\n",
            "  adding: out/indoor/1423_3.jpg (deflated 0%)\n",
            "  adding: out/indoor/1438_3.jpg (deflated 1%)\n",
            "  adding: out/indoor/1414_9.jpg (deflated 0%)\n",
            "  adding: out/indoor/1404_8.jpg (deflated 1%)\n",
            "  adding: out/indoor/1415_10.jpg (deflated 1%)\n",
            "  adding: out/indoor/1400_5.jpg (deflated 0%)\n",
            "  adding: out/indoor/1432_6.jpg (deflated 1%)\n",
            "  adding: out/indoor/1436_4.jpg (deflated 0%)\n",
            "  adding: out/indoor/1435_10.jpg (deflated 1%)\n",
            "  adding: out/indoor/1409_1.jpg (deflated 0%)\n",
            "  adding: out/indoor/1448_5.jpg (deflated 1%)\n",
            "  adding: out/indoor/1401_10.jpg (deflated 0%)\n",
            "  adding: out/indoor/1417_1.jpg (deflated 0%)\n",
            "  adding: out/indoor/1414_6.jpg (deflated 1%)\n",
            "  adding: out/indoor/1403_6.jpg (deflated 0%)\n",
            "  adding: out/indoor/1408_9.jpg (deflated 0%)\n",
            "  adding: out/indoor/1410_10.jpg (deflated 0%)\n",
            "  adding: out/indoor/1413_6.jpg (deflated 0%)\n",
            "  adding: out/indoor/1413_5.jpg (deflated 0%)\n",
            "  adding: out/indoor/1434_1.jpg (deflated 1%)\n",
            "  adding: out/indoor/1438_10.jpg (deflated 1%)\n",
            "  adding: out/indoor/1440_4.jpg (deflated 1%)\n",
            "  adding: out/indoor/1413_2.jpg (deflated 0%)\n",
            "  adding: out/indoor/1422_5.jpg (deflated 0%)\n",
            "  adding: out/indoor/1404_3.jpg (deflated 0%)\n",
            "  adding: out/indoor/1403_10.jpg (deflated 0%)\n",
            "  adding: out/indoor/1412_9.jpg (deflated 1%)\n",
            "  adding: out/indoor/1403_5.jpg (deflated 0%)\n",
            "  adding: out/indoor/1425_10.jpg (deflated 0%)\n",
            "  adding: out/indoor/1448_4.jpg (deflated 0%)\n",
            "  adding: out/indoor/1426_9.jpg (deflated 0%)\n",
            "  adding: out/indoor/1402_8.jpg (deflated 1%)\n",
            "  adding: out/indoor/1431_10.jpg (deflated 1%)\n",
            "  adding: out/indoor/1439_4.jpg (deflated 0%)\n",
            "  adding: out/indoor/1415_8.jpg (deflated 1%)\n",
            "  adding: out/indoor/1435_7.jpg (deflated 1%)\n",
            "  adding: out/indoor/1407_8.jpg (deflated 0%)\n",
            "  adding: out/indoor/1441_2.jpg (deflated 1%)\n",
            "  adding: out/indoor/1402_10.jpg (deflated 1%)\n",
            "  adding: out/indoor/1437_4.jpg (deflated 1%)\n",
            "  adding: out/indoor/1414_4.jpg (deflated 1%)\n",
            "  adding: out/indoor/1404_10.jpg (deflated 1%)\n",
            "  adding: out/indoor/1411_1.jpg (deflated 1%)\n",
            "  adding: out/indoor/1432_1.jpg (deflated 1%)\n",
            "  adding: out/indoor/1428_1.jpg (deflated 0%)\n",
            "  adding: out/indoor/1431_5.jpg (deflated 1%)\n",
            "  adding: out/indoor/1405_5.jpg (deflated 1%)\n",
            "  adding: out/indoor/1402_9.jpg (deflated 2%)\n",
            "  adding: out/indoor/1437_1.jpg (deflated 1%)\n",
            "  adding: out/indoor/1416_6.jpg (deflated 0%)\n",
            "  adding: out/indoor/1434_7.jpg (deflated 1%)\n",
            "  adding: out/indoor/1408_10.jpg (deflated 0%)\n",
            "  adding: out/indoor/1445_6.jpg (deflated 0%)\n",
            "  adding: out/indoor/1428_4.jpg (deflated 0%)\n",
            "  adding: out/indoor/1419_9.jpg (deflated 0%)\n",
            "  adding: out/indoor/1423_1.jpg (deflated 0%)\n",
            "  adding: out/indoor/1412_5.jpg (deflated 1%)\n",
            "  adding: out/indoor/1433_9.jpg (deflated 0%)\n",
            "  adding: out/indoor/1411_5.jpg (deflated 2%)\n",
            "  adding: out/indoor/1407_4.jpg (deflated 0%)\n",
            "  adding: out/indoor/1447_9.jpg (deflated 0%)\n",
            "  adding: out/indoor/1426_10.jpg (deflated 0%)\n",
            "  adding: out/indoor/1442_9.jpg (deflated 0%)\n",
            "  adding: out/indoor/1411_3.jpg (deflated 1%)\n",
            "  adding: out/indoor/1444_6.jpg (deflated 0%)\n",
            "  adding: out/indoor/1424_2.jpg (deflated 0%)\n",
            "  adding: out/indoor/1440_8.jpg (deflated 1%)\n",
            "  adding: out/indoor/1428_3.jpg (deflated 0%)\n",
            "  adding: out/indoor/1447_3.jpg (deflated 0%)\n",
            "  adding: out/indoor/1406_9.jpg (deflated 0%)\n",
            "  adding: out/indoor/1404_4.jpg (deflated 1%)\n",
            "  adding: out/indoor/1405_8.jpg (deflated 1%)\n",
            "  adding: out/indoor/1402_4.jpg (deflated 1%)\n",
            "  adding: out/indoor/1422_7.jpg (deflated 0%)\n",
            "  adding: out/indoor/1427_3.jpg (deflated 1%)\n",
            "  adding: out/indoor/1406_5.jpg (deflated 1%)\n",
            "  adding: out/indoor/1408_8.jpg (deflated 0%)\n",
            "  adding: out/indoor/1401_9.jpg (deflated 0%)\n",
            "  adding: out/indoor/1420_7.jpg (deflated 0%)\n",
            "  adding: out/indoor/1439_3.jpg (deflated 0%)\n",
            "  adding: out/indoor/1439_10.jpg (deflated 0%)\n",
            "  adding: out/indoor/1419_2.jpg (deflated 0%)\n",
            "  adding: out/indoor/1431_2.jpg (deflated 1%)\n",
            "  adding: out/indoor/1430_4.jpg (deflated 1%)\n",
            "  adding: out/indoor/1424_3.jpg (deflated 0%)\n",
            "  adding: out/indoor/1434_8.jpg (deflated 0%)\n",
            "  adding: out/indoor/1439_9.jpg (deflated 0%)\n",
            "  adding: out/indoor/1415_3.jpg (deflated 1%)\n",
            "  adding: out/indoor/1402_2.jpg (deflated 1%)\n",
            "  adding: out/indoor/1431_9.jpg (deflated 1%)\n",
            "  adding: out/indoor/1422_6.jpg (deflated 0%)\n",
            "  adding: out/indoor/1448_3.jpg (deflated 1%)\n",
            "  adding: out/indoor/1407_7.jpg (deflated 0%)\n",
            "  adding: out/indoor/1415_6.jpg (deflated 1%)\n",
            "  adding: out/indoor/1446_8.jpg (deflated 0%)\n",
            "  adding: out/indoor/1421_5.jpg (deflated 0%)\n",
            "  adding: out/indoor/1429_10.jpg (deflated 0%)\n",
            "  adding: out/indoor/1425_1.jpg (deflated 0%)\n",
            "  adding: out/indoor/1416_9.jpg (deflated 0%)\n",
            "  adding: out/indoor/1417_6.jpg (deflated 0%)\n",
            "  adding: out/indoor/1436_6.jpg (deflated 0%)\n",
            "  adding: out/indoor/1445_4.jpg (deflated 0%)\n",
            "  adding: out/indoor/1442_6.jpg (deflated 0%)\n",
            "  adding: out/indoor/1429_7.jpg (deflated 0%)\n",
            "  adding: out/indoor/1428_10.jpg (deflated 0%)\n",
            "  adding: out/indoor/1418_8.jpg (deflated 0%)\n",
            "  adding: out/indoor/1413_3.jpg (deflated 0%)\n",
            "  adding: out/indoor/1410_6.jpg (deflated 1%)\n",
            "  adding: out/indoor/1416_10.jpg (deflated 0%)\n",
            "  adding: out/indoor/1408_6.jpg (deflated 0%)\n",
            "  adding: out/indoor/1433_1.jpg (deflated 0%)\n",
            "  adding: out/indoor/1419_8.jpg (deflated 0%)\n",
            "  adding: out/indoor/1425_5.jpg (deflated 0%)\n",
            "  adding: out/indoor/1434_3.jpg (deflated 1%)\n",
            "  adding: out/indoor/1436_3.jpg (deflated 0%)\n",
            "  adding: out/indoor/1436_10.jpg (deflated 0%)\n",
            "  adding: out/indoor/1424_6.jpg (deflated 0%)\n",
            "  adding: out/indoor/1417_7.jpg (deflated 0%)\n",
            "  adding: out/indoor/1411_7.jpg (deflated 2%)\n",
            "  adding: out/indoor/1400_8.jpg (deflated 0%)\n",
            "  adding: out/indoor/1443_1.jpg (deflated 0%)\n",
            "  adding: out/indoor/1419_4.jpg (deflated 0%)\n",
            "  adding: out/indoor/1426_1.jpg (deflated 0%)\n",
            "  adding: out/indoor/1410_2.jpg (deflated 1%)\n",
            "  adding: out/indoor/1435_2.jpg (deflated 0%)\n",
            "  adding: out/indoor/1400_6.jpg (deflated 0%)\n",
            "  adding: out/indoor/1439_2.jpg (deflated 0%)\n",
            "  adding: out/indoor/1421_1.jpg (deflated 0%)\n",
            "  adding: out/indoor/1420_3.jpg (deflated 0%)\n",
            "  adding: out/indoor/1410_3.jpg (deflated 1%)\n",
            "  adding: out/indoor/1425_2.jpg (deflated 0%)\n",
            "  adding: out/indoor/1418_1.jpg (deflated 0%)\n",
            "  adding: out/indoor/1405_6.jpg (deflated 1%)\n",
            "  adding: out/indoor/1432_10.jpg (deflated 1%)\n",
            "  adding: out/indoor/1426_4.jpg (deflated 0%)\n",
            "  adding: out/indoor/1441_9.jpg (deflated 1%)\n",
            "  adding: out/indoor/1404_5.jpg (deflated 1%)\n",
            "  adding: out/indoor/1446_2.jpg (deflated 0%)\n",
            "  adding: out/indoor/1415_9.jpg (deflated 1%)\n",
            "  adding: out/indoor/1443_7.jpg (deflated 1%)\n",
            "  adding: out/indoor/1400_7.jpg (deflated 0%)\n",
            "  adding: out/indoor/1425_8.jpg (deflated 0%)\n",
            "  adding: out/indoor/1434_10.jpg (deflated 1%)\n",
            "  adding: out/indoor/1442_2.jpg (deflated 0%)\n",
            "  adding: out/indoor/1438_9.jpg (deflated 0%)\n",
            "  adding: out/indoor/1449_7.jpg (deflated 0%)\n",
            "  adding: out/indoor/1405_2.jpg (deflated 1%)\n",
            "  adding: out/indoor/1416_7.jpg (deflated 0%)\n",
            "  adding: out/indoor/1443_9.jpg (deflated 0%)\n",
            "  adding: out/indoor/1421_3.jpg (deflated 0%)\n",
            "  adding: out/indoor/1404_2.jpg (deflated 1%)\n",
            "  adding: out/indoor/1435_6.jpg (deflated 1%)\n",
            "  adding: out/indoor/1423_6.jpg (deflated 0%)\n",
            "  adding: out/indoor/1431_6.jpg (deflated 1%)\n",
            "  adding: out/indoor/1404_1.jpg (deflated 0%)\n",
            "  adding: out/indoor/1445_1.jpg (deflated 0%)\n",
            "  adding: out/indoor/1432_2.jpg (deflated 1%)\n",
            "  adding: out/indoor/1443_2.jpg (deflated 0%)\n",
            "  adding: out/indoor/1433_10.jpg (deflated 0%)\n",
            "  adding: out/indoor/1437_2.jpg (deflated 1%)\n",
            "  adding: out/indoor/1406_4.jpg (deflated 1%)\n",
            "  adding: out/indoor/1422_2.jpg (deflated 0%)\n",
            "  adding: out/indoor/1449_8.jpg (deflated 0%)\n",
            "  adding: out/indoor/1429_9.jpg (deflated 0%)\n",
            "  adding: out/indoor/1418_6.jpg (deflated 0%)\n",
            "  adding: out/indoor/1430_8.jpg (deflated 1%)\n",
            "  adding: out/indoor/1410_5.jpg (deflated 1%)\n",
            "  adding: out/indoor/1443_5.jpg (deflated 0%)\n",
            "  adding: out/indoor/1415_4.jpg (deflated 1%)\n",
            "  adding: out/indoor/1402_3.jpg (deflated 1%)\n",
            "  adding: out/indoor/1435_3.jpg (deflated 0%)\n",
            "  adding: out/indoor/1436_9.jpg (deflated 0%)\n",
            "  adding: out/indoor/1429_4.jpg (deflated 0%)\n",
            "  adding: out/indoor/1417_5.jpg (deflated 0%)\n",
            "  adding: out/indoor/1411_8.jpg (deflated 2%)\n",
            "  adding: out/indoor/1417_4.jpg (deflated 0%)\n",
            "  adding: out/indoor/1412_1.jpg (deflated 1%)\n",
            "  adding: out/indoor/1406_3.jpg (deflated 0%)\n",
            "  adding: out/indoor/1402_6.jpg (deflated 1%)\n",
            "  adding: out/indoor/1408_4.jpg (deflated 0%)\n",
            "  adding: out/indoor/1407_3.jpg (deflated 0%)\n",
            "  adding: out/indoor/1426_6.jpg (deflated 0%)\n",
            "  adding: out/indoor/1417_10.jpg (deflated 0%)\n",
            "  adding: out/indoor/1434_6.jpg (deflated 1%)\n",
            "  adding: out/indoor/1421_10.jpg (deflated 0%)\n",
            "  adding: out/indoor/1440_7.jpg (deflated 2%)\n",
            "  adding: out/indoor/1448_1.jpg (deflated 0%)\n",
            "  adding: out/indoor/1402_7.jpg (deflated 1%)\n",
            "  adding: out/indoor/1433_3.jpg (deflated 0%)\n",
            "  adding: out/indoor/1448_10.jpg (deflated 0%)\n",
            "  adding: out/indoor/1439_7.jpg (deflated 0%)\n",
            "  adding: out/indoor/1419_6.jpg (deflated 0%)\n",
            "  adding: out/indoor/1449_5.jpg (deflated 0%)\n",
            "  adding: out/indoor/1444_3.jpg (deflated 0%)\n",
            "  adding: out/indoor/1442_7.jpg (deflated 0%)\n",
            "  adding: out/indoor/1449_6.jpg (deflated 0%)\n",
            "  adding: out/indoor/1406_10.jpg (deflated 0%)\n",
            "  adding: out/indoor/1447_6.jpg (deflated 0%)\n",
            "  adding: out/indoor/1436_5.jpg (deflated 0%)\n",
            "  adding: out/indoor/1413_9.jpg (deflated 0%)\n",
            "  adding: out/indoor/1444_9.jpg (deflated 0%)\n",
            "  adding: out/indoor/1437_8.jpg (deflated 1%)\n",
            "  adding: out/indoor/1418_9.jpg (deflated 0%)\n",
            "  adding: out/indoor/1447_8.jpg (deflated 0%)\n",
            "  adding: out/indoor/1417_2.jpg (deflated 0%)\n",
            "  adding: out/indoor/1446_4.jpg (deflated 0%)\n",
            "  adding: out/indoor/1424_5.jpg (deflated 0%)\n",
            "  adding: out/indoor/1424_1.jpg (deflated 0%)\n",
            "  adding: out/indoor/1443_10.jpg (deflated 1%)\n",
            "  adding: out/indoor/1439_8.jpg (deflated 0%)\n",
            "  adding: out/indoor/1421_9.jpg (deflated 0%)\n",
            "  adding: out/indoor/1432_4.jpg (deflated 1%)\n",
            "  adding: out/indoor/1431_7.jpg (deflated 1%)\n",
            "  adding: out/indoor/1429_6.jpg (deflated 0%)\n",
            "  adding: out/indoor/1403_4.jpg (deflated 0%)\n",
            "  adding: out/indoor/1422_4.jpg (deflated 1%)\n",
            "  adding: out/indoor/1432_9.jpg (deflated 1%)\n",
            "  adding: out/indoor/1413_8.jpg (deflated 0%)\n",
            "  adding: out/indoor/1435_4.jpg (deflated 0%)\n",
            "  adding: out/indoor/1446_1.jpg (deflated 0%)\n",
            "  adding: out/indoor/1440_6.jpg (deflated 2%)\n",
            "  adding: out/indoor/1417_8.jpg (deflated 0%)\n",
            "  adding: out/indoor/1434_5.jpg (deflated 1%)\n",
            "  adding: out/indoor/1413_4.jpg (deflated 0%)\n",
            "  adding: out/indoor/1430_1.jpg (deflated 1%)\n",
            "  adding: out/indoor/1446_7.jpg (deflated 0%)\n",
            "  adding: out/indoor/1401_6.jpg (deflated 0%)\n",
            "  adding: out/indoor/1426_8.jpg (deflated 0%)\n",
            "  adding: out/indoor/1412_4.jpg (deflated 0%)\n",
            "  adding: out/indoor/1422_3.jpg (deflated 0%)\n",
            "  adding: out/indoor/1416_3.jpg (deflated 0%)\n",
            "  adding: out/indoor/1426_2.jpg (deflated 0%)\n",
            "  adding: out/indoor/1421_2.jpg (deflated 0%)\n",
            "  adding: out/indoor/1414_1.jpg (deflated 1%)\n",
            "  adding: out/indoor/1404_9.jpg (deflated 1%)\n",
            "  adding: out/indoor/1409_8.jpg (deflated 0%)\n",
            "  adding: out/indoor/1439_1.jpg (deflated 0%)\n",
            "  adding: out/indoor/1401_1.jpg (deflated 0%)\n",
            "  adding: out/indoor/1420_2.jpg (deflated 0%)\n",
            "  adding: out/indoor/1444_8.jpg (deflated 0%)\n",
            "  adding: out/indoor/1401_2.jpg (deflated 0%)\n",
            "  adding: out/indoor/1442_1.jpg (deflated 0%)\n",
            "  adding: out/indoor/1417_9.jpg (deflated 0%)\n",
            "  adding: out/indoor/1405_1.jpg (deflated 1%)\n",
            "  adding: out/indoor/1400_1.jpg (deflated 0%)\n",
            "  adding: out/indoor/1428_8.jpg (deflated 0%)\n",
            "  adding: out/indoor/1412_3.jpg (deflated 1%)\n",
            "  adding: out/indoor/1415_5.jpg (deflated 1%)\n",
            "  adding: out/indoor/1423_10.jpg (deflated 0%)\n",
            "  adding: out/indoor/1444_10.jpg (deflated 0%)\n",
            "  adding: out/indoor/1428_9.jpg (deflated 0%)\n",
            "  adding: out/indoor/1445_3.jpg (deflated 0%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r zid_dehazed.zip out/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VBVnAK3kTsgR"
      },
      "outputs": [],
      "source": [
        "!cp zid_dehazed.zip /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25xuxukZTcE_"
      },
      "source": [
        "# TEST IT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHCTPFKyF4Qs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLtIjOs-GI-J",
        "outputId": "0dd3122d-1ef8-4a00-943d-75d57e73d11d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1400_3_original.jpg  1400_3_psnr.jpg  1400_3_run_final.jpg  1400_3_ssim.jpg\n"
          ]
        }
      ],
      "source": [
        "!ls output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YA6sWAsVGJBK"
      },
      "outputs": [],
      "source": [
        "def show_image(fname):\n",
        "  image = cv2.imread(fname)\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  plt.figure(figsize=(16, 8))\n",
        "  plt.imshow(image)\n",
        "  plt.title(fname)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LT-n1SgFMmfp"
      },
      "outputs": [],
      "source": [
        "!rm -rf output/\n",
        "!mkdir output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_e-FLrgMvwL",
        "outputId": "1d0c5abc-12f2-4e31-8cec-b7dfde39df9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  0% 0/2 [00:00<?, ?it/s]./data/gt.png\n",
            "Iteration 00000    Loss 0.437227  0.000013\n",
            "Iteration 00001    Loss 61.178154  0.000006\n",
            "Iteration 00002    Loss 0.359289  0.000002\n",
            "Iteration 00003    Loss 0.761435  0.000001\n",
            "Iteration 00004    Loss 0.272061  0.000001\n",
            "Iteration 00005    Loss 0.457880  0.000001\n",
            "Iteration 00006    Loss 0.259834  0.000001\n",
            "Iteration 00007    Loss 0.228142  0.000002\n",
            "Iteration 00008    Loss 0.211412  0.000002\n",
            "Iteration 00009    Loss 0.204447  0.000003\n",
            "Iteration 00010    Loss 0.200695  0.000004\n",
            "Iteration 00011    Loss 0.191796  0.000006\n",
            "Iteration 00012    Loss 0.187779  0.000008\n",
            "Iteration 00013    Loss 0.183004  0.000009\n",
            "Iteration 00014    Loss 0.179918  0.000012\n",
            "Iteration 00015    Loss 0.176314  0.000015\n",
            "Iteration 00016    Loss 0.173641  0.000017\n",
            "Iteration 00017    Loss 0.171665  0.000019\n",
            "Iteration 00018    Loss 0.168854  0.000024\n",
            "Iteration 00019    Loss 0.166603  0.000028\n",
            "Iteration 00020    Loss 0.164588  0.000030\n",
            "Iteration 00021    Loss 0.162510  0.000032\n",
            "Iteration 00022    Loss 0.160279  0.000037\n",
            "Iteration 00023    Loss 0.158762  0.000038\n",
            "Iteration 00024    Loss 0.156737  0.000041\n",
            "Iteration 00025    Loss 0.154122  0.000042\n",
            "Iteration 00026    Loss 0.153042  0.000045\n",
            "Iteration 00027    Loss 0.150603  0.000051\n",
            "Iteration 00028    Loss 0.148910  0.000053\n",
            "Iteration 00029    Loss 0.147126  0.000057\n",
            "Iteration 00030    Loss 0.145393  0.000064\n",
            "Iteration 00031    Loss 0.143348  0.000065\n",
            "Iteration 00032    Loss 0.142363  0.000070\n",
            "Iteration 00033    Loss 0.140138  0.000071\n",
            "Iteration 00034    Loss 0.138432  0.000079\n",
            "Iteration 00035    Loss 0.136950  0.000080\n",
            "Iteration 00036    Loss 0.134926  0.000086\n",
            "Iteration 00037    Loss 0.133457  0.000093\n",
            "Iteration 00038    Loss 0.131568  0.000098\n",
            "Iteration 00039    Loss 0.130398  0.000099\n",
            "Iteration 00040    Loss 0.128035  0.000106\n",
            "Iteration 00041    Loss 0.126535  0.000116\n",
            "Iteration 00042    Loss 0.124738  0.000113\n",
            "Iteration 00043    Loss 0.123102  0.000120\n",
            "Iteration 00044    Loss 0.121477  0.000122\n",
            "Iteration 00045    Loss 0.119654  0.000125\n",
            "Iteration 00046    Loss 0.118256  0.000130\n",
            "Iteration 00047    Loss 0.116489  0.000136\n",
            "Iteration 00048    Loss 0.115017  0.000145\n",
            "Iteration 00049    Loss 0.113370  0.000147\n",
            "Iteration 00050    Loss 0.112230  0.000160\n",
            "Iteration 00051    Loss 0.110720  0.000177\n",
            "Iteration 00052    Loss 0.109148  0.000186\n",
            "Iteration 00053    Loss 0.107617  0.000186\n",
            "Iteration 00054    Loss 0.106208  0.000191\n",
            "Iteration 00055    Loss 0.104856  0.000202\n",
            "Iteration 00056    Loss 0.103137  0.000220\n",
            "Iteration 00057    Loss 0.101547  0.000233\n",
            "Iteration 00058    Loss 0.100071  0.000236\n",
            "Iteration 00059    Loss 0.098649  0.000238\n",
            "Iteration 00060    Loss 0.097234  0.000251\n",
            "Iteration 00061    Loss 0.095859  0.000260\n",
            "Iteration 00062    Loss 0.094579  0.000269\n",
            "Iteration 00063    Loss 0.092963  0.000290\n",
            "Iteration 00064    Loss 0.091474  0.000281\n",
            "Iteration 00065    Loss 0.090199  0.000269\n",
            "Iteration 00066    Loss 0.088995  0.000290\n",
            "Iteration 00067    Loss 0.087512  0.000307\n",
            "Iteration 00068    Loss 0.086241  0.000311\n",
            "Iteration 00069    Loss 0.084902  0.000324\n",
            "Iteration 00070    Loss 0.083443  0.000338\n",
            "Iteration 00071    Loss 0.082449  0.000354\n",
            "Iteration 00072    Loss 0.080953  0.000356\n",
            "Iteration 00073    Loss 0.079611  0.000358\n",
            "Iteration 00074    Loss 0.078679  0.000370\n",
            "Iteration 00075    Loss 0.077041  0.000363\n",
            "Iteration 00076    Loss 0.075954  0.000385\n",
            "Iteration 00077    Loss 0.074765  0.000420\n",
            "Iteration 00078    Loss 0.073552  0.000412\n",
            "Iteration 00079    Loss 0.072543  0.000420\n",
            "Iteration 00080    Loss 0.071158  0.000437\n",
            "Iteration 00081    Loss 0.070063  0.000445\n",
            "Iteration 00082    Loss 0.068960  0.000449\n",
            "Iteration 00083    Loss 0.067827  0.000455\n",
            "Iteration 00084    Loss 0.066838  0.000471\n",
            "Iteration 00085    Loss 0.065717  0.000491\n",
            "Iteration 00086    Loss 0.064638  0.000515\n",
            "Iteration 00087    Loss 0.063629  0.000512\n",
            "Iteration 00088    Loss 0.062551  0.000529\n",
            "Iteration 00089    Loss 0.061487  0.000551\n",
            "Iteration 00090    Loss 0.060360  0.000549\n",
            "Iteration 00091    Loss 0.059451  0.000556\n",
            "Iteration 00092    Loss 0.058469  0.000568\n",
            "Iteration 00093    Loss 0.057403  0.000595\n",
            "Iteration 00094    Loss 0.056449  0.000607\n",
            "Iteration 00095    Loss 0.055526  0.000599\n",
            "Iteration 00096    Loss 0.054565  0.000623\n",
            "Iteration 00097    Loss 0.053609  0.000616\n",
            "Iteration 00098    Loss 0.052572  0.000631\n",
            "Iteration 00099    Loss 0.051644  0.000647\n",
            "Iteration 00100    Loss 0.050721  0.000689\n",
            "Iteration 00101    Loss 0.049802  0.000660\n",
            "Iteration 00102    Loss 0.048934  0.000670\n",
            "Iteration 00103    Loss 0.047951  0.000695\n",
            "Iteration 00104    Loss 0.047006  0.000730\n",
            "Iteration 00105    Loss 0.046213  0.000701\n",
            "Iteration 00106    Loss 0.045307  0.000737\n",
            "Iteration 00107    Loss 0.044483  0.000709\n",
            "Iteration 00108    Loss 0.043764  0.000784\n",
            "Iteration 00109    Loss 0.042954  0.000703\n",
            "Iteration 00110    Loss 0.042109  0.000784\n",
            "Iteration 00111    Loss 0.041159  0.000779\n",
            "Iteration 00112    Loss 0.040222  0.000773\n",
            "Iteration 00113    Loss 0.039574  0.000841\n",
            "Iteration 00114    Loss 0.038721  0.000778\n",
            "Iteration 00115    Loss 0.037816  0.000814\n",
            "Iteration 00116    Loss 0.036975  0.000853\n",
            "Iteration 00117    Loss 0.036211  0.000818\n",
            "Iteration 00118    Loss 0.035432  0.000852\n",
            "Iteration 00119    Loss 0.034550  0.000867\n",
            "Iteration 00120    Loss 0.033941  0.000831\n",
            "Iteration 00121    Loss 0.033001  0.000876\n",
            "Iteration 00122    Loss 0.032293  0.000917\n",
            "Iteration 00123    Loss 0.031477  0.000873\n",
            "Iteration 00124    Loss 0.030715  0.000922\n",
            "Iteration 00125    Loss 0.029960  0.000931\n",
            "Iteration 00126    Loss 0.029236  0.000926\n",
            "Iteration 00127    Loss 0.028436  0.000919\n",
            "Iteration 00128    Loss 0.027730  0.000952\n",
            "Iteration 00129    Loss 0.027067  0.001008\n",
            "Iteration 00130    Loss 0.026285  0.000951\n",
            "Iteration 00131    Loss 0.025611  0.000993\n",
            "Iteration 00132    Loss 0.024867  0.001014\n",
            "Iteration 00133    Loss 0.024077  0.000991\n",
            "Iteration 00134    Loss 0.023423  0.001038\n",
            "Iteration 00135    Loss 0.022803  0.001005\n",
            "Iteration 00136    Loss 0.022696  0.001093\n",
            "Iteration 00137    Loss 0.021513  0.000992\n",
            "Iteration 00138    Loss 0.021135  0.001018\n",
            "Iteration 00139    Loss 0.020571  0.001082\n",
            "Iteration 00140    Loss 0.020037  0.001060\n",
            "Iteration 00141    Loss 0.019281  0.001067\n",
            "Iteration 00142    Loss 0.018675  0.001043\n",
            "Iteration 00143    Loss 0.017689  0.001062\n",
            "Iteration 00144    Loss 0.017182  0.001073\n",
            "Iteration 00145    Loss 0.016369  0.001073\n",
            "Iteration 00146    Loss 0.015836  0.001091\n",
            "Iteration 00147    Loss 0.015158  0.001122\n",
            "Iteration 00148    Loss 0.014348  0.001134\n",
            "Iteration 00149    Loss 0.013983  0.001121\n",
            "Iteration 00150    Loss 0.013111  0.001161\n",
            "Iteration 00151    Loss 0.012624  0.001155\n",
            "Iteration 00152    Loss 0.012016  0.001174\n",
            "Iteration 00153    Loss 0.011393  0.001173\n",
            "Iteration 00154    Loss 0.010836  0.001160\n",
            "Iteration 00155    Loss 0.010205  0.001236\n",
            "Iteration 00156    Loss 0.009574  0.001245\n",
            "Iteration 00157    Loss 0.009126  0.001216\n",
            "Iteration 00158    Loss 0.008554  0.001269\n",
            "Iteration 00159    Loss 0.008002  0.001191\n",
            "Iteration 00160    Loss 0.007461  0.001327\n",
            "Iteration 00161    Loss 0.006876  0.001298\n",
            "Iteration 00162    Loss 0.006406  0.001266\n",
            "Iteration 00163    Loss 0.005897  0.001354\n",
            "Iteration 00164    Loss 0.005417  0.001260\n",
            "Iteration 00165    Loss 0.004823  0.001343\n",
            "Iteration 00166    Loss 0.004426  0.001348\n",
            "Iteration 00167    Loss 0.003907  0.001310\n",
            "Iteration 00168    Loss 0.003417  0.001370\n",
            "Iteration 00169    Loss 0.002932  0.001349\n",
            "Iteration 00170    Loss 0.002388  0.001372\n",
            "Iteration 00171    Loss 0.001959  0.001371\n",
            "Iteration 00172    Loss 0.001401  0.001397\n",
            "Iteration 00173    Loss 0.000936  0.001423\n",
            "Iteration 00174    Loss 0.000581  0.001405\n",
            "Iteration 00175    Loss 0.000125  0.001436\n",
            "Iteration 00176    Loss -0.000405  0.001435\n",
            "Iteration 00177    Loss -0.000794  0.001422\n",
            "Iteration 00178    Loss -0.001251  0.001464\n",
            "Iteration 00179    Loss -0.001718  0.001468\n",
            "Iteration 00180    Loss -0.002094  0.001453\n",
            "Iteration 00181    Loss -0.002505  0.001466\n",
            "Iteration 00182    Loss -0.002964  0.001507\n",
            "Iteration 00183    Loss -0.003363  0.001472\n",
            "Iteration 00184    Loss -0.003735  0.001508\n",
            "Iteration 00185    Loss -0.004201  0.001516\n",
            "Iteration 00186    Loss -0.004560  0.001521\n",
            "Iteration 00187    Loss -0.004950  0.001535\n",
            "Iteration 00188    Loss -0.005318  0.001552\n",
            "Iteration 00189    Loss -0.005730  0.001555\n",
            "Iteration 00190    Loss -0.006041  0.001551\n",
            "Iteration 00191    Loss -0.006421  0.001590\n",
            "Iteration 00192    Loss -0.006750  0.001486\n",
            "Iteration 00193    Loss -0.006923  0.001637\n",
            "Iteration 00194    Loss -0.007050  0.001450\n",
            "Iteration 00195    Loss -0.007494  0.001590\n",
            "Iteration 00196    Loss -0.007980  0.001490\n",
            "Iteration 00197    Loss -0.008148  0.001519\n",
            "Iteration 00198    Loss -0.008497  0.001545\n",
            "Iteration 00199    Loss -0.009158  0.001610\n",
            "Iteration 00200    Loss -0.009290  0.001562\n",
            "Iteration 00201    Loss -0.009647  0.001604\n",
            "Iteration 00202    Loss -0.010092  0.001643\n",
            "Iteration 00203    Loss -0.010333  0.001543\n",
            "Iteration 00204    Loss -0.010735  0.001617\n",
            "Iteration 00205    Loss -0.011061  0.001628\n",
            "Iteration 00206    Loss -0.011450  0.001569\n",
            "Iteration 00207    Loss -0.011738  0.001613\n",
            "Iteration 00208    Loss -0.012088  0.001649\n",
            "Iteration 00209    Loss -0.012402  0.001581\n",
            "Iteration 00210    Loss -0.012701  0.001710\n",
            "Iteration 00211    Loss -0.013036  0.001707\n",
            "Iteration 00212    Loss -0.013319  0.001630\n",
            "Iteration 00213    Loss -0.013619  0.001643\n",
            "Iteration 00214    Loss -0.013934  0.001664\n",
            "Iteration 00215    Loss -0.014201  0.001691\n",
            "Iteration 00216    Loss -0.014502  0.001701\n",
            "Iteration 00217    Loss -0.014806  0.001719\n",
            "Iteration 00218    Loss -0.015053  0.001742\n",
            "Iteration 00219    Loss -0.015340  0.001739\n",
            "Iteration 00220    Loss -0.015603  0.001711\n",
            "Iteration 00221    Loss -0.015808  0.001756\n",
            "Iteration 00222    Loss -0.016058  0.001763\n",
            "Iteration 00223    Loss -0.016305  0.001759\n",
            "Iteration 00224    Loss -0.016646  0.001733\n",
            "Iteration 00225    Loss -0.016923  0.001810\n",
            "Iteration 00226    Loss -0.017117  0.001774\n",
            "Iteration 00227    Loss -0.017370  0.001769\n",
            "Iteration 00228    Loss -0.017657  0.001807\n",
            "Iteration 00229    Loss -0.017935  0.001799\n",
            "Iteration 00230    Loss -0.018208  0.001799\n",
            "Iteration 00231    Loss -0.018388  0.001822\n",
            "Iteration 00232    Loss -0.018654  0.001813\n",
            "Iteration 00233    Loss -0.018914  0.001831\n",
            "Iteration 00234    Loss -0.019158  0.001843\n",
            "Iteration 00235    Loss -0.019367  0.001830\n",
            "Iteration 00236    Loss -0.019611  0.001828\n",
            "Iteration 00237    Loss -0.019808  0.001888\n",
            "Iteration 00238    Loss -0.020022  0.001847\n",
            "Iteration 00239    Loss -0.020098  0.001853\n",
            "Iteration 00240    Loss -0.020104  0.001772\n",
            "Iteration 00241    Loss -0.020051  0.001873\n",
            "Iteration 00242    Loss -0.020563  0.001789\n",
            "Iteration 00243    Loss -0.020804  0.001860\n",
            "Iteration 00244    Loss -0.020917  0.001867\n",
            "Iteration 00245    Loss -0.021224  0.001855\n",
            "Iteration 00246    Loss -0.021448  0.001793\n",
            "Iteration 00247    Loss -0.021737  0.001761\n",
            "Iteration 00248    Loss -0.021872  0.001837\n",
            "Iteration 00249    Loss -0.022141  0.001836\n",
            "Iteration 00250    Loss -0.022378  0.001844\n",
            "Iteration 00251    Loss -0.022562  0.001865\n",
            "Iteration 00252    Loss -0.022788  0.001883\n",
            "Iteration 00253    Loss -0.022968  0.001909\n",
            "Iteration 00254    Loss -0.023162  0.001880\n",
            "Iteration 00255    Loss -0.023379  0.001889\n",
            "Iteration 00256    Loss -0.023584  0.001922\n",
            "Iteration 00257    Loss -0.023797  0.001915\n",
            "Iteration 00258    Loss -0.023931  0.001901\n",
            "Iteration 00259    Loss -0.024143  0.001922\n",
            "Iteration 00260    Loss -0.024320  0.001948\n",
            "Iteration 00261    Loss -0.024432  0.001915\n",
            "Iteration 00262    Loss -0.024636  0.001924\n",
            "Iteration 00263    Loss -0.024796  0.001930\n",
            "Iteration 00264    Loss -0.024981  0.001953\n",
            "Iteration 00265    Loss -0.025221  0.001968\n",
            "Iteration 00266    Loss -0.025387  0.001978\n",
            "Iteration 00267    Loss -0.025593  0.001978\n",
            "Iteration 00268    Loss -0.025772  0.002000\n",
            "Iteration 00269    Loss -0.025916  0.001999\n",
            "Iteration 00270    Loss -0.026066  0.001965\n",
            "Iteration 00271    Loss -0.026162  0.002025\n",
            "Iteration 00272    Loss -0.026242  0.001999\n",
            "Iteration 00273    Loss -0.026310  0.001981\n",
            "Iteration 00274    Loss -0.026562  0.001971\n",
            "Iteration 00275    Loss -0.026754  0.002082\n",
            "Iteration 00276    Loss -0.026882  0.001997\n",
            "Iteration 00277    Loss -0.026907  0.001998\n",
            "Iteration 00278    Loss -0.027065  0.002065\n",
            "Iteration 00279    Loss -0.027376  0.002007\n",
            "Iteration 00280    Loss -0.027481  0.002032\n",
            "Iteration 00281    Loss -0.027582  0.002041\n",
            "Iteration 00282    Loss -0.027742  0.001988\n",
            "Iteration 00283    Loss -0.027909  0.002111\n",
            "Iteration 00284    Loss -0.027966  0.001997\n",
            "Iteration 00285    Loss -0.028048  0.001967\n",
            "Iteration 00286    Loss -0.028191  0.002040\n",
            "Iteration 00287    Loss -0.028451  0.002074\n",
            "Iteration 00288    Loss -0.028611  0.002000\n",
            "Iteration 00289    Loss -0.028778  0.002003\n",
            "Iteration 00290    Loss -0.028907  0.002083\n",
            "Iteration 00291    Loss -0.029010  0.002062\n",
            "Iteration 00292    Loss -0.029195  0.002042\n",
            "Iteration 00293    Loss -0.029343  0.002083\n",
            "Iteration 00294    Loss -0.029486  0.002035\n",
            "Iteration 00295    Loss -0.029637  0.002082\n",
            "Iteration 00296    Loss -0.029774  0.002132\n",
            "Iteration 00297    Loss -0.029942  0.002093\n",
            "Iteration 00298    Loss -0.030052  0.002042\n",
            "Iteration 00299    Loss -0.030187  0.002099\n",
            "Iteration 00300    Loss -0.030319  0.002109\n",
            "Iteration 00301    Loss -0.030394  0.002088\n",
            "Iteration 00302    Loss -0.030413  0.002072\n",
            "Iteration 00303    Loss -0.030418  0.002158\n",
            "Iteration 00304    Loss -0.030432  0.002057\n",
            "Iteration 00305    Loss -0.030736  0.002149\n",
            "Iteration 00306    Loss -0.030925  0.002074\n",
            "Iteration 00307    Loss -0.030980  0.002102\n",
            "Iteration 00308    Loss -0.031076  0.002166\n",
            "Iteration 00309    Loss -0.031321  0.002096\n",
            "Iteration 00310    Loss -0.031382  0.002119\n",
            "Iteration 00311    Loss -0.031525  0.002114\n",
            "Iteration 00312    Loss -0.031672  0.002119\n",
            "Iteration 00313    Loss -0.031752  0.002172\n",
            "Iteration 00314    Loss -0.031889  0.002115\n",
            "Iteration 00315    Loss -0.032005  0.002178\n",
            "Iteration 00316    Loss -0.032171  0.002184\n",
            "Iteration 00317    Loss -0.032259  0.002125\n",
            "Iteration 00318    Loss -0.032346  0.002175\n",
            "Iteration 00319    Loss -0.032475  0.002158\n",
            "Iteration 00320    Loss -0.032599  0.002169\n",
            "Iteration 00321    Loss -0.032701  0.002163\n",
            "Iteration 00322    Loss -0.032795  0.002175\n",
            "Iteration 00323    Loss -0.032920  0.002189\n",
            "Iteration 00324    Loss -0.033030  0.002182\n",
            "Iteration 00325    Loss -0.033101  0.002154\n",
            "Iteration 00326    Loss -0.033193  0.002234\n",
            "Iteration 00327    Loss -0.033270  0.002175\n",
            "Iteration 00328    Loss -0.033243  0.002178\n",
            "Iteration 00329    Loss -0.033160  0.002181\n",
            "Iteration 00330    Loss -0.033111  0.002238\n",
            "Iteration 00331    Loss -0.033495  0.002132\n",
            "Iteration 00332    Loss -0.033642  0.002185\n",
            "Iteration 00333    Loss -0.033660  0.002241\n",
            "Iteration 00334    Loss -0.033755  0.002171\n",
            "Iteration 00335    Loss -0.033919  0.002240\n",
            "Iteration 00336    Loss -0.034060  0.002206\n",
            "Iteration 00337    Loss -0.034204  0.002147\n",
            "Iteration 00338    Loss -0.034195  0.002189\n",
            "Iteration 00339    Loss -0.034320  0.002254\n",
            "Iteration 00340    Loss -0.034434  0.002263\n",
            "Iteration 00341    Loss -0.034481  0.002179\n",
            "Iteration 00342    Loss -0.034576  0.002184\n",
            "Iteration 00343    Loss -0.034698  0.002279\n",
            "Iteration 00344    Loss -0.034798  0.002189\n",
            "Iteration 00345    Loss -0.034883  0.002227\n",
            "Iteration 00346    Loss -0.034995  0.002190\n",
            "Iteration 00347    Loss -0.035103  0.002241\n",
            "Iteration 00348    Loss -0.035228  0.002229\n",
            "Iteration 00349    Loss -0.035301  0.002263\n",
            "Iteration 00350    Loss -0.035291  0.002229\n",
            "Iteration 00351    Loss -0.035096  0.002243\n",
            "Iteration 00352    Loss -0.035064  0.002213\n",
            "Iteration 00353    Loss -0.035245  0.002275\n",
            "Iteration 00354    Loss -0.035456  0.002177\n",
            "Iteration 00355    Loss -0.035585  0.002181\n",
            "Iteration 00356    Loss -0.035721  0.002230\n",
            "Iteration 00357    Loss -0.035668  0.002286\n",
            "Iteration 00358    Loss -0.035814  0.002200\n",
            "Iteration 00359    Loss -0.035933  0.002104\n",
            "Iteration 00360    Loss -0.036020  0.002308\n",
            "Iteration 00361    Loss -0.036152  0.002260\n",
            "Iteration 00362    Loss -0.036223  0.002181\n",
            "Iteration 00363    Loss -0.036355  0.002203\n",
            "Iteration 00364    Loss -0.036447  0.002220\n",
            "Iteration 00365    Loss -0.036555  0.002292\n",
            "Iteration 00366    Loss -0.036675  0.002261\n",
            "Iteration 00367    Loss -0.036729  0.002168\n",
            "Iteration 00368    Loss -0.036862  0.002286\n",
            "Iteration 00369    Loss -0.036920  0.002315\n",
            "Iteration 00370    Loss -0.037007  0.002189\n",
            "Iteration 00371    Loss -0.037133  0.002224\n",
            "Iteration 00372    Loss -0.037189  0.002345\n",
            "Iteration 00373    Loss -0.037279  0.002295\n",
            "Iteration 00374    Loss -0.037356  0.002258\n",
            "Iteration 00375    Loss -0.037440  0.002240\n",
            "Iteration 00376    Loss -0.037506  0.002296\n",
            "Iteration 00377    Loss -0.037583  0.002322\n",
            "Iteration 00378    Loss -0.037674  0.002301\n",
            "Iteration 00379    Loss -0.037737  0.002294\n",
            "Iteration 00380    Loss -0.037806  0.002320\n",
            "Iteration 00381    Loss -0.037824  0.002254\n",
            "Iteration 00382    Loss -0.037818  0.002328\n",
            "Iteration 00383    Loss -0.037735  0.002271\n",
            "Iteration 00384    Loss -0.037580  0.002312\n",
            "Iteration 00385    Loss -0.037805  0.002270\n",
            "Iteration 00386    Loss -0.038157  0.002347\n",
            "Iteration 00387    Loss -0.038110  0.002312\n",
            "Iteration 00388    Loss -0.038134  0.002233\n",
            "Iteration 00389    Loss -0.038250  0.002301\n",
            "Iteration 00390    Loss -0.038355  0.002365\n",
            "Iteration 00391    Loss -0.038366  0.002310\n",
            "Iteration 00392    Loss -0.038393  0.002299\n",
            "Iteration 00393    Loss -0.038492  0.002270\n",
            "Iteration 00394    Loss -0.038523  0.002363\n",
            "Iteration 00395    Loss -0.038532  0.002271\n",
            "Iteration 00396    Loss -0.038616  0.002347\n",
            "Iteration 00397    Loss -0.038738  0.002294\n",
            "Iteration 00398    Loss -0.038808  0.002343\n",
            "Iteration 00399    Loss -0.038895  0.002319\n",
            "Iteration 00400    Loss -0.038979  0.002269\n",
            "Iteration 00401    Loss -0.039020  0.002348\n",
            "Iteration 00402    Loss -0.039110  0.002333\n",
            "Iteration 00403    Loss -0.039177  0.002325\n",
            "Iteration 00404    Loss -0.039247  0.002348\n",
            "Iteration 00405    Loss -0.039297  0.002306\n",
            "Iteration 00406    Loss -0.039386  0.002360\n",
            "Iteration 00407    Loss -0.039438  0.002367\n",
            "Iteration 00408    Loss -0.039485  0.002326\n",
            "Iteration 00409    Loss -0.039540  0.002379\n",
            "Iteration 00410    Loss -0.039578  0.002341\n",
            "Iteration 00411    Loss -0.039622  0.002337\n",
            "Iteration 00412    Loss -0.039695  0.002388\n",
            "Iteration 00413    Loss -0.039717  0.002311\n",
            "Iteration 00414    Loss -0.039737  0.002355\n",
            "Iteration 00415    Loss -0.039834  0.002344\n",
            "Iteration 00416    Loss -0.039881  0.002382\n",
            "Iteration 00417    Loss -0.039935  0.002373\n",
            "Iteration 00418    Loss -0.039997  0.002330\n",
            "Iteration 00419    Loss -0.040040  0.002408\n",
            "Iteration 00420    Loss -0.040083  0.002327\n",
            "Iteration 00421    Loss -0.040111  0.002371\n",
            "Iteration 00422    Loss -0.040154  0.002348\n",
            "Iteration 00423    Loss -0.040182  0.002357\n",
            "Iteration 00424    Loss -0.040200  0.002414\n",
            "Iteration 00425    Loss -0.040217  0.002325\n",
            "Iteration 00426    Loss -0.040193  0.002379\n",
            "Iteration 00427    Loss -0.040180  0.002340\n",
            "Iteration 00428    Loss -0.040251  0.002325\n",
            "Iteration 00429    Loss -0.040427  0.002359\n",
            "Iteration 00430    Loss -0.040532  0.002379\n",
            "Iteration 00431    Loss -0.040544  0.002346\n",
            "Iteration 00432    Loss -0.040522  0.002369\n",
            "Iteration 00433    Loss -0.040602  0.002366\n",
            "Iteration 00434    Loss -0.040730  0.002367\n",
            "Iteration 00435    Loss -0.040754  0.002354\n",
            "Iteration 00436    Loss -0.040761  0.002372\n",
            "Iteration 00437    Loss -0.040815  0.002408\n",
            "Iteration 00438    Loss -0.040897  0.002372\n",
            "Iteration 00439    Loss -0.040945  0.002344\n",
            "Iteration 00440    Loss -0.040980  0.002425\n",
            "Iteration 00441    Loss -0.041029  0.002383\n",
            "Iteration 00442    Loss -0.041077  0.002363\n",
            "Iteration 00443    Loss -0.041118  0.002420\n",
            "Iteration 00444    Loss -0.041147  0.002389\n",
            "Iteration 00445    Loss -0.041178  0.002399\n",
            "Iteration 00446    Loss -0.041194  0.002395\n",
            "Iteration 00447    Loss -0.041178  0.002387\n",
            "Iteration 00448    Loss -0.041130  0.002396\n",
            "Iteration 00449    Loss -0.041132  0.002385\n",
            "Iteration 00450    Loss -0.041240  0.002328\n",
            "Iteration 00451    Loss -0.041311  0.002409\n",
            "Iteration 00452    Loss -0.041298  0.002403\n",
            "Iteration 00453    Loss -0.041347  0.002320\n",
            "Iteration 00454    Loss -0.041406  0.002432\n",
            "Iteration 00455    Loss -0.041457  0.002283\n",
            "Iteration 00456    Loss -0.041493  0.002378\n",
            "Iteration 00457    Loss -0.041541  0.002349\n",
            "Iteration 00458    Loss -0.041622  0.002398\n",
            "Iteration 00459    Loss -0.041627  0.002399\n",
            "Iteration 00460    Loss -0.041675  0.002353\n",
            "Iteration 00461    Loss -0.041737  0.002410\n",
            "Iteration 00462    Loss -0.041771  0.002377\n",
            "Iteration 00463    Loss -0.041813  0.002432\n",
            "Iteration 00464    Loss -0.041860  0.002386\n",
            "Iteration 00465    Loss -0.041940  0.002364\n",
            "Iteration 00466    Loss -0.041967  0.002420\n",
            "Iteration 00467    Loss -0.042040  0.002393\n",
            "Iteration 00468    Loss -0.042088  0.002406\n",
            "Iteration 00469    Loss -0.042152  0.002401\n",
            "Iteration 00470    Loss -0.042189  0.002430\n",
            "Iteration 00471    Loss -0.042221  0.002437\n",
            "Iteration 00472    Loss -0.042261  0.002390\n",
            "Iteration 00473    Loss -0.042301  0.002453\n",
            "Iteration 00474    Loss -0.042336  0.002415\n",
            "Iteration 00475    Loss -0.042384  0.002402\n",
            "Iteration 00476    Loss -0.042416  0.002432\n",
            "Iteration 00477    Loss -0.042435  0.002421\n",
            "Iteration 00478    Loss -0.042420  0.002425\n",
            "Iteration 00479    Loss -0.042322  0.002389\n",
            "Iteration 00480    Loss -0.042134  0.002469\n",
            "Iteration 00481    Loss -0.041934  0.002352\n",
            "Iteration 00482    Loss -0.042230  0.002421\n",
            "Iteration 00483    Loss -0.042401  0.002394\n",
            "Iteration 00484    Loss -0.042271  0.002423\n",
            "Iteration 00485    Loss -0.042386  0.002295\n",
            "Iteration 00486    Loss -0.042366  0.002457\n",
            "Iteration 00487    Loss -0.042504  0.002342\n",
            "Iteration 00488    Loss -0.042543  0.002357\n",
            "Iteration 00489    Loss -0.042559  0.002328\n",
            "Iteration 00490    Loss -0.042712  0.002399\n",
            "Iteration 00491    Loss -0.042706  0.002286\n",
            "Iteration 00492    Loss -0.042809  0.002471\n",
            "Iteration 00493    Loss -0.042813  0.002498\n",
            "Iteration 00494    Loss -0.042878  0.002332\n",
            "Iteration 00495    Loss -0.042936  0.002345\n",
            "Iteration 00496    Loss -0.042971  0.002451\n",
            "Iteration 00497    Loss -0.043003  0.002421\n",
            "Iteration 00498    Loss -0.043049  0.002388\n",
            "Iteration 00499    Loss -0.043091  0.002391\n",
            " 50% 1/2 [00:40<00:40, 40.41s/it]./data/hazy.png\n",
            "Iteration 00000    Loss 0.477040  0.000041\n",
            "Iteration 00001    Loss 27.174112  0.000019\n",
            "Iteration 00002    Loss 0.365992  0.000007\n",
            "Iteration 00003    Loss 0.364526  0.000004\n",
            "Iteration 00004    Loss 1.267912  0.000003\n",
            "Iteration 00005    Loss 0.530447  0.000003\n",
            "Iteration 00006    Loss 0.518086  0.000003\n",
            "Iteration 00007    Loss 0.200137  0.000004\n",
            "Iteration 00008    Loss 0.456430  0.000004\n",
            "Iteration 00009    Loss 0.197659  0.000005\n",
            "Iteration 00010    Loss 0.128463  0.000005\n",
            "Iteration 00011    Loss 0.117744  0.000007\n",
            "Iteration 00012    Loss 0.110710  0.000007\n",
            "Iteration 00013    Loss 0.104006  0.000010\n",
            "Iteration 00014    Loss 0.098323  0.000011\n",
            "Iteration 00015    Loss 0.093765  0.000012\n",
            "Iteration 00016    Loss 0.089918  0.000012\n",
            "Iteration 00017    Loss 0.086145  0.000013\n",
            "Iteration 00018    Loss 0.083104  0.000015\n",
            "Iteration 00019    Loss 0.080765  0.000017\n",
            "Iteration 00020    Loss 0.078364  0.000020\n",
            "Iteration 00021    Loss 0.076239  0.000021\n",
            "Iteration 00022    Loss 0.074569  0.000024\n",
            "Iteration 00023    Loss 0.072913  0.000027\n",
            "Iteration 00024    Loss 0.071095  0.000031\n",
            "Iteration 00025    Loss 0.069484  0.000034\n",
            "Iteration 00026    Loss 0.067921  0.000037\n",
            "Iteration 00027    Loss 0.066638  0.000040\n",
            "Iteration 00028    Loss 0.065286  0.000044\n",
            "Iteration 00029    Loss 0.063979  0.000044\n",
            "Iteration 00030    Loss 0.062843  0.000045\n",
            "Iteration 00031    Loss 0.061687  0.000048\n",
            "Iteration 00032    Loss 0.060523  0.000050\n",
            "Iteration 00033    Loss 0.059482  0.000052\n",
            "Iteration 00034    Loss 0.058485  0.000054\n",
            "Iteration 00035    Loss 0.057302  0.000055\n",
            "Iteration 00036    Loss 0.056101  0.000056\n",
            "Iteration 00037    Loss 0.055210  0.000059\n",
            "Iteration 00038    Loss 0.054179  0.000060\n",
            "Iteration 00039    Loss 0.053082  0.000063\n",
            "Iteration 00040    Loss 0.052157  0.000065\n",
            "Iteration 00041    Loss 0.051304  0.000066\n",
            "Iteration 00042    Loss 0.050448  0.000071\n",
            "Iteration 00043    Loss 0.049418  0.000076\n",
            "Iteration 00044    Loss 0.048569  0.000076\n",
            "Iteration 00045    Loss 0.047754  0.000078\n",
            "Iteration 00046    Loss 0.046807  0.000084\n",
            "Iteration 00047    Loss 0.045961  0.000085\n",
            "Iteration 00048    Loss 0.045234  0.000086\n",
            "Iteration 00049    Loss 0.044360  0.000092\n",
            "Iteration 00050    Loss 0.043617  0.000096\n",
            "Iteration 00051    Loss 0.042773  0.000096\n",
            "Iteration 00052    Loss 0.042050  0.000099\n",
            "Iteration 00053    Loss 0.041257  0.000104\n",
            "Iteration 00054    Loss 0.040622  0.000108\n",
            "Iteration 00055    Loss 0.039864  0.000112\n",
            "Iteration 00056    Loss 0.039224  0.000116\n",
            "Iteration 00057    Loss 0.038347  0.000120\n",
            "Iteration 00058    Loss 0.037697  0.000117\n",
            "Iteration 00059    Loss 0.036995  0.000125\n",
            "Iteration 00060    Loss 0.036256  0.000127\n",
            "Iteration 00061    Loss 0.035640  0.000130\n",
            "Iteration 00062    Loss 0.034837  0.000144\n",
            "Iteration 00063    Loss 0.034114  0.000139\n",
            "Iteration 00064    Loss 0.033564  0.000142\n",
            "Iteration 00065    Loss 0.032761  0.000150\n",
            "Iteration 00066    Loss 0.032164  0.000150\n",
            "Iteration 00067    Loss 0.031537  0.000158\n",
            "Iteration 00068    Loss 0.030868  0.000164\n",
            "Iteration 00069    Loss 0.030250  0.000156\n",
            "Iteration 00070    Loss 0.029602  0.000160\n",
            "Iteration 00071    Loss 0.029044  0.000171\n",
            "Iteration 00072    Loss 0.028470  0.000172\n",
            "Iteration 00073    Loss 0.027895  0.000180\n",
            "Iteration 00074    Loss 0.027328  0.000190\n",
            "Iteration 00075    Loss 0.026699  0.000188\n",
            "Iteration 00076    Loss 0.026070  0.000193\n",
            "Iteration 00077    Loss 0.025465  0.000190\n",
            "Iteration 00078    Loss 0.025010  0.000200\n",
            "Iteration 00079    Loss 0.024437  0.000213\n",
            "Iteration 00080    Loss 0.023921  0.000203\n",
            "Iteration 00081    Loss 0.023312  0.000207\n",
            "Iteration 00082    Loss 0.022756  0.000216\n",
            "Iteration 00083    Loss 0.022228  0.000217\n",
            "Iteration 00084    Loss 0.021724  0.000218\n",
            "Iteration 00085    Loss 0.021147  0.000219\n",
            "Iteration 00086    Loss 0.020673  0.000225\n",
            "Iteration 00087    Loss 0.020264  0.000223\n",
            "Iteration 00088    Loss 0.019687  0.000233\n",
            "Iteration 00089    Loss 0.019169  0.000246\n",
            "Iteration 00090    Loss 0.018547  0.000239\n",
            "Iteration 00091    Loss 0.018059  0.000250\n",
            "Iteration 00092    Loss 0.017576  0.000254\n",
            "Iteration 00093    Loss 0.016996  0.000253\n",
            "Iteration 00094    Loss 0.016619  0.000251\n",
            "Iteration 00095    Loss 0.016018  0.000260\n",
            "Iteration 00096    Loss 0.015606  0.000263\n",
            "Iteration 00097    Loss 0.015255  0.000277\n",
            "Iteration 00098    Loss 0.014993  0.000264\n",
            "Iteration 00099    Loss 0.014228  0.000279\n",
            "Iteration 00100    Loss 0.013946  0.000268\n",
            "Iteration 00101    Loss 0.013475  0.000260\n",
            "Iteration 00102    Loss 0.012859  0.000273\n",
            "Iteration 00103    Loss 0.012388  0.000273\n",
            "Iteration 00104    Loss 0.011864  0.000273\n",
            "Iteration 00105    Loss 0.011560  0.000282\n",
            "Iteration 00106    Loss 0.011001  0.000284\n",
            "Iteration 00107    Loss 0.010491  0.000286\n",
            "Iteration 00108    Loss 0.010128  0.000293\n",
            "Iteration 00109    Loss 0.009721  0.000287\n",
            "Iteration 00110    Loss 0.009296  0.000283\n",
            "Iteration 00111    Loss 0.008902  0.000296\n",
            "Iteration 00112    Loss 0.008379  0.000301\n",
            "Iteration 00113    Loss 0.007970  0.000302\n",
            "Iteration 00114    Loss 0.007598  0.000313\n",
            "Iteration 00115    Loss 0.007130  0.000310\n",
            "Iteration 00116    Loss 0.006764  0.000307\n",
            "Iteration 00117    Loss 0.006520  0.000313\n",
            "Iteration 00118    Loss 0.005944  0.000332\n",
            "Iteration 00119    Loss 0.005590  0.000330\n",
            "Iteration 00120    Loss 0.005044  0.000329\n",
            "Iteration 00121    Loss 0.004724  0.000330\n",
            "Iteration 00122    Loss 0.004195  0.000332\n",
            "Iteration 00123    Loss 0.003974  0.000329\n",
            "Iteration 00124    Loss 0.003513  0.000335\n",
            "Iteration 00125    Loss 0.003092  0.000354\n",
            "Iteration 00126    Loss 0.002759  0.000351\n",
            "Iteration 00127    Loss 0.002268  0.000348\n",
            "Iteration 00128    Loss 0.002196  0.000364\n",
            "Iteration 00129    Loss 0.001644  0.000357\n",
            "Iteration 00130    Loss 0.001261  0.000356\n",
            "Iteration 00131    Loss 0.001002  0.000366\n",
            "Iteration 00132    Loss 0.000772  0.000344\n",
            "Iteration 00133    Loss 0.000336  0.000363\n",
            "Iteration 00134    Loss -0.000198  0.000384\n",
            "Iteration 00135    Loss -0.000261  0.000360\n",
            "Iteration 00136    Loss -0.000526  0.000382\n",
            "Iteration 00137    Loss -0.001067  0.000385\n",
            "Iteration 00138    Loss -0.001317  0.000356\n",
            "Iteration 00139    Loss -0.001843  0.000374\n",
            "Iteration 00140    Loss -0.002017  0.000387\n",
            "Iteration 00141    Loss -0.002339  0.000378\n",
            "Iteration 00142    Loss -0.002866  0.000386\n",
            "Iteration 00143    Loss -0.002874  0.000393\n",
            "Iteration 00144    Loss -0.003482  0.000402\n",
            "Iteration 00145    Loss -0.003471  0.000387\n",
            "Iteration 00146    Loss -0.003902  0.000402\n",
            "Iteration 00147    Loss -0.004421  0.000407\n",
            "Iteration 00148    Loss -0.004597  0.000398\n",
            "Iteration 00149    Loss -0.004784  0.000376\n",
            "Iteration 00150    Loss -0.004970  0.000411\n",
            "Iteration 00151    Loss -0.005515  0.000411\n",
            "Iteration 00152    Loss -0.005662  0.000394\n",
            "Iteration 00153    Loss -0.006144  0.000422\n",
            "Iteration 00154    Loss -0.006131  0.000427\n",
            "Iteration 00155    Loss -0.006605  0.000414\n",
            "Iteration 00156    Loss -0.006780  0.000430\n",
            "Iteration 00157    Loss -0.007048  0.000416\n",
            "Iteration 00158    Loss -0.007549  0.000417\n",
            "Iteration 00159    Loss -0.007875  0.000419\n",
            "Iteration 00160    Loss -0.008165  0.000422\n",
            "Iteration 00161    Loss -0.008374  0.000431\n",
            "Iteration 00162    Loss -0.008425  0.000422\n",
            "Iteration 00163    Loss -0.008984  0.000438\n",
            "Iteration 00164    Loss -0.008876  0.000430\n",
            "Iteration 00165    Loss -0.009310  0.000430\n",
            "Iteration 00166    Loss -0.009904  0.000427\n",
            "Iteration 00167    Loss -0.009511  0.000434\n",
            "Iteration 00168    Loss -0.010048  0.000435\n",
            "Iteration 00169    Loss -0.010264  0.000453\n",
            "Iteration 00170    Loss -0.010425  0.000461\n",
            "Iteration 00171    Loss -0.010698  0.000425\n",
            "Iteration 00172    Loss -0.010967  0.000444\n",
            "Iteration 00173    Loss -0.011066  0.000426\n",
            "Iteration 00174    Loss -0.011464  0.000436\n",
            "Iteration 00175    Loss -0.011485  0.000465\n",
            "Iteration 00176    Loss -0.012102  0.000448\n",
            "Iteration 00177    Loss -0.012355  0.000434\n",
            "Iteration 00178    Loss -0.012292  0.000426\n",
            "Iteration 00179    Loss -0.012401  0.000415\n",
            "Iteration 00180    Loss -0.013064  0.000420\n",
            "Iteration 00181    Loss -0.013254  0.000431\n",
            "Iteration 00182    Loss -0.013230  0.000440\n",
            "Iteration 00183    Loss -0.013656  0.000438\n",
            "Iteration 00184    Loss -0.013759  0.000436\n",
            "Iteration 00185    Loss -0.014327  0.000446\n",
            "Iteration 00186    Loss -0.014259  0.000437\n",
            "Iteration 00187    Loss -0.014780  0.000432\n",
            "Iteration 00188    Loss -0.014918  0.000440\n",
            "Iteration 00189    Loss -0.014921  0.000425\n",
            "Iteration 00190    Loss -0.015191  0.000435\n",
            "Iteration 00191    Loss -0.015180  0.000453\n",
            "Iteration 00192    Loss -0.015380  0.000429\n",
            "Iteration 00193    Loss -0.015791  0.000420\n",
            "Iteration 00194    Loss -0.015794  0.000444\n",
            "Iteration 00195    Loss -0.016185  0.000446\n",
            "Iteration 00196    Loss -0.016555  0.000426\n",
            "Iteration 00197    Loss -0.016887  0.000427\n",
            "Iteration 00198    Loss -0.017047  0.000441\n",
            "Iteration 00199    Loss -0.016835  0.000438\n",
            "Iteration 00200    Loss -0.017363  0.000430\n",
            "Iteration 00201    Loss -0.017746  0.000431\n",
            "Iteration 00202    Loss -0.017853  0.000437\n",
            "Iteration 00203    Loss -0.017964  0.000433\n",
            "Iteration 00204    Loss -0.018225  0.000432\n",
            "Iteration 00205    Loss -0.018484  0.000428\n",
            "Iteration 00206    Loss -0.018321  0.000429\n",
            "Iteration 00207    Loss -0.018917  0.000439\n",
            "Iteration 00208    Loss -0.019040  0.000434\n",
            "Iteration 00209    Loss -0.019340  0.000434\n",
            "Iteration 00210    Loss -0.019545  0.000435\n",
            "Iteration 00211    Loss -0.019667  0.000443\n",
            "Iteration 00212    Loss -0.019883  0.000441\n",
            "Iteration 00213    Loss -0.019968  0.000440\n",
            "Iteration 00214    Loss -0.020110  0.000433\n",
            "Iteration 00215    Loss -0.020436  0.000421\n",
            "Iteration 00216    Loss -0.020531  0.000428\n",
            "Iteration 00217    Loss -0.020624  0.000443\n",
            "Iteration 00218    Loss -0.020836  0.000438\n",
            "Iteration 00219    Loss -0.021152  0.000445\n",
            "Iteration 00220    Loss -0.021324  0.000439\n",
            "Iteration 00221    Loss -0.021326  0.000425\n",
            "Iteration 00222    Loss -0.021679  0.000443\n",
            "Iteration 00223    Loss -0.021545  0.000452\n",
            "Iteration 00224    Loss -0.021822  0.000462\n",
            "Iteration 00225    Loss -0.021865  0.000464\n",
            "Iteration 00226    Loss -0.022019  0.000440\n",
            "Iteration 00227    Loss -0.022566  0.000463\n",
            "Iteration 00228    Loss -0.022646  0.000451\n",
            "Iteration 00229    Loss -0.022935  0.000445\n",
            "Iteration 00230    Loss -0.022849  0.000446\n",
            "Iteration 00231    Loss -0.023051  0.000436\n",
            "Iteration 00232    Loss -0.023430  0.000442\n",
            "Iteration 00233    Loss -0.023433  0.000444\n",
            "Iteration 00234    Loss -0.023114  0.000451\n",
            "Iteration 00235    Loss -0.023913  0.000453\n",
            "Iteration 00236    Loss -0.023809  0.000450\n",
            "Iteration 00237    Loss -0.023823  0.000454\n",
            "Iteration 00238    Loss -0.023990  0.000450\n",
            "Iteration 00239    Loss -0.024187  0.000431\n",
            "Iteration 00240    Loss -0.024475  0.000448\n",
            "Iteration 00241    Loss -0.024485  0.000447\n",
            "Iteration 00242    Loss -0.024186  0.000435\n",
            "Iteration 00243    Loss -0.024978  0.000442\n",
            "Iteration 00244    Loss -0.025154  0.000430\n",
            "Iteration 00245    Loss -0.024871  0.000425\n",
            "Iteration 00246    Loss -0.025102  0.000457\n",
            "Iteration 00247    Loss -0.025568  0.000436\n",
            "Iteration 00248    Loss -0.025540  0.000430\n",
            "Iteration 00249    Loss -0.025780  0.000443\n",
            "Iteration 00250    Loss -0.025841  0.000431\n",
            "Iteration 00251    Loss -0.025967  0.000444\n",
            "Iteration 00252    Loss -0.025603  0.000449\n",
            "Iteration 00253    Loss -0.026384  0.000426\n",
            "Iteration 00254    Loss -0.026554  0.000422\n",
            "Iteration 00255    Loss -0.026455  0.000446\n",
            "Iteration 00256    Loss -0.026616  0.000444\n",
            "Iteration 00257    Loss -0.026304  0.000445\n",
            "Iteration 00258    Loss -0.026560  0.000464\n",
            "Iteration 00259    Loss -0.026995  0.000459\n",
            "Iteration 00260    Loss -0.027029  0.000447\n",
            "Iteration 00261    Loss -0.027281  0.000450\n",
            "Iteration 00262    Loss -0.027422  0.000446\n",
            "Iteration 00263    Loss -0.027258  0.000451\n",
            "Iteration 00264    Loss -0.027720  0.000447\n",
            "Iteration 00265    Loss -0.027669  0.000453\n",
            "Iteration 00266    Loss -0.027831  0.000456\n",
            "Iteration 00267    Loss -0.027947  0.000441\n",
            "Iteration 00268    Loss -0.028193  0.000439\n",
            "Iteration 00269    Loss -0.028100  0.000460\n",
            "Iteration 00270    Loss -0.028308  0.000452\n",
            "Iteration 00271    Loss -0.028273  0.000441\n",
            "Iteration 00272    Loss -0.028651  0.000447\n",
            "Iteration 00273    Loss -0.028789  0.000435\n",
            "Iteration 00274    Loss -0.028823  0.000431\n",
            "Iteration 00275    Loss -0.028911  0.000446\n",
            "Iteration 00276    Loss -0.028673  0.000443\n",
            "Iteration 00277    Loss -0.029107  0.000434\n",
            "Iteration 00278    Loss -0.028837  0.000436\n",
            "Iteration 00279    Loss -0.029416  0.000445\n",
            "Iteration 00280    Loss -0.029536  0.000441\n",
            "Iteration 00281    Loss -0.029421  0.000438\n",
            "Iteration 00282    Loss -0.029620  0.000438\n",
            "Iteration 00283    Loss -0.029589  0.000440\n",
            "Iteration 00284    Loss -0.029761  0.000436\n",
            "Iteration 00285    Loss -0.029871  0.000434\n",
            "Iteration 00286    Loss -0.030149  0.000433\n",
            "Iteration 00287    Loss -0.030036  0.000437\n",
            "Iteration 00288    Loss -0.030124  0.000443\n",
            "Iteration 00289    Loss -0.030286  0.000440\n",
            "Iteration 00290    Loss -0.030428  0.000443\n",
            "Iteration 00291    Loss -0.030753  0.000447\n",
            "Iteration 00292    Loss -0.030527  0.000445\n",
            "Iteration 00293    Loss -0.030583  0.000449\n",
            "Iteration 00294    Loss -0.031016  0.000438\n",
            "Iteration 00295    Loss -0.031014  0.000431\n",
            "Iteration 00296    Loss -0.031120  0.000434\n",
            "Iteration 00297    Loss -0.031230  0.000441\n",
            "Iteration 00298    Loss -0.031293  0.000443\n",
            "Iteration 00299    Loss -0.031240  0.000436\n",
            "Iteration 00300    Loss -0.031538  0.000430\n",
            "Iteration 00301    Loss -0.031630  0.000428\n",
            "Iteration 00302    Loss -0.031724  0.000439\n",
            "Iteration 00303    Loss -0.031779  0.000438\n",
            "Iteration 00304    Loss -0.031904  0.000435\n",
            "Iteration 00305    Loss -0.031666  0.000427\n",
            "Iteration 00306    Loss -0.031917  0.000431\n",
            "Iteration 00307    Loss -0.032099  0.000443\n",
            "Iteration 00308    Loss -0.031794  0.000443\n",
            "Iteration 00309    Loss -0.032157  0.000457\n",
            "Iteration 00310    Loss -0.032040  0.000454\n",
            "Iteration 00311    Loss -0.032464  0.000434\n",
            "Iteration 00312    Loss -0.032263  0.000425\n",
            "Iteration 00313    Loss -0.032251  0.000434\n",
            "Iteration 00314    Loss -0.032640  0.000442\n",
            "Iteration 00315    Loss -0.032465  0.000437\n",
            "Iteration 00316    Loss -0.032691  0.000440\n",
            "Iteration 00317    Loss -0.032824  0.000437\n",
            "Iteration 00318    Loss -0.033068  0.000439\n",
            "Iteration 00319    Loss -0.032977  0.000425\n",
            "Iteration 00320    Loss -0.033085  0.000440\n",
            "Iteration 00321    Loss -0.033283  0.000430\n",
            "Iteration 00322    Loss -0.033217  0.000431\n",
            "Iteration 00323    Loss -0.033359  0.000433\n",
            "Iteration 00324    Loss -0.033367  0.000441\n",
            "Iteration 00325    Loss -0.033478  0.000435\n",
            "Iteration 00326    Loss -0.033581  0.000434\n",
            "Iteration 00327    Loss -0.033661  0.000437\n",
            "Iteration 00328    Loss -0.033813  0.000427\n",
            "Iteration 00329    Loss -0.033847  0.000446\n",
            "Iteration 00330    Loss -0.033854  0.000446\n",
            "Iteration 00331    Loss -0.033853  0.000441\n",
            "Iteration 00332    Loss -0.034183  0.000439\n",
            "Iteration 00333    Loss -0.034230  0.000418\n",
            "Iteration 00334    Loss -0.033993  0.000426\n",
            "Iteration 00335    Loss -0.034324  0.000430\n",
            "Iteration 00336    Loss -0.034313  0.000415\n",
            "Iteration 00337    Loss -0.034387  0.000412\n",
            "Iteration 00338    Loss -0.034505  0.000416\n",
            "Iteration 00339    Loss -0.034593  0.000415\n",
            "Iteration 00340    Loss -0.034472  0.000421\n",
            "Iteration 00341    Loss -0.034716  0.000430\n",
            "Iteration 00342    Loss -0.034874  0.000416\n",
            "Iteration 00343    Loss -0.034740  0.000420\n",
            "Iteration 00344    Loss -0.034950  0.000413\n",
            "Iteration 00345    Loss -0.034985  0.000434\n",
            "Iteration 00346    Loss -0.035051  0.000412\n",
            "Iteration 00347    Loss -0.035140  0.000412\n",
            "Iteration 00348    Loss -0.035254  0.000421\n",
            "Iteration 00349    Loss -0.035206  0.000414\n",
            "Iteration 00350    Loss -0.035446  0.000423\n",
            "Iteration 00351    Loss -0.035459  0.000425\n",
            "Iteration 00352    Loss -0.035630  0.000405\n",
            "Iteration 00353    Loss -0.035587  0.000421\n",
            "Iteration 00354    Loss -0.035617  0.000430\n",
            "Iteration 00355    Loss -0.035707  0.000424\n",
            "Iteration 00356    Loss -0.035827  0.000427\n",
            "Iteration 00357    Loss -0.035818  0.000422\n",
            "Iteration 00358    Loss -0.036030  0.000424\n",
            "Iteration 00359    Loss -0.036134  0.000424\n",
            "Iteration 00360    Loss -0.035914  0.000419\n",
            "Iteration 00361    Loss -0.036185  0.000424\n",
            "Iteration 00362    Loss -0.036216  0.000418\n",
            "Iteration 00363    Loss -0.036285  0.000420\n",
            "Iteration 00364    Loss -0.036389  0.000424\n",
            "Iteration 00365    Loss -0.036039  0.000423\n",
            "Iteration 00366    Loss -0.036508  0.000428\n",
            "Iteration 00367    Loss -0.036539  0.000419\n",
            "Iteration 00368    Loss -0.036380  0.000426\n",
            "Iteration 00369    Loss -0.036714  0.000428\n",
            "Iteration 00370    Loss -0.036713  0.000427\n",
            "Iteration 00371    Loss -0.036744  0.000425\n",
            "Iteration 00372    Loss -0.036720  0.000414\n",
            "Iteration 00373    Loss -0.036787  0.000420\n",
            "Iteration 00374    Loss -0.036927  0.000413\n",
            "Iteration 00375    Loss -0.036946  0.000431\n",
            "Iteration 00376    Loss -0.037084  0.000427\n",
            "Iteration 00377    Loss -0.037108  0.000421\n",
            "Iteration 00378    Loss -0.036962  0.000427\n",
            "Iteration 00379    Loss -0.037206  0.000419\n",
            "Iteration 00380    Loss -0.037254  0.000419\n",
            "Iteration 00381    Loss -0.037328  0.000422\n",
            "Iteration 00382    Loss -0.037314  0.000436\n",
            "Iteration 00383    Loss -0.037471  0.000421\n",
            "Iteration 00384    Loss -0.037423  0.000419\n",
            "Iteration 00385    Loss -0.037535  0.000422\n",
            "Iteration 00386    Loss -0.037487  0.000418\n",
            "Iteration 00387    Loss -0.037667  0.000430\n",
            "Iteration 00388    Loss -0.037730  0.000426\n",
            "Iteration 00389    Loss -0.037781  0.000419\n",
            "Iteration 00390    Loss -0.037848  0.000429\n",
            "Iteration 00391    Loss -0.037841  0.000439\n",
            "Iteration 00392    Loss -0.037867  0.000438\n",
            "Iteration 00393    Loss -0.037814  0.000427\n",
            "Iteration 00394    Loss -0.037781  0.000424\n",
            "Iteration 00395    Loss -0.037985  0.000431\n",
            "Iteration 00396    Loss -0.037826  0.000423\n",
            "Iteration 00397    Loss -0.038170  0.000426\n",
            "Iteration 00398    Loss -0.038116  0.000418\n",
            "Iteration 00399    Loss -0.038131  0.000426\n",
            "Iteration 00400    Loss -0.038316  0.000408\n",
            "Iteration 00401    Loss -0.038194  0.000409\n",
            "Iteration 00402    Loss -0.038248  0.000418\n",
            "Iteration 00403    Loss -0.038424  0.000415\n",
            "Iteration 00404    Loss -0.038396  0.000418\n",
            "Iteration 00405    Loss -0.038504  0.000409\n",
            "Iteration 00406    Loss -0.038559  0.000423\n",
            "Iteration 00407    Loss -0.038551  0.000430\n",
            "Iteration 00408    Loss -0.038665  0.000416\n",
            "Iteration 00409    Loss -0.038712  0.000409\n",
            "Iteration 00410    Loss -0.038652  0.000405\n",
            "Iteration 00411    Loss -0.038707  0.000413\n",
            "Iteration 00412    Loss -0.038716  0.000410\n",
            "Iteration 00413    Loss -0.038833  0.000411\n",
            "Iteration 00414    Loss -0.038907  0.000427\n",
            "Iteration 00415    Loss -0.038806  0.000416\n",
            "Iteration 00416    Loss -0.039047  0.000417\n",
            "Iteration 00417    Loss -0.039075  0.000415\n",
            "Iteration 00418    Loss -0.038982  0.000417\n",
            "Iteration 00419    Loss -0.039125  0.000407\n",
            "Iteration 00420    Loss -0.039154  0.000416\n",
            "Iteration 00421    Loss -0.039135  0.000426\n",
            "Iteration 00422    Loss -0.039262  0.000419\n",
            "Iteration 00423    Loss -0.039349  0.000411\n",
            "Iteration 00424    Loss -0.039373  0.000400\n",
            "Iteration 00425    Loss -0.039332  0.000407\n",
            "Iteration 00426    Loss -0.039408  0.000402\n",
            "Iteration 00427    Loss -0.039417  0.000412\n",
            "Iteration 00428    Loss -0.039619  0.000410\n",
            "Iteration 00429    Loss -0.039263  0.000406\n",
            "Iteration 00430    Loss -0.039480  0.000410\n",
            "Iteration 00431    Loss -0.039666  0.000398\n",
            "Iteration 00432    Loss -0.039667  0.000403\n",
            "Iteration 00433    Loss -0.039729  0.000408\n",
            "Iteration 00434    Loss -0.039719  0.000406\n",
            "Iteration 00435    Loss -0.039677  0.000415\n",
            "Iteration 00436    Loss -0.039828  0.000413\n",
            "Iteration 00437    Loss -0.039893  0.000412\n",
            "Iteration 00438    Loss -0.039949  0.000395\n",
            "Iteration 00439    Loss -0.040025  0.000403\n",
            "Iteration 00440    Loss -0.040038  0.000405\n",
            "Iteration 00441    Loss -0.040101  0.000400\n",
            "Iteration 00442    Loss -0.040109  0.000412\n",
            "Iteration 00443    Loss -0.040144  0.000408\n",
            "Iteration 00444    Loss -0.040222  0.000406\n",
            "Iteration 00445    Loss -0.040085  0.000414\n",
            "Iteration 00446    Loss -0.040174  0.000409\n",
            "Iteration 00447    Loss -0.040254  0.000413\n",
            "Iteration 00448    Loss -0.039974  0.000396\n",
            "Iteration 00449    Loss -0.040415  0.000409\n",
            "Iteration 00450    Loss -0.040309  0.000410\n",
            "Iteration 00451    Loss -0.040120  0.000403\n",
            "Iteration 00452    Loss -0.040436  0.000412\n",
            "Iteration 00453    Loss -0.040489  0.000409\n",
            "Iteration 00454    Loss -0.040429  0.000406\n",
            "Iteration 00455    Loss -0.040415  0.000419\n",
            "Iteration 00456    Loss -0.040579  0.000405\n",
            "Iteration 00457    Loss -0.040629  0.000394\n",
            "Iteration 00458    Loss -0.040653  0.000400\n",
            "Iteration 00459    Loss -0.040612  0.000404\n",
            "Iteration 00460    Loss -0.040736  0.000402\n",
            "Iteration 00464    Loss -0.040913  0.000410\n",
            "Iteration 00465    Loss -0.040841  0.000398\n",
            "Iteration 00466    Loss -0.040582  0.000395\n",
            "Iteration 00467    Loss -0.040926  0.000405\n",
            "Iteration 00468    Loss -0.040929  0.000401\n",
            "Iteration 00469    Loss -0.040944  0.000405\n",
            "Iteration 00470    Loss -0.041109  0.000402\n",
            "Iteration 00471    Loss -0.041125  0.000413\n",
            "Iteration 00472    Loss -0.041107  0.000409\n",
            "Iteration 00473    Loss -0.041191  0.000401\n",
            "Iteration 00474    Loss -0.041305  0.000401\n",
            "Iteration 00475    Loss -0.041242  0.000411\n",
            "Iteration 00476    Loss -0.041302  0.000400\n",
            "Iteration 00477    Loss -0.041253  0.000407\n",
            "Iteration 00478    Loss -0.041411  0.000415\n",
            "Iteration 00479    Loss -0.041313  0.000405\n",
            "Iteration 00480    Loss -0.041394  0.000409\n",
            "Iteration 00481    Loss -0.041520  0.000405\n",
            "Iteration 00482    Loss -0.041442  0.000407\n",
            "Iteration 00483    Loss -0.041439  0.000401\n",
            "Iteration 00484    Loss -0.041521  0.000404\n",
            "Iteration 00485    Loss -0.041516  0.000414\n",
            "Iteration 00486    Loss -0.041573  0.000413\n",
            "Iteration 00487    Loss -0.041621  0.000408\n",
            "Iteration 00488    Loss -0.041653  0.000392\n",
            "Iteration 00489    Loss -0.041666  0.000408\n",
            "Iteration 00490    Loss -0.041408  0.000397\n",
            "Iteration 00491    Loss -0.041603  0.000418\n",
            "Iteration 00492    Loss -0.041680  0.000413\n",
            "Iteration 00493    Loss -0.041874  0.000403\n",
            "Iteration 00494    Loss -0.041801  0.000390\n",
            "Iteration 00495    Loss -0.041833  0.000398\n",
            "Iteration 00496    Loss -0.041803  0.000412\n",
            "Iteration 00497    Loss -0.041938  0.000400\n",
            "Iteration 00498    Loss -0.041950  0.000408\n",
            "Iteration 00499    Loss -0.041983  0.000398\n",
            "100% 2/2 [01:15<00:00, 37.94s/it]\n"
          ]
        }
      ],
      "source": [
        "!python RW_dehazing.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcDlwzi_NRL6",
        "outputId": "775aceb6-6c52-48b1-ed62-6a8b46421f47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gt.jpg\thazy.jpg\n"
          ]
        }
      ],
      "source": [
        "!ls output/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "798awLbbGJED",
        "outputId": "3c258517-1946-4bba-9c0b-9ec5c41399ae"
      },
      "outputs": [
        {
          "ename": "error",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-eb40ab676c24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshow_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output/1400_3_original.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mshow_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output/1400_3_run_final.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-d9f1d93619b3>\u001b[0m in \u001b[0;36mshow_image\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mshow_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.1.2) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
          ]
        }
      ],
      "source": [
        "show_image('output/1400_3_original.jpg')\n",
        "show_image('output/1400_3_run_final.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzr1-1uw-TE1"
      },
      "source": [
        "# Process outdoor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNp3bunZF3r0"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rrwhh7Ic-Uvz"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOXpf0sw-VT0"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYC60p4ZQMpG"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "run_ZID_on_sots.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOpYaMJDMA/rhqqIyldHlsm",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}